{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 11: Building the Transformer\n",
    "\n",
    "> \"We are what we repeatedly do. Excellence, then, is not an act, but a habit.\"\n",
    "> — **Aristotle**, Philosopher\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to stack Transformer blocks into a complete language model\n",
    "- The configuration pattern that makes model experimentation easy\n",
    "- What the language modeling head does and why we need it\n",
    "- Weight tying: the elegant trick that saves 38 million parameters\n",
    "- Essential sanity checks to verify your model before training\n",
    "- How to load pretrained GPT-2 weights into your architecture\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Components from Chapter 10\n",
    "\n",
    "First, let's bring in the components we built in Chapter 10: `MultiHeadAttention`, `FeedForward`, and `TransformerBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MULTI-HEAD ATTENTION (from Chapter 10) =====\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Efficient multi-head attention (batches all heads together).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "        # Combined QKV projection\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq, d_model = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch, seq, 3, self.num_heads, self.d_head)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Weighted sum and concatenate\n",
    "        attn_output = attn_weights @ V\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch, seq, d_model)\n",
    "        \n",
    "        return self.out_proj(attn_output), attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FEEDFORWARD NETWORK (from Chapter 10) =====\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"FeedForward defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRANSFORMER BLOCK (from Chapter 10) =====\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete Transformer block (pre-norm style like GPT-2).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention with residual\n",
    "        attn_out, attn_weights = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        # FFN with residual\n",
    "        ffn_out = self.ffn(self.ln2(x))\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"TransformerBlock defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Model Configuration\n",
    "\n",
    "Let's create a configuration dataclass to bundle all model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for MiniGPT model.\"\"\"\n",
    "    vocab_size: int = 50257      # GPT-2 vocabulary size\n",
    "    max_seq_len: int = 1024      # Maximum context length\n",
    "    embed_dim: int = 768         # Embedding dimension\n",
    "    num_heads: int = 12          # Number of attention heads\n",
    "    num_layers: int = 12         # Number of Transformer blocks\n",
    "    d_ff: int = 3072             # Feedforward hidden dimension\n",
    "    dropout: float = 0.1         # Dropout probability\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        assert self.embed_dim % self.num_heads == 0, \\\n",
    "            f\"embed_dim ({self.embed_dim}) must be divisible by num_heads ({self.num_heads})\"\n",
    "\n",
    "\n",
    "# Test different configurations\n",
    "print(\"GPT-2 Small (default):\")\n",
    "config = GPTConfig()\n",
    "print(f\"  Layers: {config.num_layers}, Heads: {config.num_heads}, Embed: {config.embed_dim}\")\n",
    "\n",
    "print(\"\\nTiny config (for experiments):\")\n",
    "tiny_config = GPTConfig(embed_dim=64, num_heads=2, num_layers=2, d_ff=256)\n",
    "print(f\"  Layers: {tiny_config.num_layers}, Heads: {tiny_config.num_heads}, Embed: {tiny_config.embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. The Complete MiniGPT Model\n",
    "\n",
    "Now let's build the complete model by stacking Transformer blocks and adding the language modeling head with weight tying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT-style language model.\n",
    "    Combines embeddings, Transformer blocks, and language modeling head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # ===== Token and Position Embeddings =====\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # ===== Transformer Blocks =====\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.embed_dim,\n",
    "                num_heads=config.num_heads,\n",
    "                d_ff=config.d_ff,\n",
    "                dropout=config.dropout\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        # ===== Final LayerNorm =====\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "        # ===== Language Modeling Head =====\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        # ===== Weight Tying =====\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with small random values.\"\"\"\n",
    "        nn.init.normal_(self.token_embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            token_ids: Input token IDs (batch, seq)\n",
    "            return_attention: Whether to return attention weights\n",
    "\n",
    "        Returns:\n",
    "            logits: Vocabulary scores (batch, seq, vocab_size)\n",
    "        \"\"\"\n",
    "        batch, seq = token_ids.shape\n",
    "        device = token_ids.device\n",
    "\n",
    "        # Embeddings\n",
    "        tok_emb = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq, device=device)\n",
    "        pos_emb = self.pos_embed(positions)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(seq, seq, device=device))\n",
    "\n",
    "        # Transformer blocks\n",
    "        attention_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            if return_attention:\n",
    "                attention_weights.append(attn)\n",
    "\n",
    "        # Final norm and projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"MiniGPT class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with tiny config\n",
    "tiny_config = GPTConfig(embed_dim=64, num_heads=2, num_layers=2, d_ff=256)\n",
    "model = MiniGPT(tiny_config)\n",
    "\n",
    "# Test forward pass\n",
    "test_tokens = torch.randint(0, tiny_config.vocab_size, (2, 16))\n",
    "logits = model(test_tokens)\n",
    "\n",
    "print(f\"Input shape: {test_tokens.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"\\nExpected: (2, 16, {tiny_config.vocab_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Sanity Checks\n",
    "\n",
    "Let's verify our model is wired correctly with 5 essential tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST 1: Shape Verification =====\n",
    "\n",
    "def test_forward_shapes(config):\n",
    "    \"\"\"Verify model output shapes.\"\"\"\n",
    "    model = MiniGPT(config)\n",
    "    \n",
    "    batch_size, seq_len = 2, 16\n",
    "    token_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
    "    \n",
    "    logits = model(token_ids)\n",
    "    \n",
    "    expected_shape = (batch_size, seq_len, config.vocab_size)\n",
    "    assert logits.shape == expected_shape, \\\n",
    "        f\"Expected {expected_shape}, got {logits.shape}\"\n",
    "    \n",
    "    print(f\"Shape check PASSED: {logits.shape}\")\n",
    "\n",
    "test_forward_shapes(tiny_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST 2: Parameter Count =====\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def test_parameter_count(config):\n",
    "    \"\"\"Verify parameter count is reasonable.\"\"\"\n",
    "    model = MiniGPT(config)\n",
    "    total = count_parameters(model)\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    return total\n",
    "\n",
    "# Test with tiny config\n",
    "print(\"Tiny model:\")\n",
    "test_parameter_count(tiny_config)\n",
    "\n",
    "# Test with full GPT-2 config\n",
    "print(\"\\nGPT-2 Small:\")\n",
    "test_parameter_count(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST 3: Causal Masking =====\n",
    "\n",
    "def test_causal_masking():\n",
    "    \"\"\"Verify model can't see future tokens.\"\"\"\n",
    "    config = GPTConfig(\n",
    "        num_layers=1,\n",
    "        embed_dim=64,\n",
    "        num_heads=2,\n",
    "        d_ff=256,\n",
    "        dropout=0.0  # No dropout for determinism\n",
    "    )\n",
    "    model = MiniGPT(config)\n",
    "    model.eval()\n",
    "\n",
    "    # Same prefix, different suffix\n",
    "    tokens_a = torch.tensor([[100, 200, 300, 400]])\n",
    "    tokens_b = torch.tensor([[100, 200, 300, 999]])  # Last token different\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_a = model(tokens_a)\n",
    "        logits_b = model(tokens_b)\n",
    "\n",
    "    # Positions 0, 1, 2 should be IDENTICAL\n",
    "    for pos in range(3):\n",
    "        assert torch.allclose(logits_a[0, pos], logits_b[0, pos], atol=1e-5), \\\n",
    "            f\"Position {pos} logits differ!\"\n",
    "\n",
    "    # Position 3 SHOULD differ\n",
    "    assert not torch.allclose(logits_a[0, 3], logits_b[0, 3], atol=1e-5), \\\n",
    "        \"Position 3 logits same despite different input!\"\n",
    "\n",
    "    print(\"Causal masking PASSED!\")\n",
    "\n",
    "test_causal_masking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST 4: Weight Tying =====\n",
    "\n",
    "def test_weight_tying():\n",
    "    \"\"\"Verify embedding and lm_head share weights.\"\"\"\n",
    "    config = GPTConfig(embed_dim=64, num_heads=2, num_layers=2, d_ff=256)\n",
    "    model = MiniGPT(config)\n",
    "\n",
    "    # Should be the SAME tensor\n",
    "    assert model.lm_head.weight is model.token_embed.weight, \\\n",
    "        \"Weight tying failed: different tensors!\"\n",
    "\n",
    "    # Modify one, check the other changes\n",
    "    with torch.no_grad():\n",
    "        original = model.token_embed.weight[0, 0].item()\n",
    "        model.token_embed.weight[0, 0] = 999.0\n",
    "        \n",
    "        assert model.lm_head.weight[0, 0].item() == 999.0, \\\n",
    "            \"Weight tying failed: changes don't propagate!\"\n",
    "        \n",
    "        model.token_embed.weight[0, 0] = original\n",
    "\n",
    "    print(\"Weight tying PASSED!\")\n",
    "\n",
    "test_weight_tying()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TEST 5: Gradient Flow =====\n",
    "\n",
    "def test_gradient_flow():\n",
    "    \"\"\"Verify gradients reach all parameters.\"\"\"\n",
    "    config = GPTConfig(num_layers=2, embed_dim=64, num_heads=2, d_ff=256)\n",
    "    model = MiniGPT(config)\n",
    "\n",
    "    # Forward pass\n",
    "    tokens = torch.randint(0, config.vocab_size, (1, 8))\n",
    "    logits = model(tokens)\n",
    "\n",
    "    # Backward pass\n",
    "    loss = logits.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    # Check all parameters have gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        assert param.grad is not None, f\"No gradient for {name}\"\n",
    "\n",
    "    print(\"Gradient flow PASSED!\")\n",
    "\n",
    "test_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Your First Forward Pass\n",
    "\n",
    "Let's run real text through our model and see what it outputs with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Create small model\n",
    "config = GPTConfig(num_layers=2, embed_dim=256, num_heads=4, d_ff=1024)\n",
    "model = MiniGPT(config)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize a prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Tokens: {[tokenizer.decode([t]) for t in token_ids[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(token_ids)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Get top 5 predictions for the next token\n",
    "last_logits = logits[0, -1, :]\n",
    "top_probs, top_indices = torch.softmax(last_logits, dim=-1).topk(5)\n",
    "\n",
    "print(f\"\\nTop 5 predictions after '{prompt}':\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  '{token}': {prob:.4f}\")\n",
    "\n",
    "print(\"\\n(Random predictions - model has random weights!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6. Loading Pretrained Weights\n",
    "\n",
    "Now the exciting part: let's load real GPT-2 weights into our MiniGPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2_weights(model, model_name=\"gpt2\"):\n",
    "    \"\"\"\n",
    "    Load pretrained GPT-2 weights into our MiniGPT model.\n",
    "    \"\"\"\n",
    "    from transformers import GPT2LMHeadModel\n",
    "\n",
    "    print(f\"Loading weights from '{model_name}'...\")\n",
    "\n",
    "    # Load HuggingFace model\n",
    "    hf_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    hf_state = hf_model.state_dict()\n",
    "\n",
    "    # Our model's state dict\n",
    "    our_state = model.state_dict()\n",
    "\n",
    "    # Copy embeddings\n",
    "    our_state['token_embed.weight'].copy_(hf_state['transformer.wte.weight'])\n",
    "    our_state['pos_embed.weight'].copy_(hf_state['transformer.wpe.weight'])\n",
    "\n",
    "    # Copy each Transformer block\n",
    "    for i in range(model.config.num_layers):\n",
    "        # Layer norms\n",
    "        our_state[f'blocks.{i}.ln1.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.ln_1.weight'])\n",
    "        our_state[f'blocks.{i}.ln1.bias'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.ln_1.bias'])\n",
    "        our_state[f'blocks.{i}.ln2.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.ln_2.weight'])\n",
    "        our_state[f'blocks.{i}.ln2.bias'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.ln_2.bias'])\n",
    "\n",
    "        # Attention (need to transpose!)\n",
    "        our_state[f'blocks.{i}.attn.qkv_proj.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.attn.c_attn.weight'].T)\n",
    "        our_state[f'blocks.{i}.attn.out_proj.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.attn.c_proj.weight'].T)\n",
    "\n",
    "        # FFN (need to transpose!)\n",
    "        our_state[f'blocks.{i}.ffn.fc1.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.mlp.c_fc.weight'].T)\n",
    "        our_state[f'blocks.{i}.ffn.fc1.bias'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.mlp.c_fc.bias'])\n",
    "        our_state[f'blocks.{i}.ffn.fc2.weight'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.mlp.c_proj.weight'].T)\n",
    "        our_state[f'blocks.{i}.ffn.fc2.bias'].copy_(\n",
    "            hf_state[f'transformer.h.{i}.mlp.c_proj.bias'])\n",
    "\n",
    "    # Final layer norm\n",
    "    our_state['ln_f.weight'].copy_(hf_state['transformer.ln_f.weight'])\n",
    "    our_state['ln_f.bias'].copy_(hf_state['transformer.ln_f.bias'])\n",
    "\n",
    "    print(\"Weights loaded successfully!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_simple(model, tokenizer, prompt, max_new_tokens=20):\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Encode prompt\n",
    "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate tokens one at a time\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        next_logits = logits[:, -1, :]\n",
    "        next_token = next_logits.argmax(dim=-1, keepdim=True)\n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with GPT-2 Small config\n",
    "config = GPTConfig()  # Defaults match GPT-2 Small\n",
    "model = MiniGPT(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "model = load_gpt2_weights(model, \"gpt2\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel on device: {device}\")\n",
    "print(f\"Parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text!\n",
    "prompt = \"The quick brown fox\"\n",
    "generated = generate_simple(model, tokenizer, prompt, max_new_tokens=30)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generated: '{generated}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more prompts!\n",
    "prompts = [\n",
    "    \"Artificial intelligence will\",\n",
    "    \"Once upon a time\",\n",
    "    \"The capital of France is\",\n",
    "    \"def fibonacci(n):\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generate_simple(model, tokenizer, prompt, max_new_tokens=20)\n",
    "    print(f\"'{prompt}' -> {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 7. Random vs Pretrained Comparison\n",
    "\n",
    "Let's dramatically compare random weights vs pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random weights model\n",
    "model_random = MiniGPT(GPTConfig())\n",
    "model_random = model_random.to(device)\n",
    "\n",
    "prompt = \"Artificial intelligence will\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RANDOM WEIGHTS:\")\n",
    "print(generate_simple(model_random, tokenizer, prompt, max_new_tokens=20))\n",
    "print()\n",
    "print(\"PRETRAINED WEIGHTS:\")\n",
    "print(generate_simple(model, tokenizer, prompt, max_new_tokens=20))\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nSame architecture. Same code. Training makes all the difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "### Exercise 1: Tiny Model\n",
    "\n",
    "Build and test a tiny model with specific specs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build a tiny MiniGPT with:\n",
    "# - vocab_size=500\n",
    "# - max_seq_len=16\n",
    "# - embed_dim=64\n",
    "# - num_heads=2\n",
    "# - num_layers=2\n",
    "# - d_ff=256\n",
    "\n",
    "# 1. Create the config\n",
    "# 2. Build the model\n",
    "# 3. Run a forward pass on random tokens\n",
    "# 4. Verify output shape\n",
    "# 5. Count parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Exercise 2: Temperature Exploration\n",
    "\n",
    "Modify generation to use temperature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Modify generate_simple to accept a temperature parameter:\n",
    "# next_logits = next_logits / temperature\n",
    "# \n",
    "# Try temperatures: 0.5, 1.0, 1.5\n",
    "# How does the output change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### Exercise 3: Attention Visualization\n",
    "\n",
    "Visualize attention patterns in the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Run model with return_attention=True\n",
    "# 2. Get attention weights from the last block\n",
    "# 3. Plot a heatmap for head 0\n",
    "# Hint: Use matplotlib.pyplot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. **GPTConfig**: Clean configuration pattern\n",
    "2. **MiniGPT**: Complete language model with weight tying\n",
    "3. **Sanity checks**: 5 tests to verify correctness\n",
    "4. **Weight loading**: Transfer GPT-2 weights\n",
    "5. **Text generation**: Greedy decoding\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- `nn.ModuleList` for stacking layers\n",
    "- Weight tying saves 38M parameters\n",
    "- LM head: `(batch, seq, embed_dim)` → `(batch, seq, vocab_size)`\n",
    "- `model.train()` vs `model.eval()`\n",
    "\n",
    "**Next:** Chapter 12 will teach you to train this model from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
