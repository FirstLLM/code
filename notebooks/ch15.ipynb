{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Chapter 15: Building Applications\n\n> \"Knowledge is of no value unless you put it into practice.\"\n> — **Anton Chekhov**, Writer\n\n---\n\n## What You'll Learn\n\n- How to build a simple chat application with conversation history\n- The RAG (Retrieval-Augmented Generation) pattern for giving models access to external knowledge\n- How to chunk documents, create embeddings, and retrieve relevant information\n- Basic tool-calling patterns that let models take actions\n- Evaluation strategies for testing whether your application actually works\n\n---\n\n## Setup\n\nFirst, let's install required packages and set up **Ollama** for free local LLM inference.\n\n> **Why Ollama?** It's completely free, works offline, and runs on any computer. \n> No API keys or credit cards needed. Many production applications now use local \n> models for privacy and cost savings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q sentence-transformers numpy requests\n\n# === OLLAMA SETUP ===\n# Ollama runs locally - completely free, no API key needed!\n\nprint(\"Installing Ollama...\")\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# Start Ollama server in background\nimport subprocess\nsubprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\nimport time\ntime.sleep(3)  # Wait for server to start\n\n# Pull a small model (~2GB download, one-time)\nprint(\"\\nPulling llama3.2 model (this may take a few minutes on first run)...\")\n!ollama pull llama3.2\n\n# Download our helper library\n!wget -q https://raw.githubusercontent.com/FirstLLM/code/main/llm_helper.py\n\nprint(\"\\n✓ Setup complete! You can now use local LLMs for free.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# ===== IMPORTS =====\nimport os\nimport json\nimport re\nfrom datetime import datetime\n\nimport numpy as np\n\n# Import our LLM helper\nfrom llm_helper import chat, chat_with_history\nprint(\"✓ llm_helper loaded\")\n\n# Check if sentence-transformers is available (for RAG embeddings)\ntry:\n    from sentence_transformers import SentenceTransformer\n    print(\"✓ sentence-transformers installed\")\nexcept ImportError:\n    print(\"⚠ sentence-transformers not found. Run: pip install sentence-transformers\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# ===== TEST OLLAMA CONNECTION =====\n# Verify that Ollama is running and the model is available\n\nprint(\"Testing Ollama connection...\")\ntry:\n    response = chat(\"Say 'Connection successful!' and nothing else.\", temperature=0)\n    print(f\"✓ Ollama is working!\")\n    print(f\"  Response: {response}\")\nexcept Exception as e:\n    print(f\"⚠ Ollama connection failed: {e}\")\n    print(\"  Make sure Ollama is running: ollama serve\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Building a Chat Loop\n",
    "\n",
    "Let's start with a simple chat application that remembers conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "class ChatSession:\n    \"\"\"Chat session with history management.\"\"\"\n    \n    def __init__(self, max_history=20):\n        self.max_history = max_history\n        self.history = []\n        self.system_prompt = \"You are a helpful assistant.\"\n    \n    def count_messages(self):\n        \"\"\"Count messages in conversation history.\"\"\"\n        return len(self.history)\n    \n    def trim_history_if_needed(self):\n        \"\"\"Remove oldest messages if we're exceeding the limit.\"\"\"\n        while len(self.history) > self.max_history:\n            # Remove the oldest user/assistant pair\n            self.history.pop(0)\n            if self.history and self.history[0][\"role\"] == \"assistant\":\n                self.history.pop(0)\n    \n    def chat(self, user_message):\n        \"\"\"Process a user message and return the response.\"\"\"\n        # Add user message\n        self.history.append({\"role\": \"user\", \"content\": user_message})\n        \n        # Trim if needed\n        self.trim_history_if_needed()\n        \n        # Get response using our helper\n        assistant_message = chat_with_history(\n            self.history,\n            system=self.system_prompt\n        )\n        \n        # Add to history\n        self.history.append({\"role\": \"assistant\", \"content\": assistant_message})\n        \n        return assistant_message\n\nprint(\"ChatSession class defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Test the chat session\nsession = ChatSession()\n\n# First message\nresponse1 = session.chat(\"Hello! What's 2+2?\")\nprint(f\"You: Hello! What's 2+2?\")\nprint(f\"Assistant: {response1}\")\nprint(f\"History messages: {session.count_messages()}\\n\")\n\n# Follow-up (demonstrates memory)\nresponse2 = session.chat(\"What did I just ask you?\")\nprint(f\"You: What did I just ask you?\")\nprint(f\"Assistant: {response2}\")\nprint(f\"History messages: {session.count_messages()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. RAG: Retrieval-Augmented Generation\n",
    "\n",
    "RAG gives your model access to documents it wasn't trained on.\n",
    "\n",
    "Think of it like an **open-book exam**: the model can look things up instead of relying only on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"A simple RAG system for document retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model=\"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the RAG system.\"\"\"\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self.documents = []  # Original text chunks\n",
    "        self.embeddings = None  # NumPy array of vectors\n",
    "        self.metadata = []  # Source information for citations\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=200, overlap=50):\n",
    "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_document(self, text, source_name=\"unknown\"):\n",
    "        \"\"\"Add a document to the knowledge base.\"\"\"\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.documents.append(chunk)\n",
    "            self.metadata.append({\n",
    "                \"source\": source_name,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "        \n",
    "        # Re-embed all documents\n",
    "        self.embeddings = self.encoder.encode(\n",
    "            self.documents,\n",
    "            normalize_embeddings=True  # Important for cosine similarity\n",
    "        )\n",
    "        \n",
    "        print(f\"Added {len(chunks)} chunks from '{source_name}'\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=3, min_score=0.3):\n",
    "        \"\"\"Find the most relevant chunks for a query.\"\"\"\n",
    "        if self.embeddings is None or len(self.embeddings) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = self.encoder.encode(\n",
    "            query,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        \n",
    "        # Calculate similarities (dot product of normalized vectors = cosine)\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        # Build results, filtering by minimum score\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            score = float(similarities[idx])\n",
    "            if score >= min_score:\n",
    "                results.append({\n",
    "                    \"text\": self.documents[idx],\n",
    "                    \"score\": score,\n",
    "                    \"source\": self.metadata[idx][\"source\"],\n",
    "                    \"index\": int(idx)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"SimpleRAG class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for testing\n",
    "vacation_policy = \"\"\"\n",
    "Employees receive 15 days of paid vacation per year. \n",
    "Unused vacation days can be carried over to the next year, up to a maximum of 5 days. \n",
    "Vacation requests must be submitted at least 2 weeks in advance through the HR portal.\n",
    "New employees are eligible for vacation after completing their 90-day probation period.\n",
    "\"\"\"\n",
    "\n",
    "expense_policy = \"\"\"\n",
    "Business expenses must be submitted within 30 days of the expense date.\n",
    "All expenses over $50 require a receipt. Meals during business travel are reimbursed up to $75 per day.\n",
    "Submit expense reports through the finance portal with appropriate documentation.\n",
    "Manager approval is required for expenses over $500.\n",
    "\"\"\"\n",
    "\n",
    "remote_work_policy = \"\"\"\n",
    "Employees may work remotely up to 3 days per week with manager approval.\n",
    "Remote workers must be available during core hours: 10am to 3pm in their local timezone.\n",
    "Home office equipment can be reimbursed up to $500 with manager approval.\n",
    "Remote work arrangements should be documented in writing.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample documents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG and add documents\n",
    "rag = SimpleRAG()\n",
    "\n",
    "rag.add_document(vacation_policy, \"vacation_policy.txt\")\n",
    "rag.add_document(expense_policy, \"expense_policy.txt\")\n",
    "rag.add_document(remote_work_policy, \"remote_work_policy.txt\")\n",
    "\n",
    "print(f\"\\nTotal documents in knowledge base: {len(rag.documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "query = \"How many vacation days do I get?\"\n",
    "results = rag.retrieve(query)\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n[{i+1}] Score: {doc['score']:.3f} | Source: {doc['source']}\")\n",
    "    print(f\"    {doc['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a different query\n",
    "query2 = \"What's the expense limit for meals?\"\n",
    "results2 = rag.retrieve(query2)\n",
    "\n",
    "print(f\"Query: '{query2}'\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(results2):\n",
    "    print(f\"\\n[{i+1}] Score: {doc['score']:.3f} | Source: {doc['source']}\")\n",
    "    print(f\"    {doc['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 3. Prompting with Retrieved Context\n",
    "\n",
    "Now let's build prompts that use the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query, retrieved_docs, min_score=0.3):\n",
    "    \"\"\"Build a prompt with retrieved context and citation instructions.\"\"\"\n",
    "    # Filter by score\n",
    "    good_docs = [d for d in retrieved_docs if d[\"score\"] >= min_score]\n",
    "    \n",
    "    # Handle case where no relevant documents were found\n",
    "    if not good_docs:\n",
    "        return f\"\"\"I could not find relevant information to answer your question.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please either rephrase your question or let me know if you'd like me to answer based on general knowledge.\"\"\"\n",
    "    \n",
    "    # Build context with citation markers\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(good_docs):\n",
    "        source = doc.get(\"source\", \"unknown\")\n",
    "        context_parts.append(f\"[{i+1}] (Source: {source})\\n{doc['text']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    return f\"\"\"Use the following sources to answer the question.\n",
    "Cite sources using [1], [2], etc. Only use information from the provided sources.\n",
    "If the sources don't contain the answer, say so.\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"build_rag_prompt() defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the RAG prompt looks like\n",
    "query = \"How many vacation days do I get?\"\n",
    "docs = rag.retrieve(query)\n",
    "prompt = build_rag_prompt(query, docs)\n",
    "\n",
    "print(\"RAG PROMPT:\")\n",
    "print(\"=\" * 50)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "def rag_answer(query, rag_system):\n    \"\"\"Answer a question using RAG.\"\"\"\n    # Retrieve relevant documents\n    docs = rag_system.retrieve(query, top_k=3)\n    \n    # Build the prompt\n    prompt = build_rag_prompt(query, docs)\n    \n    # Generate response using our helper\n    response = chat(\n        prompt,\n        system=\"You are a helpful assistant that answers questions based on provided sources. Always cite your sources.\",\n        temperature=0.3  # Lower temperature for factual accuracy\n    )\n    \n    return response\n\nprint(\"rag_answer() defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Test RAG answering\nquestions = [\n    \"How many vacation days do employees get?\",\n    \"What's the meal expense limit for business travel?\",\n    \"Can I work from home?\",\n]\n\nfor q in questions:\n    print(f\"Q: {q}\")\n    answer = rag_answer(q, rag)\n    print(f\"A: {answer}\\n\")\n    print(\"-\" * 50 + \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 4. Tool Calling Basics\n",
    "\n",
    "Sometimes a model needs to do more than retrieve information. It needs to take actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available tools\n",
    "# WARNING: eval() is used here for simplicity. In production, use a proper\n",
    "# math parser library like `simpleeval` to prevent code injection attacks.\n",
    "TOOLS = {\n",
    "    \"calculate\": {\n",
    "        \"description\": \"Perform basic arithmetic. Input should be a math expression like '2 + 2' or '15 * 3'.\",\n",
    "        \"function\": lambda expr: str(eval(expr, {\"__builtins__\": {}}, {}))\n",
    "    },\n",
    "    \"get_date\": {\n",
    "        \"description\": \"Get the current date.\",\n",
    "        \"function\": lambda: datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    }\n",
    "}\n",
    "\n",
    "def parse_tool_call(response):\n",
    "    \"\"\"Extract tool call from model output.\"\"\"\n",
    "    match = re.search(r'<tool>(\\w+)\\((.*)\\)</tool>', response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2).strip()\n",
    "    return None, None\n",
    "\n",
    "def execute_tool(tool_name, argument):\n",
    "    \"\"\"Safely execute a whitelisted tool.\"\"\"\n",
    "    if tool_name not in TOOLS:\n",
    "        return f\"Error: Unknown tool '{tool_name}'\"\n",
    "    \n",
    "    try:\n",
    "        if argument:\n",
    "            result = TOOLS[tool_name][\"function\"](argument)\n",
    "        else:\n",
    "            result = TOOLS[tool_name][\"function\"]()\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error executing {tool_name}: {e}\"\n",
    "\n",
    "print(\"Tool functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "def chat_with_tools(user_message):\n    \"\"\"Chat with tool-calling capability.\"\"\"\n    \n    # Build tool descriptions\n    tool_descriptions = \"\\n\".join(\n        f\"- {name}: {info['description']}\"\n        for name, info in TOOLS.items()\n    )\n    \n    # First turn: ask the model\n    prompt = f\"\"\"You have access to these tools:\n{tool_descriptions}\n\nTo use a tool, write: <tool>name(argument)</tool>\nOnly use a tool if you need it to answer the question.\n\nUser: {user_message}\nAssistant:\"\"\"\n    \n    first_response = chat(prompt, temperature=0)  # Deterministic for reliable parsing\n    print(f\"Model's first response: {first_response}\")\n    \n    # Check if model wants to use a tool\n    tool_name, argument = parse_tool_call(first_response)\n    \n    if tool_name:\n        # Execute the tool\n        tool_result = execute_tool(tool_name, argument)\n        print(f\"Tool executed: {tool_name}({argument}) = {tool_result}\")\n        \n        # Second turn: give result back to model\n        followup = f\"\"\"{prompt}{first_response}\n\nTool result: {tool_result}\n\nNow provide your final answer to the user:\"\"\"\n        \n        final_response = chat(followup, temperature=0.3)\n        return final_response\n    \n    # No tool needed, return first response\n    return first_response\n\nprint(\"chat_with_tools() defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Test tool calling\nprint(\"Testing tool calling...\\n\")\n\n# Test calculation\nprint(\"Q: What is 15% of 847?\")\nanswer = chat_with_tools(\"What is 15% of 847?\")\nprint(f\"Final answer: {answer}\\n\")\nprint(\"-\" * 50)\n\n# Test date\nprint(\"\\nQ: What's today's date?\")\nanswer = chat_with_tools(\"What's today's date?\")\nprint(f\"Final answer: {answer}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Testing\n",
    "\n",
    "How do you know if your RAG system is working well? Build an evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response, expected_traits):\n",
    "    \"\"\"Evaluate a response against expected traits.\"\"\"\n",
    "    results = {}\n",
    "    response_lower = response.lower()\n",
    "    \n",
    "    # Check for citation markers\n",
    "    if \"cites_source\" in expected_traits:\n",
    "        has_citation = bool(re.search(r'\\[\\d+\\]', response))\n",
    "        results[\"cites_source\"] = (has_citation == expected_traits[\"cites_source\"])\n",
    "    \n",
    "    # Check for required keywords\n",
    "    if \"contains_keywords\" in expected_traits:\n",
    "        keywords = expected_traits[\"contains_keywords\"]\n",
    "        all_present = all(kw.lower() in response_lower for kw in keywords)\n",
    "        results[\"contains_keywords\"] = all_present\n",
    "    \n",
    "    # Check it's not a refusal\n",
    "    if \"not_refusal\" in expected_traits:\n",
    "        refusal_phrases = [\"i don't know\", \"i cannot\", \"not in the sources\", \"no relevant\"]\n",
    "        is_refusal = any(phrase in response_lower for phrase in refusal_phrases)\n",
    "        if expected_traits[\"not_refusal\"]:\n",
    "            results[\"not_refusal\"] = not is_refusal\n",
    "        else:\n",
    "            results[\"not_refusal\"] = is_refusal\n",
    "    \n",
    "    # Check minimum length\n",
    "    if \"min_words\" in expected_traits:\n",
    "        word_count = len(response.split())\n",
    "        results[\"min_words\"] = (word_count >= expected_traits[\"min_words\"])\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"evaluate_response() defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation set\n",
    "eval_set = [\n",
    "    {\n",
    "        \"query\": \"How many vacation days do employees get?\",\n",
    "        \"expected_traits\": {\n",
    "            \"contains_keywords\": [\"15\", \"days\"],\n",
    "            \"cites_source\": True,\n",
    "            \"not_refusal\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the expense limit for meals?\",\n",
    "        \"expected_traits\": {\n",
    "            \"contains_keywords\": [\"75\"],\n",
    "            \"cites_source\": True,\n",
    "            \"not_refusal\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the meaning of life?\",  # Out of scope!\n",
    "        \"expected_traits\": {\n",
    "            \"not_refusal\": False,  # Should refuse\n",
    "            \"cites_source\": False\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Evaluation set: {len(eval_set)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(rag_system, eval_set):\n",
    "    \"\"\"Run full evaluation and report results.\"\"\"\n",
    "    total_tests = 0\n",
    "    passed_tests = 0\n",
    "    \n",
    "    for case in eval_set:\n",
    "        print(f\"\\nQuery: {case['query']}\")\n",
    "        \n",
    "        response = rag_answer(case[\"query\"], rag_system)\n",
    "        results = evaluate_response(response, case[\"expected_traits\"])\n",
    "        \n",
    "        for trait, passed in results.items():\n",
    "            total_tests += 1\n",
    "            if passed:\n",
    "                passed_tests += 1\n",
    "                print(f\"  [PASS] {trait}\")\n",
    "            else:\n",
    "                print(f\"  [FAIL] {trait}\")\n",
    "        \n",
    "        print(f\"  Response: {response[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Results: {passed_tests}/{total_tests} tests passed ({100*passed_tests/total_tests:.1f}%)\")\n",
    "\n",
    "print(\"run_evaluation() defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": "# Run evaluation\nprint(\"Running RAG evaluation...\")\nrun_evaluation(rag, eval_set)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "Streaming makes responses feel faster by showing output as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "def stream_response(query, rag_system):\n    \"\"\"Stream a RAG response using Ollama.\"\"\"\n    import requests\n    \n    docs = rag_system.retrieve(query, top_k=3)\n    prompt = build_rag_prompt(query, docs)\n    \n    # Use Ollama's streaming endpoint directly\n    response = requests.post(\n        \"http://localhost:11434/api/chat\",\n        json={\n            \"model\": \"llama3.2\",\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"stream\": True\n        },\n        stream=True\n    )\n    \n    full_response = \"\"\n    for line in response.iter_lines():\n        if line:\n            data = json.loads(line)\n            if \"message\" in data and \"content\" in data[\"message\"]:\n                content = data[\"message\"][\"content\"]\n                print(content, end=\"\", flush=True)\n                full_response += content\n    \n    print()  # Newline at the end\n    return full_response\n\nprint(\"stream_response() defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# Test streaming\nprint(\"Testing streaming response...\\n\")\nprint(\"Q: How many days can I work remotely?\\n\")\nprint(\"A: \", end=\"\")\nstream_response(\"How many days can I work remotely?\", rag)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Build Your Own Knowledge Base\n",
    "\n",
    "Create a knowledge base about a topic you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Write 10 short documents about a topic (recipes, game rules, class notes)\n",
    "# 2. Initialize a SimpleRAG instance\n",
    "# 3. Add your documents\n",
    "# 4. Test with 5 questions\n",
    "# 5. Observe: Does it retrieve the right chunks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Exercise 2: Temperature Experiment\n",
    "\n",
    "Test how temperature affects RAG responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Modify rag_answer() to accept a temperature parameter\n",
    "# 2. Ask the same question with temperatures 0.3, 0.7, 1.0\n",
    "# 3. Compare the responses\n",
    "# 4. Which is most reliable for factual questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Exercise 3: Evaluation Set Creation\n",
    "\n",
    "Build a comprehensive evaluation set for your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create 10 evaluation questions for your knowledge base\n",
    "# 2. Include: 5 easy, 3 hard, 2 adversarial (out of scope)\n",
    "# 3. Define expected traits for each\n",
    "# 4. Run evaluation and report accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Exercise 4: Checkpoint - Personal Knowledge Assistant\n",
    "\n",
    "Build a complete \"chat with your documents\" application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Collect 5-10 text files (notes, articles, documentation)\n",
    "# 2. Load them into a SimpleRAG instance\n",
    "# 3. Build a chat loop that uses RAG for every response\n",
    "# 4. Include: token counting, history management, citations\n",
    "# 5. Create a 10-question evaluation set\n",
    "# 6. Run evaluation and report accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "- A chat loop with history management and token budgeting\n",
    "- A complete RAG system from document chunking to citation generation\n",
    "- A preview of tool calling with safety considerations\n",
    "- Evaluation and validation for production readiness\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "- Your training knowledge (embeddings, chunking, data quality, reproducibility) transfers directly to applications\n",
    "- RAG solves the \"model doesn't know my data\" problem\n",
    "- Tool calling extends what models can do, but requires careful safety\n",
    "- Evaluation is essential, not optional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}