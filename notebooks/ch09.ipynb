{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: The Embedding Layer\n",
    "\n",
    "> \"You shall know a word by the company it keeps.\" — **John Rupert Firth**, Linguist\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Why token IDs alone aren't enough and what embeddings actually solve\n",
    "- How to build token embeddings from scratch using lookup tables\n",
    "- Why positional information matters and how to add it\n",
    "- How to combine token and position embeddings for complete input representations\n",
    "- How to explore real model embeddings and discover semantic relationships\n",
    "- Practical initialization and implementation considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== IMPORTS =====\nimport torch                     # PyTorch: tensor operations and neural networks\nimport torch.nn as nn            # Neural network modules (layers, etc.)\nimport torch.nn.functional as F  # Mathematical functions (cosine_similarity, etc.)\nfrom transformers import AutoModel, AutoTokenizer  # Pre-trained models\n\n# Quick tensor reminder:\n# - torch.tensor([1,2,3]) creates a 1D tensor (like a list)\n# - torch.randn(3, 4) creates a 3×4 tensor of random numbers\n# - tensor[0] indexes into the first dimension\n# - tensor.shape tells you the dimensions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Can't We Just Use Token IDs?\n",
    "\n",
    "Token IDs are arbitrary integers with no semantic meaning. Let's see the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token IDs are just integers - no relationships\n",
    "token_ids = {\n",
    "    \"cat\": 3797,\n",
    "    \"kitten\": 28387,\n",
    "    \"dog\": 4273,\n",
    "    \"car\": 1097\n",
    "}\n",
    "\n",
    "print(\"Token IDs:\")\n",
    "for word, id in token_ids.items():\n",
    "    print(f\"  '{word}' → {id}\")\n",
    "\n",
    "# Problem: \"cat\" is numerically closer to \"dog\" than to \"kitten\"!\n",
    "print(f\"\\nDistance from 'cat' to 'dog': {abs(3797 - 4273)}\")\n",
    "print(f\"Distance from 'cat' to 'kitten': {abs(3797 - 28387)}\")\n",
    "print(\"\\nBut semantically, 'cat' and 'kitten' are more similar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Solution: Embeddings\n\nConvert each token ID to a dense vector that captures meaning.\n\n**What is `nn.Embedding`?**\n- A lookup table with `vocab_size` rows and `embed_dim` columns\n- Each row is a vector representing one token\n- Input: token ID (integer) → Output: that row (vector)\n- Think of it as a dictionary: `{0: [0.1, 0.2, ...], 1: [0.5, -0.1, ...], ...}`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== The Problem: Token IDs =====\n",
    "token_ids = torch.tensor([3797, 28387, 4273])  # cat, kitten, dog\n",
    "print(f\"Token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Just integers: {token_ids}\")\n",
    "\n",
    "# ===== The Solution: Embeddings =====\n",
    "vocab_size = 50257  # GPT-2 vocabulary\n",
    "embed_dim = 768     # GPT-2 embedding dimension\n",
    "\n",
    "# Create embedding layer (this is a lookup table!)\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "# Look up vectors for our token IDs\n",
    "token_vectors = embedding(token_ids)\n",
    "print(f\"\\nToken embeddings shape: {token_vectors.shape}\")\n",
    "print(f\"First token's vector (first 10 dims): {token_vectors[0, :10]}\")\n",
    "\n",
    "# Now each token is a 768-dimensional vector that can capture meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Token Embeddings: The Lookup Table\n",
    "\n",
    "Let's build token embeddings from scratch to understand the mechanism.\n",
    "\n",
    "### Step 1: Create the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "embed_dim = 768     # Embedding dimension\n",
    "\n",
    "# Create embedding matrix: one row per token\n",
    "embedding_matrix = torch.randn(vocab_size, embed_dim)\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"\\nToken 3797's embedding (first 10 dims): {embedding_matrix[3797, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Look Up Multiple Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([464, 3797, 3332])  # \"The cat sat\"\n",
    "\n",
    "# Manual lookup (what embedding layers do internally)\n",
    "embeddings = embedding_matrix[token_ids]\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")       # torch.Size([3])\n",
    "print(f\"Output shape: {embeddings.shape}\")     # torch.Size([3, 768])\n",
    "\n",
    "# Each token ID → its 768-dimensional vector\n",
    "print(f\"\\nToken 464's embedding (first 5 dims): {embeddings[0, :5]}\")\n",
    "print(f\"Token 3797's embedding (first 5 dims): {embeddings[1, :5]}\")\n",
    "print(f\"Token 3332's embedding (first 5 dims): {embeddings[2, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Handle Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of 2 sequences, each with 4 tokens\n",
    "token_ids_batch = torch.tensor([\n",
    "    [464, 3797, 3332, 319],    # Sequence 1: \"The cat sat on\"\n",
    "    [314, 588, 4695, 345]      # Sequence 2: \"I will help you\"\n",
    "])\n",
    "\n",
    "print(f\"Batch shape: {token_ids_batch.shape}\")  # torch.Size([2, 4])\n",
    "\n",
    "# Look up embeddings for entire batch\n",
    "embeddings_batch = embedding_matrix[token_ids_batch]\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_batch.shape}\")  # torch.Size([2, 4, 768])\n",
    "print(\"\\nShape transformation: (batch, seq) → (batch, seq, embed_dim)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use PyTorch's nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Token embedding layer: converts token IDs to dense vectors.\n",
    "    \n",
    "    This is what GPT-2's 'wte' (word token embeddings) layer does.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # Create the embedding matrix as a learnable parameter\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Initialize with small random values (GPT-2 style)\n",
    "        nn.init.normal_(self.embedding.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch, seq) tensor of token IDs\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: (batch, seq, embed_dim) tensor of token vectors\n",
    "        \"\"\"\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "# Create token embedding layer\n",
    "token_embed = TokenEmbedding(vocab_size=50257, embed_dim=768)\n",
    "\n",
    "# Embed a batch\n",
    "token_ids = torch.tensor([[464, 3797, 3332, 319]])  # Shape: (1, 4)\n",
    "embeddings = token_embed(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")        # torch.Size([1, 4])\n",
    "print(f\"Output shape: {embeddings.shape}\")      # torch.Size([1, 4, 768])\n",
    "print(f\"\\nFirst token embedding (first 5 dims): {embeddings[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Embeddings: Teaching Position\n",
    "\n",
    "Token embeddings have no sense of position. Let's see the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two sequences with same tokens, different order\n",
    "token_ids_1 = torch.tensor([[464, 3797, 3332]])  # \"The cat sat\"\n",
    "token_ids_2 = torch.tensor([[3332, 3797, 464]])  # \"sat cat The\"\n",
    "\n",
    "# Get token embeddings\n",
    "token_embed = TokenEmbedding(vocab_size=50257, embed_dim=768)\n",
    "embeddings_1 = token_embed(token_ids_1)\n",
    "embeddings_2 = token_embed(token_ids_2)\n",
    "\n",
    "print(f\"Embeddings 1 shape: {embeddings_1.shape}\")\n",
    "print(f\"Embeddings 2 shape: {embeddings_2.shape}\")\n",
    "\n",
    "# They're different...\n",
    "print(f\"\\nAre embeddings identical? {torch.equal(embeddings_1, embeddings_2)}\")\n",
    "\n",
    "# But if you sort both, they contain the same vectors!\n",
    "# This is the problem: without position information,\n",
    "# \"The cat sat\" and \"sat cat The\" look the same to attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Learned Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings: one trainable vector per position.\n",
    "    \n",
    "    GPT-2 uses this approach (called 'wpe' - word position embeddings).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len: Maximum sequence length (e.g., 1024 for GPT-2)\n",
    "            embed_dim: Embedding dimension (must match token embeddings)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create position embedding matrix: (max_seq_len, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \n",
    "        # Initialize with small random values (GPT-2 style)\n",
    "        nn.init.normal_(self.pos_embed.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch, seq) tensor of token IDs\n",
    "        \n",
    "        Returns:\n",
    "            pos_embeddings: (batch, seq, embed_dim) tensor of position vectors\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Validate sequence length\n",
    "        if seq_len > self.max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length {seq_len} exceeds max_seq_len {self.max_seq_len}\"\n",
    "            )\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        position_ids = torch.arange(\n",
    "            seq_len,\n",
    "            device=token_ids.device  # Match device (CPU/GPU) of input\n",
    "        )\n",
    "        \n",
    "        # Expand for batch: (seq_len,) → (batch, seq_len)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \n",
    "        # Look up position embeddings\n",
    "        pos_embeddings = self.pos_embed(position_ids)\n",
    "        \n",
    "        return pos_embeddings\n",
    "\n",
    "# Create positional embedding layer\n",
    "pos_embed = PositionalEmbedding(max_seq_len=1024, embed_dim=768)\n",
    "\n",
    "# Example: sequence of length 4\n",
    "token_ids = torch.tensor([[464, 3797, 3332, 319]])  # Shape: (1, 4)\n",
    "pos_embeddings = pos_embed(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")           # torch.Size([1, 4])\n",
    "print(f\"Position embeddings shape: {pos_embeddings.shape}\")  # torch.Size([1, 4, 768])\n",
    "\n",
    "# Each position gets its own learned vector\n",
    "print(f\"\\nPosition 0 embedding (first 5 dims): {pos_embeddings[0, 0, :5]}\")\n",
    "print(f\"Position 1 embedding (first 5 dims): {pos_embeddings[0, 1, :5]}\")\n",
    "print(f\"Position 2 embedding (first 5 dims): {pos_embeddings[0, 2, :5]}\")\n",
    "print(f\"Position 3 embedding (first 5 dims): {pos_embeddings[0, 3, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Token + Position Embeddings\n",
    "\n",
    "### Building the Complete GPT2Embeddings Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GPT2Embeddings(nn.Module):\n    \"\"\"\n    Complete GPT-2 embedding layer: token + position embeddings.\n    \n    This matches GPT-2's 'wte' + 'wpe' layers.\n    \"\"\"\n    def __init__(self, vocab_size, max_seq_len, embed_dim):\n        \"\"\"\n        Args:\n            vocab_size: Size of vocabulary (50257 for GPT-2)\n            max_seq_len: Maximum sequence length (1024 for GPT-2)\n            embed_dim: Embedding dimension (768 for GPT-2 Small)\n        \"\"\"\n        super().__init__()\n        \n        # Token embeddings: vocab_size × embed_dim\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        \n        # Position embeddings: max_seq_len × embed_dim\n        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n        \n        # Initialize both with GPT-2's standard initialization\n        nn.init.normal_(self.token_embed.weight, mean=0.0, std=0.02)\n        nn.init.normal_(self.pos_embed.weight, mean=0.0, std=0.02)\n        \n        self.max_seq_len = max_seq_len\n    \n    def forward(self, token_ids):\n        \"\"\"\n        Args:\n            token_ids: (batch, seq) tensor of token IDs\n        \n        Returns:\n            embeddings: (batch, seq, embed_dim) tensor of combined embeddings\n        \"\"\"\n        batch_size, seq_len = token_ids.shape\n        \n        # Validate sequence length\n        if seq_len > self.max_seq_len:\n            raise ValueError(\n                f\"Sequence length {seq_len} exceeds max_seq_len {self.max_seq_len}\"\n            )\n        \n        # ===== Token Embeddings =====\n        token_embeddings = self.token_embed(token_ids)\n        \n        # ===== Position Embeddings =====\n        position_ids = torch.arange(seq_len, device=token_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_len)\n        position_embeddings = self.pos_embed(position_ids)\n        \n        # ===== Combine via Addition =====\n        embeddings = token_embeddings + position_embeddings\n        \n        return embeddings\n\n# Create GPT-2 Small embedding layer\ngpt2_embed = GPT2Embeddings(\n    vocab_size=50257,\n    max_seq_len=1024,\n    embed_dim=768\n)\n\n# Example: embed a batch of sequences (BOTH must have same length!)\n# Shorter sequences are padded with 0s to match the longest\ntoken_ids = torch.tensor([\n    [464, 3797, 3332, 319, 0, 0],  # \"The cat sat on\" + padding\n    [314, 588, 4695, 345, 0, 0]    # \"I will help you\" + padding\n])\n\nembeddings = gpt2_embed(token_ids)\n\nprint(f\"Input shape: {token_ids.shape}\")         # torch.Size([2, 6])\nprint(f\"Output shape: {embeddings.shape}\")       # torch.Size([2, 6, 768])\nprint(f\"Each token now has: {embeddings.shape[-1]} dimensions\")\n\nprint(f\"\\nFirst sequence, first token (first 10 dims):\")\nprint(embeddings[0, 0, :10])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Chapter 8: The Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ===== Step 1: Tokenize (Chapter 8) =====\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"The cat sat on the mat\"\n",
    "token_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Step 1: Tokenization\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Shape: {token_ids.shape}\\n\")\n",
    "\n",
    "# ===== Step 2: Embed (Chapter 9) =====\n",
    "gpt2_embed = GPT2Embeddings(vocab_size=50257, max_seq_len=1024, embed_dim=768)\n",
    "embeddings = gpt2_embed(token_ids)\n",
    "\n",
    "print(\"Step 2: Embedding\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First token embedding (first 10 dims): {embeddings[0, 0, :10]}\\n\")\n",
    "\n",
    "print(\"Step 3: Next up — Attention layers (Chapter 10)\")\n",
    "print(f\"These {embeddings.shape} embeddings will flow into self-attention,\")\n",
    "print(\"where tokens learn from each other's context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring GPT-2's Embeddings\n",
    "\n",
    "Load a real pretrained GPT-2 model and explore what it learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Small\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Access token embeddings\n",
    "token_embeddings = model.wte.weight  # Word Token Embeddings\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "# torch.Size([50257, 768]) — one 768-dim vector per token\n",
    "\n",
    "# Access position embeddings\n",
    "position_embeddings = model.wpe.weight  # Word Position Embeddings\n",
    "print(f\"Position embeddings shape: {position_embeddings.shape}\")\n",
    "# torch.Size([1024, 768]) — one 768-dim vector per position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Finding Similar Words\n\n**What is Cosine Similarity?**\nMeasures how similar two vectors are based on the angle between them:\n- **1.0** = identical direction (very similar)\n- **0.0** = perpendicular (unrelated)\n- **-1.0** = opposite direction (opposite meaning)\n\nThink of it as: \"are these two arrows pointing the same way?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tokens(word, embeddings, tokenizer, top_k=5):\n",
    "    \"\"\"Find tokens with embeddings most similar to the given word.\"\"\"\n",
    "    # Get token ID for the word\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "    target_vec = embeddings[token_id]\n",
    "    \n",
    "    # Compute cosine similarity with all tokens\n",
    "    similarities = F.cosine_similarity(\n",
    "        target_vec.unsqueeze(0),  # (1, 768)\n",
    "        embeddings,               # (50257, 768)\n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Get top-k most similar (excluding the word itself)\n",
    "    top_indices = similarities.argsort(descending=True)[1:top_k+1]\n",
    "    \n",
    "    print(f\"\\nWords most similar to '{word}':\")\n",
    "    for idx in top_indices:\n",
    "        token = tokenizer.decode([idx])\n",
    "        score = similarities[idx].item()\n",
    "        print(f\"  {score:.3f} — '{token}'\")\n",
    "\n",
    "# Explore semantic relationships\n",
    "find_similar_tokens(\"king\", token_embeddings, tokenizer, top_k=8)\n",
    "find_similar_tokens(\"computer\", token_embeddings, tokenizer, top_k=8)\n",
    "find_similar_tokens(\"happy\", token_embeddings, tokenizer, top_k=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Your Own Explorations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different words:\n",
    "words_to_explore = [\"Python\", \"doctor\", \"fast\", \"beautiful\"]\n",
    "\n",
    "for word in words_to_explore:\n",
    "    find_similar_tokens(word, token_embeddings, tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Word Analogies\n",
    "\n",
    "Can we find vectors that satisfy \"king - man + woman ≈ queen\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word1, word2, word3, embeddings, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Find: word1 - word2 + word3 ≈ ?\n",
    "    Example: king - man + woman ≈ queen\n",
    "    \"\"\"\n",
    "    # Get token IDs\n",
    "    id1 = tokenizer.encode(word1, add_special_tokens=False)[0]\n",
    "    id2 = tokenizer.encode(word2, add_special_tokens=False)[0]\n",
    "    id3 = tokenizer.encode(word3, add_special_tokens=False)[0]\n",
    "    \n",
    "    # Compute target vector: word1 - word2 + word3\n",
    "    target_vec = embeddings[id1] - embeddings[id2] + embeddings[id3]\n",
    "    \n",
    "    # Find most similar tokens\n",
    "    similarities = F.cosine_similarity(\n",
    "        target_vec.unsqueeze(0),\n",
    "        embeddings,\n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "    # Exclude the input words from results\n",
    "    similarities[id1] = -1\n",
    "    similarities[id2] = -1\n",
    "    similarities[id3] = -1\n",
    "    \n",
    "    top_indices = similarities.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    print(f\"\\n{word1} - {word2} + {word3} ≈ ?\")\n",
    "    for idx in top_indices:\n",
    "        token = tokenizer.decode([idx])\n",
    "        score = similarities[idx].item()\n",
    "        print(f\"  {score:.3f} — '{token}'\")\n",
    "\n",
    "# Classic example: king - man + woman ≈ queen\n",
    "word_analogy(\"king\", \"man\", \"woman\", token_embeddings, tokenizer, top_k=5)\n",
    "\n",
    "# Try others!\n",
    "word_analogy(\"Paris\", \"France\", \"Germany\", token_embeddings, tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Manual Embedding Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a tiny vocabulary (10 tokens) and embedding dimension of 8\n# Manually create an embedding matrix and look up embeddings for token IDs [2, 5, 7]\n# Print the shapes at each step\n\n# Step 1: Create embedding matrix\nvocab_size = 10\nembed_dim = 8\nembedding_matrix = torch.randn(???, ???)  # Fill in the sizes\nprint(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n\n# Step 2: Create token IDs to look up\ntoken_ids = torch.tensor([2, 5, 7])\nprint(f\"Token IDs: {token_ids}\")\n\n# Step 3: Look up embeddings (hint: use indexing like embedding_matrix[...])\nembeddings = ???\nprint(f\"Embeddings shape: {embeddings.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build TokenEmbedding from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the TokenEmbedding class without looking at the example\n",
    "# Include proper initialization and shape validation\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of token IDs: [10, 20, 30, 40, 50]\n",
    "# Generate position indices and look them up in a position embedding layer\n",
    "# Verify that position 0 always gets the same vector regardless of token\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Complete GPT2Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the full GPT2Embeddings class\n",
    "# Test it with:\n",
    "# - A batch of 2 sequences\n",
    "# - Different sequence lengths (one length 5, one length 8 with padding)\n",
    "# - Verify output shape is (2, 8, embed_dim)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Explore GPT-2 Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 and find tokens similar to:\n",
    "# - \"Python\" (should find programming-related words)\n",
    "# - \"doctor\" (should find medical/professional words)\n",
    "# - \"fast\" (should find speed-related words)\n",
    "#\n",
    "# Does the model group related concepts together?\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Device Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPT2Embeddings instance\n",
    "# Move it to GPU (if available)\n",
    "# Embed token IDs that start on CPU\n",
    "# What happens? Fix it by moving token IDs to GPU first\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Sequence Length Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPT2Embeddings with max_seq_len=10\n",
    "# Try to embed a sequence of length 15\n",
    "# Handle the error gracefully by truncating the sequence before embedding\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Chapter 8 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the JSONL output from Chapter 8 (your tokenized dataset)\n",
    "# Load one record, extract the token IDs, convert to PyTorch tensor\n",
    "# Embed it using GPT2Embeddings\n",
    "# Print the shape at each step\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. **Token embeddings:** Lookup table converting token IDs to semantic vectors\n",
    "2. **Positional embeddings:** Learned vectors encoding position in sequence\n",
    "3. **Complete GPT2Embeddings:** Combines token + position via addition\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "- Token IDs are arbitrary indices with no semantic meaning\n",
    "- Embeddings convert IDs to dense vectors that capture relationships\n",
    "- Position information is critical (\"cat chased dog\" ≠ \"dog chased cat\")\n",
    "- GPT-2 uses learned positional embeddings (trainable parameters)\n",
    "- Cosine similarity reveals learned semantic relationships\n",
    "- Real models learn that \"king\" and \"queen\" are related without being told!\n",
    "\n",
    "**Next:** Chapter 10 will use these embeddings for self-attention, where tokens learn from each other's context!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}