{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Attention Is All You Need\n",
    "\n",
    "> \"Attention is All You Need.\" — **Vaswani et al.**, Google Research, 2017\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Why static embeddings aren't enough (the \"bank\" in \"river bank\" vs \"savings bank\")\n",
    "- What attention IS at a conceptual level (tokens looking at each other)\n",
    "- How to build self-attention step-by-step from Query, Key, Value concepts\n",
    "- Why we scale attention scores and mask future tokens\n",
    "- How to go from single-head to multi-head attention efficiently\n",
    "- How to combine attention with feedforward networks into complete Transformer blocks\n",
    "- How to visualize what attention patterns emerge\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== IMPORTS =====\nimport torch                     # PyTorch: tensor operations\nimport torch.nn as nn            # Neural network layers\nimport torch.nn.functional as F  # Mathematical functions (softmax, gelu)\nimport math                      # For sqrt in attention scaling\nimport matplotlib.pyplot as plt  # Visualization\nimport numpy as np               # Array operations\n\n# Key building blocks we'll use:\n# - nn.Linear(in, out): Matrix multiplication layer (learns weights)\n# - F.softmax(x, dim): Converts scores to probabilities (sum to 1)\n# - @ operator: Matrix multiplication (same as torch.matmul)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Static Embeddings Aren't Enough\n",
    "\n",
    "The problem: Embeddings from Chapter 9 are **static**—each token gets the same vector regardless of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate output from Chapter 9 GPT2Embeddings\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embed_dim = 768\n",
    "\n",
    "# Embeddings from Ch9 (random for this example, but imagine they're real)\n",
    "embeddings = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"Input embeddings shape: {embeddings.shape}\")\n",
    "# Expected output: torch.Size([2, 6, 768])\n",
    "\n",
    "# These are STATIC embeddings from Ch9\n",
    "# Our goal: Transform them into CONTEXT-AWARE embeddings\n",
    "\n",
    "print(\"\\nThe word 'bank' always gets the same vector:\")\n",
    "print(\"- 'river bank' → same embedding\")\n",
    "print(\"- 'savings bank' → same embedding\")\n",
    "print(\"- But they mean completely different things!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Building Self-Attention Step-by-Step\n\nLet's build attention incrementally, showing shapes at every step.\n\n**The Intuition:** Each token asks \"Which other tokens are relevant to understanding me?\" \n- **Query (Q)**: \"What am I looking for?\"\n- **Key (K)**: \"What do I offer?\"\n- **Value (V)**: \"My information to share\"\n\nThink of it like a search engine: Query is your search, Keys are the titles/tags of documents, Values are the actual content.\n\n### Step 1: Create Query, Key, Value Projections\n\n**What is `nn.Linear(in_dim, out_dim)`?**\n- Creates a weight matrix of shape (in_dim, out_dim)\n- When you pass input through it: `output = input @ weight`\n- These weights are \"learnable\" — they get updated during training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a smaller dimension for clarity\n",
    "d_model = 768  # From embeddings (Ch9)\n",
    "d_k = 64       # Dimension for Q, K, V (typical: d_model / num_heads)\n",
    "\n",
    "# Create projection layers (these have learnable parameters!)\n",
    "W_q = nn.Linear(d_model, d_k, bias=False)  # Query projection\n",
    "W_k = nn.Linear(d_model, d_k, bias=False)  # Key projection\n",
    "W_v = nn.Linear(d_model, d_k, bias=False)  # Value projection\n",
    "\n",
    "# Project embeddings to Q, K, V\n",
    "# Why Linear? It learns the best transformation for each role\n",
    "Q = W_q(embeddings)  # (batch, seq, d_k) = (2, 6, 64)\n",
    "K = W_k(embeddings)  # (batch, seq, d_k) = (2, 6, 64)\n",
    "V = W_v(embeddings)  # (batch, seq, d_k) = (2, 6, 64)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")  # Expected: torch.Size([2, 6, 64])\n",
    "print(f\"K shape: {K.shape}\")  # Expected: torch.Size([2, 6, 64])\n",
    "print(f\"V shape: {V.shape}\")  # Expected: torch.Size([2, 6, 64])\n",
    "\n",
    "print(\"\\nEach token now has:\")\n",
    "print(\"- Q vector (64 dims): 'What I'm looking for'\")\n",
    "print(\"- K vector (64 dims): 'What I offer'\")\n",
    "print(\"- V vector (64 dims): 'My information to share'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute Attention Scores (Q · K^T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute attention scores: Q @ K^T\n# We need to transpose K so dimensions align for matmul\n\n# Q shape: (batch, seq, d_k) = (2, 6, 64)\n# K shape: (batch, seq, d_k) = (2, 6, 64)\n\n# K.transpose(-2, -1) swaps the last two dimensions:\n# Negative indices: -1 = last dim, -2 = second-to-last\n# So K goes from (2, 6, 64) → (2, 64, 6)\n\n# Matrix multiplication: (2, 6, 64) @ (2, 64, 6) → (2, 6, 6)\nscores = Q @ K.transpose(-2, -1)\n\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"K shape: {K.shape}\")\nprint(f\"K transposed shape: {K.transpose(-2, -1).shape}\")\nprint(f\"Attention scores shape: {scores.shape}\")\n# Expected: torch.Size([2, 6, 6])\n\nprint(f\"\\nScores for first item in batch:\")\nprint(scores[0])\nprint(\"\\n6×6 matrix where entry [i,j] = how much token i attends to token j\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scale the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale by sqrt(dimension)\n",
    "# Why sqrt? Math proof shows this keeps variance stable\n",
    "scores = scores / math.sqrt(d_k)\n",
    "\n",
    "print(f\"Scaled scores shape: {scores.shape}\")  # Still (2, 6, 6)\n",
    "print(f\"\\nBefore scaling, score range might be: ±{d_k}\")\n",
    "print(f\"After scaling by sqrt({d_k}) = {math.sqrt(d_k):.2f}, range is roughly: ±8\")\n",
    "\n",
    "print(\"\\nWhy this matters: Without scaling, high-dimensional attention\")\n",
    "print(\"would put almost all weight on one token, losing the benefit\")\n",
    "print(\"of attending to multiple tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Apply Softmax to Get Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply softmax over the last dimension (across keys)\n",
    "# This makes each row (each query) sum to 1\n",
    "attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # Expected: (2, 6, 6)\n",
    "print(f\"\\nAttention weights for token 0 (first batch):\")\n",
    "print(attn_weights[0, 0])\n",
    "# Example output: tensor([0.15, 0.20, 0.30, 0.18, 0.10, 0.07])\n",
    "# These sum to 1.0!\n",
    "\n",
    "print(f\"\\nSum of weights for token 0: {attn_weights[0, 0].sum().item():.4f}\")\n",
    "# Expected: Sum of weights for token 0: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Weighted Sum of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights: (batch, seq, seq) = (2, 6, 6)\n",
    "# Values:           (batch, seq, d_k)  = (2, 6, 64)\n",
    "# We want:          (batch, seq, d_k)  = (2, 6, 64)\n",
    "\n",
    "output = attn_weights @ V\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: torch.Size([2, 6, 64])\n",
    "\n",
    "print(f\"\\nOriginal embedding for token 0 (first 10 dims):\")\n",
    "print(embeddings[0, 0, :10])\n",
    "\n",
    "print(f\"\\nOutput after attention for token 0 (first 10 dims):\")\n",
    "print(output[0, 0, :10])\n",
    "print(\"\\nDifferent values! This token has incorporated context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Attention Function\n",
    "\n",
    "Let's package the 5 steps into one function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (batch, seq, d_k)\n",
    "        K: Keys    (batch, seq, d_k)\n",
    "        V: Values  (batch, seq, d_k)\n",
    "        mask: Optional mask (batch, seq, seq)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch, seq, d_k)\n",
    "        attn_weights: Attention weights (batch, seq, seq)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # Get dimension of queries/keys\n",
    "    \n",
    "    # Step 1: Compute scores Q @ K^T\n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch, seq, seq)\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Softmax to get attention weights\n",
    "    attn_weights = F.softmax(scores, dim=-1)  # (batch, seq, seq)\n",
    "    \n",
    "    # Step 5: Weighted sum of values\n",
    "    output = attn_weights @ V  # (batch, seq, d_k)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "# Test it\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: (2, 6, 64)\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")  # Expected: (2, 6, 6)\n",
    "print(f\"\\nFirst token's attention distribution:\")\n",
    "print(attn_weights[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Causal Masking for Autoregressive Generation\n",
    "\n",
    "### The Cheating Problem\n",
    "\n",
    "Without masking, when processing \"cat\", the model can attend to ALL tokens—including future ones! This is cheating during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask: upper triangle is False (block), lower is True (allow).\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) boolean tensor\n",
    "    \"\"\"\n",
    "    # torch.tril creates a lower triangular matrix\n",
    "    # 1s below diagonal (including diagonal), 0s above\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "# Example with seq_len = 6\n",
    "mask = create_causal_mask(6)\n",
    "print(\"Causal mask (1 = allow, 0 = block):\")\n",
    "print(mask)\n",
    "\n",
    "print(\"\\nReading the mask:\")\n",
    "print(\"- Row 0 (token 0): Can attend to column 0 only\")\n",
    "print(\"- Row 2 (token 2): Can attend to columns 0, 1, 2\")\n",
    "print(\"- Row 5 (token 5): Can attend to all columns 0-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with causal mask\n",
    "Q_test = torch.randn(2, 6, 64)  # (batch, seq, d_k)\n",
    "K_test = torch.randn(2, 6, 64)\n",
    "V_test = torch.randn(2, 6, 64)\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = create_causal_mask(6)  # (seq, seq)\n",
    "\n",
    "# Apply attention with mask\n",
    "output_masked, attn_weights_masked = scaled_dot_product_attention(\n",
    "    Q_test, K_test, V_test, mask=causal_mask\n",
    ")\n",
    "\n",
    "print(\"Attention weights WITH causal mask (first item in batch):\")\n",
    "print(attn_weights_masked[0])\n",
    "\n",
    "print(\"\\nNotice: Upper triangle is all zeros! No future attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Multi-Head Attention\n\n### Why Multiple Heads?\n\nOne attention head can only capture ONE type of relationship at a time. Multiple heads let the model learn different patterns simultaneously:\n\n| Head | What it might learn |\n|------|---------------------|\n| Head 1 | Subject-verb relationships (\"cat\" → \"sat\") |\n| Head 2 | Adjective-noun connections (\"lazy\" → \"dog\") |\n| Head 3 | Nearby word patterns (local context) |\n| Head 4 | Long-range dependencies (pronoun resolution) |\n\n**Key Insight:** 12 heads with 64 dims each = 768 total dims = same as 1 big head!\nNo extra parameters — just different perspectives.\n\n### Efficient Implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient multi-head attention (batches all heads together).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads  # 768 / 12 = 64\n",
    "        \n",
    "        # Combined QKV projection (3x more efficient than separate!)\n",
    "        # Why 3 * d_model? Because we project to Q, K, V simultaneously\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Dropout on attention weights\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq, d_model = x.shape\n",
    "        \n",
    "        # ===== Step 1: Project to Q, K, V (all at once!) =====\n",
    "        qkv = self.qkv_proj(x)  # (batch, seq, 3 * d_model)\n",
    "        \n",
    "        # ===== Step 2: Split into Q, K, V and reshape for multi-head =====\n",
    "        # Reshape to (batch, seq, 3, num_heads, d_head)\n",
    "        qkv = qkv.reshape(batch, seq, 3, self.num_heads, self.d_head)\n",
    "        \n",
    "        # Permute to (3, batch, num_heads, seq, d_head)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        # Split into Q, K, V: each is (batch, num_heads, seq, d_head)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # ===== Step 3: Scaled dot-product attention (batched over heads) =====\n",
    "        d_k = self.d_head\n",
    "        scores = Q @ K.transpose(-2, -1)  # (batch, num_heads, seq, seq)\n",
    "        scores = scores / math.sqrt(d_k)\n",
    "        \n",
    "        # Apply causal mask if provided\n",
    "        if mask is not None:\n",
    "            # Expand mask for heads: (seq, seq) → (1, 1, seq, seq)\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq, seq)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        attn_output = attn_weights @ V  # (batch, num_heads, seq, d_head)\n",
    "        \n",
    "        # ===== Step 4: Concatenate heads =====\n",
    "        # Transpose to (batch, seq, num_heads, d_head)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        \n",
    "        # Reshape to (batch, seq, d_model) — this concatenates heads\n",
    "        attn_output = attn_output.reshape(batch, seq, d_model)\n",
    "        \n",
    "        # ===== Step 5: Final projection =====\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Test it\n",
    "mha = MultiHeadAttention(d_model=768, num_heads=12, dropout=0.1)\n",
    "embeddings_test = torch.randn(2, 6, 768)\n",
    "mask_test = create_causal_mask(6)\n",
    "\n",
    "output_mha, attn_weights_mha = mha(embeddings_test, mask_test)\n",
    "\n",
    "print(f\"Input shape:  {embeddings_test.shape}\")     # Expected: (2, 6, 768)\n",
    "print(f\"Output shape: {output_mha.shape}\")          # Expected: (2, 6, 768)\n",
    "print(f\"Attention weights shape: {attn_weights_mha.shape}\")  # Expected: (2, 12, 6, 6)\n",
    "print(\"                                                         ^^ 12 heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Complete Transformer Blocks\n\n### Feedforward Network\n\nThe feedforward network expands the dimension (768 → 3072), applies a nonlinearity, then compresses back (3072 → 768). \n\n**What is GELU?**\n- GELU (Gaussian Error Linear Unit) is an activation function\n- Like ReLU but smoother — doesn't have a hard cutoff at zero\n- Used in GPT-2, BERT, and most modern transformers\n- Intuition: \"gates\" how much signal passes through based on input magnitude"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feedforward network.\n",
    "    Applied to each position independently (same weights for all positions).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (768 for GPT-2 small)\n",
    "            d_ff: Feedforward dimension (typically 4 * d_model = 3072)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, d_model)\n",
    "        x = self.fc1(x)        # (batch, seq, d_ff) — expand\n",
    "        x = F.gelu(x)          # Non-linearity\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)        # (batch, seq, d_model) — project back\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test it\n",
    "ffn = FeedForward(d_model=768, d_ff=3072, dropout=0.1)\n",
    "x_test = torch.randn(2, 6, 768)\n",
    "output_ffn = ffn(x_test)\n",
    "\n",
    "print(f\"Input shape:  {x_test.shape}\")      # Expected: (2, 6, 768)\n",
    "print(f\"Output shape: {output_ffn.shape}\")  # Expected: (2, 6, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Complete Transformer Block (Pre-Norm Style)\n\n**Pre-norm vs Post-norm:**\n- **Post-norm** (original Transformer): LayerNorm AFTER each sublayer\n- **Pre-norm** (GPT-2, modern): LayerNorm BEFORE each sublayer\n\nWhy pre-norm? It makes training more stable for deep networks (12+ layers). The gradients flow more smoothly through the residual connections.\n\n**Residual connections:** `output = x + sublayer(x)`\n- Creates \"highways\" for gradients to flow backward\n- Without residuals, gradients vanish in deep networks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer block with multi-head attention, feedforward,\n",
    "    residuals, and layer normalization (pre-norm style like GPT-2).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (768)\n",
    "            num_heads: Number of attention heads (12)\n",
    "            d_ff: Feedforward dimension (3072 = 4 * d_model)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer normalization (before each sub-layer)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Dropout (applied after each sub-layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch, seq, d_model)\n",
    "            mask: Causal mask (seq, seq) or (1, 1, seq, seq)\n",
    "        \n",
    "        Returns:\n",
    "            x: Output embeddings (batch, seq, d_model)\n",
    "            attn_weights: Attention weights (batch, heads, seq, seq)\n",
    "        \"\"\"\n",
    "        # ===== Multi-Head Attention with Residual =====\n",
    "        # Pre-norm: Normalize BEFORE attention\n",
    "        attn_out, attn_weights = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_out)  # Residual connection\n",
    "        \n",
    "        # ===== Feedforward with Residual =====\n",
    "        # Pre-norm: Normalize BEFORE FFN\n",
    "        ffn_out = self.ffn(self.ln2(x))\n",
    "        x = x + self.dropout(ffn_out)  # Residual connection\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "# Test a complete Transformer block\n",
    "block = TransformerBlock(\n",
    "    d_model=768,\n",
    "    num_heads=12,\n",
    "    d_ff=3072,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Input: Embeddings from Chapter 9\n",
    "embeddings_block = torch.randn(2, 6, 768)\n",
    "mask_block = create_causal_mask(6)\n",
    "\n",
    "# Forward pass\n",
    "output_block, attn_weights_block = block(embeddings_block, mask_block)\n",
    "\n",
    "print(f\"Input shape:  {embeddings_block.shape}\")  # Expected: (2, 6, 768)\n",
    "print(f\"Output shape: {output_block.shape}\")      # Expected: (2, 6, 768)\n",
    "print(f\"Attention weights shape: {attn_weights_block.shape}\")  # Expected: (2, 12, 6, 6)\n",
    "\n",
    "# Verify residual: output should be \"similar\" to input (not completely different)\n",
    "print(f\"\\nInput mean:  {embeddings_block.mean().item():.4f}\")\n",
    "print(f\"Output mean: {output_block.mean().item():.4f}\")\n",
    "print(f\"Difference:  {(output_block - embeddings_block).abs().mean().item():.4f}\")\n",
    "print(\"Difference should be moderate—not zero, not huge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(attn_weights, tokens, head_idx=0, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap for a specific head.\n",
    "    \n",
    "    Args:\n",
    "        attn_weights: Attention weights (batch, heads, seq, seq)\n",
    "        tokens: List of token strings (length = seq)\n",
    "        head_idx: Which head to visualize\n",
    "        ax: Matplotlib axis (if None, create new figure)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Extract weights for specified head (first item in batch)\n",
    "    weights = attn_weights[0, head_idx].detach().cpu().numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(weights, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Key (attending TO)', fontsize=10)\n",
    "    ax.set_ylabel('Query (attending FROM)', fontsize=10)\n",
    "    ax.set_title(f'Attention Weights - Head {head_idx}', fontsize=12)\n",
    "    \n",
    "    # Colorbar\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "# Example: Process a real sentence\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "token_ids = tokenizer.encode(text, return_tensors=\"pt\")  # (1, seq)\n",
    "\n",
    "# Get token strings\n",
    "tokens = [tokenizer.decode([t]) for t in token_ids[0]]\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Run through embedding layer (simulate Chapter 9 output)\n",
    "embed_layer = nn.Embedding(50257, 768)\n",
    "embeddings_viz = embed_layer(token_ids)  # (1, seq, 768)\n",
    "\n",
    "# Create causal mask\n",
    "seq_len_viz = embeddings_viz.size(1)\n",
    "mask_viz = create_causal_mask(seq_len_viz)\n",
    "\n",
    "# Run through Transformer block\n",
    "block_viz = TransformerBlock(d_model=768, num_heads=12, d_ff=3072, dropout=0.1)\n",
    "output_viz, attn_weights_viz = block_viz(embeddings_viz, mask_viz)\n",
    "\n",
    "print(f\"\\nAttention weights shape: {attn_weights_viz.shape}\")  # Expected: (1, 12, seq, seq)\n",
    "\n",
    "# Visualize first 4 heads\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    visualize_attention(attn_weights_viz, tokens, head_idx=i, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhat you see:\")\n",
    "print(\"1. Lower triangular pattern (causal masking works!)\")\n",
    "print(\"2. Different patterns per head (diversity is good)\")\n",
    "print(\"3. Some heads focus locally, others on long-range dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Complete Pipeline\n",
    "\n",
    "Let's trace data from raw text to Transformer block output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Chapter 8: Tokenization =====\n",
    "from transformers import AutoTokenizer\n",
    "text = \"The cat sat on the mat\"  # Raw text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "token_ids = tokenizer.encode(text, return_tensors=\"pt\")  # (1, 6)\n",
    "\n",
    "print(\"Step 1: Tokenization\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Shape: {token_ids.shape}\\n\")\n",
    "\n",
    "# ===== Chapter 9: Embeddings =====\n",
    "# Simulate GPT2Embeddings from Chapter 9\n",
    "embed_layer = nn.Embedding(50257, 768)\n",
    "embeddings = embed_layer(token_ids)  # (1, 6, 768)\n",
    "\n",
    "print(\"Step 2: Embedding\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First token embedding (first 10 dims): {embeddings[0, 0, :10]}\\n\")\n",
    "\n",
    "# ===== Chapter 10: Attention (THIS CHAPTER) =====\n",
    "# Create causal mask\n",
    "seq_len_final = token_ids.size(1)  # 6\n",
    "mask_final = create_causal_mask(seq_len_final)\n",
    "\n",
    "# Transformer block\n",
    "block_final = TransformerBlock(d_model=768, num_heads=12, d_ff=3072, dropout=0.1)\n",
    "output_final, attn_weights_final = block_final(embeddings, mask_final)  # (1, 6, 768)\n",
    "\n",
    "print(\"Step 3: Attention (THIS CHAPTER)\")\n",
    "print(f\"Output shape: {output_final.shape}\")\n",
    "print(f\"First token after attention (first 10 dims): {output_final[0, 0, :10]}\\n\")\n",
    "\n",
    "print(\"Pipeline complete!\")\n",
    "print(\"Raw text → Tokens → Embeddings → Attention → Context-aware vectors\")\n",
    "\n",
    "print(\"\\n===== Chapter 11 Preview: Stack 12 blocks =====\")\n",
    "print(\"In Chapter 11, we'll pass output through 11 more Transformer blocks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Manual Attention Calculation\n",
    "\n",
    "Given Q, K, V matrices, manually compute attention scores, apply softmax, and get the output. Verify your calculations match `scaled_dot_product_attention()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple 2×3 Q, K, V (batch=1, seq=2, d_k=3)\n",
    "Q_ex = torch.tensor([[[1.0, 0.0, 1.0], [0.0, 1.0, 1.0]]])  # (1, 2, 3)\n",
    "K_ex = torch.tensor([[[1.0, 1.0, 0.0], [0.0, 1.0, 1.0]]])  # (1, 2, 3)\n",
    "V_ex = torch.tensor([[[2.0, 0.0, 1.0], [1.0, 2.0, 0.0]]])  # (1, 2, 3)\n",
    "\n",
    "# YOUR CODE HERE: \n",
    "# 1. Compute scores = Q @ K^T\n",
    "# 2. Scale by sqrt(d_k)\n",
    "# 3. Apply softmax\n",
    "# 4. Weighted sum with V\n",
    "# 5. Compare with scaled_dot_product_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Causal Mask Verification\n",
    "\n",
    "Create attention weights with and without causal masking. Verify that the upper triangle is zero with masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# 1. Create Q, K, V for seq_len=5\n",
    "# 2. Compute attention WITHOUT mask\n",
    "# 3. Compute attention WITH causal mask\n",
    "# 4. Print both attention weight matrices\n",
    "# 5. Verify upper triangle is zero in masked version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multi-Head Shapes\n",
    "\n",
    "Trace the shape transformations through `MultiHeadAttention` with specific numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# Create MHA with d_model=512, num_heads=8, seq_len=10\n",
    "# Print shape after each step:\n",
    "# 1. Input\n",
    "# 2. After QKV projection\n",
    "# 3. After reshape to separate heads\n",
    "# 4. After attention computation\n",
    "# 5. After concatenating heads\n",
    "# 6. After output projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Compare Single vs Multi-Head\n",
    "\n",
    "Run the same input through single-head and 12-head attention. Compare parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# 1. Create single-head attention (d_model=768, num_heads=1)\n",
    "# 2. Create multi-head attention (d_model=768, num_heads=12)\n",
    "# 3. Count parameters in each\n",
    "# 4. Run same input through both\n",
    "# 5. Compare outputs and parameter counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Attention Visualization\n",
    "\n",
    "Process your own sentence and visualize different attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# 1. Choose an interesting sentence (e.g., \"Alice gave Bob a book\")\n",
    "# 2. Tokenize it\n",
    "# 3. Run through TransformerBlock\n",
    "# 4. Visualize heads 0, 3, 7, 11\n",
    "# 5. Describe what patterns you see (local vs long-range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Stacking Blocks\n",
    "\n",
    "Stack 3 Transformer blocks and process a sequence through all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# 1. Create 3 separate TransformerBlock instances\n",
    "# 2. Pass embeddings through block1 → block2 → block3\n",
    "# 3. Print shape after each block\n",
    "# 4. Compare input embeddings to final output\n",
    "# 5. How different are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Pre-Norm vs Post-Norm\n",
    "\n",
    "Implement a post-norm Transformer block and compare with pre-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# 1. Implement TransformerBlockPostNorm where LayerNorm comes AFTER\n",
    "# 2. Run same input through both pre-norm and post-norm\n",
    "# 3. Compare outputs\n",
    "# 4. Which has more stable gradients? (you can check gradient magnitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Parameter Count\n",
    "\n",
    "Calculate the exact parameter count for one Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "# For d_model=768, num_heads=12, d_ff=3072:\n",
    "# 1. Count QKV projection parameters\n",
    "# 2. Count output projection parameters\n",
    "# 3. Count FFN parameters (fc1 + fc2)\n",
    "# 4. Count LayerNorm parameters (2 layers)\n",
    "# 5. Sum to get total\n",
    "# 6. Verify with model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. Scaled dot-product attention (Q, K, V → scores → softmax → weighted sum)\n",
    "2. Causal masking for autoregressive generation (no peeking at future)\n",
    "3. Efficient multi-head attention (1 head → 12 heads via reshape tricks)\n",
    "4. Complete Transformer blocks (attention + FFN + residuals + layer norms)\n",
    "5. Attention visualization (heatmaps showing what model attends to)\n",
    "\n",
    "**Core concepts:**\n",
    "\n",
    "- **Static embeddings** (Ch9) → **Context-aware representations** (Ch10)\n",
    "- **Query/Key/Value**: Three learned projections serving different roles\n",
    "- **Attention weights**: Softmax probabilities showing relevance\n",
    "- **Multi-head**: Different heads learn different patterns (no extra parameters!)\n",
    "- **Residuals + Norms**: Enable deep networks (100+ layers)\n",
    "\n",
    "**Next:** Chapter 11 will stack these blocks and add the language modeling head to create a complete GPT model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}