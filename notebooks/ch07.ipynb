{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Companion Notebook\n",
    "**Build Your First LLM ‚Äî Chapter 7: Preparing Your Data**\n",
    "This notebook walks through the data prep steps: cleaning, deduping, splitting, chunking, and saving to JSONL with quick stats.\n",
    "Tiny toy data is inline; no external files needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "from collections import Counter\n",
    "import random\n",
    "print('Setup complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4sobtfdb17",
   "source": [
    "## Python Tools Quick Reference\n",
    "\n",
    "This notebook uses several Python tools. Here's a quick reminder:\n",
    "\n",
    "**File I/O:** `open(path, mode)` with `with` statement for auto-cleanup\n",
    "**Sets:** Fast membership checking - `set()`, `.add()`, `item in set`\n",
    "**JSON:** `json.dumps(dict)` to convert dict ‚Üí string, `ensure_ascii=False` keeps emoji\n",
    "**Hashing:** `hashlib.sha1(text.encode('utf-8')).hexdigest()` creates fingerprints\n",
    "**Regex:** `re.sub(pattern, replacement, text)` for pattern-based find/replace\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample texts\n",
    "A few toy paragraphs to simulate a tiny corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    \"THE TIME MACHINE ‚Äî CHAPTER I\\n\\nThis   is   a    sample text‚Ä¶ with   odd spacing, smart ‚Äúquotes‚Äù, and tabs\\t.\",\n",
    "    \"AI systems learn from examples. Data quality shapes model quality.\",\n",
    "    \"The key to machine learning is data; the secret to building AI is understanding.\",\n",
    "]\n",
    "print('Docs:', len(raw_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & normalizing\n",
    "\n",
    "Strip HTML, control chars, collapse whitespace, normalize quotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)      # strip HTML tags\n",
    "    text = re.sub(r\"[\\n\\t]\", \" \", text)     # control characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # collapse whitespace\n",
    "    text = text.replace(\"‚Äú\", '\"').replace(\"‚Äù\", '\"')\n",
    "    return text\n",
    "\n",
    "cleaned_docs = [clean_text(d) for d in raw_docs]\n",
    "for i, d in enumerate(cleaned_docs):\n",
    "    print(f\"Cleaned {i}: {d[:80]} ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "\n",
    "Hash paragraphs, drop repeats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_chunk(text: str) -> str:\n",
    "    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def dedup_chunks(chunks):\n",
    "    seen, unique = set(), []\n",
    "    for c in chunks:\n",
    "        h = hash_chunk(c)\n",
    "        if h in seen:\n",
    "            continue\n",
    "        seen.add(h)\n",
    "        unique.append(c)\n",
    "    return unique\n",
    "\n",
    "# Ensure cleaned_docs exists (fallback to cleaning raw_docs if needed)\n",
    "try:\n",
    "    cleaned_docs\n",
    "except NameError:\n",
    "    cleaned_docs = [clean_text(d) for d in raw_docs]\n",
    "\n",
    "# add a duplicate to prove it drops\n",
    "deduped = dedup_chunks(cleaned_docs + [cleaned_docs[0]])\n",
    "print('Before:', len(cleaned_docs), 'After dedup:', len(deduped))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val/test split (by document)\n",
    "\n",
    "Keep related text together; avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ohte2gk8ud",
   "source": [
    "def split_docs(docs: list, train_p: float = 0.8, val_p: float = 0.1, seed: int = 42):\n",
    "    \"\"\"Split documents into train/val/test sets.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of documents to split\n",
    "        train_p: Proportion for training (default 0.8 = 80%)\n",
    "        val_p: Proportion for validation (default 0.1 = 10%)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train, val, test lists (test gets remaining proportion)\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the caller's list\n",
    "    docs = list(docs)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(docs)\n",
    "    \n",
    "    n = len(docs)\n",
    "    n_train = int(n * train_p)\n",
    "    n_val = int(n * val_p)\n",
    "    \n",
    "    train = docs[:n_train]\n",
    "    val = docs[n_train:n_train + n_val]\n",
    "    test = docs[n_train + n_val:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Apply split to our deduped docs\n",
    "train_docs, val_docs, test_docs = split_docs(deduped, train_p=0.6, val_p=0.2, seed=42)\n",
    "print(f'Train: {len(train_docs)}, Val: {len(val_docs)}, Test: {len(test_docs)}')\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "78eubuq7iw6",
   "source": [
    "## Chunking for context windows\n",
    "Break long text with overlap to preserve context across chunk boundaries.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "l5hpyd42wc",
   "source": [
    "def chunk_text(text: str, max_chars: int = 200, overlap: int = 50):\n",
    "    \"\"\"Break text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chars: Maximum characters per chunk\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + max_chars)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += max_chars - overlap  # advance by (max_chars - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking to all splits\n",
    "chunked = []\n",
    "for split, docs in [('train', train_docs), ('val', val_docs), ('test', test_docs)]:\n",
    "    for d in docs:\n",
    "        for c in chunk_text(d, max_chars=120, overlap=30):\n",
    "            chunked.append({'text': c, 'split': split, 'source': 'toy'})\n",
    "            \n",
    "print('Total chunks:', len(chunked))\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eucoxnezsj4",
   "source": [
    "## Visualize Overlap\n",
    "See how chunks overlap to preserve context across boundaries.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7ahtzgegaed",
   "source": [
    "# Create a test text with clear positions\n",
    "test_text = \"A\" * 500  # 500 characters\n",
    "chunks = chunk_text(test_text, max_chars=200, overlap=50)\n",
    "\n",
    "print(f\"Text length: {len(test_text)}\")\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Chunk lengths: {[len(c) for c in chunks]}\")\n",
    "\n",
    "# Verify overlap between consecutive chunks\n",
    "if len(chunks) >= 2:\n",
    "    # Last 50 chars of chunk 0 should equal first 50 chars of chunk 1\n",
    "    overlap_matches = chunks[0][-50:] == chunks[1][:50]\n",
    "    print(f\"\\nOverlap verification: {overlap_matches}\")\n",
    "    print(f\"Chunk 0 ends with: ...{chunks[0][-10:]}\")\n",
    "    print(f\"Chunk 1 starts with: {chunks[1][:10]}...\")\n",
    "    print(\"These 10 chars match (part of 50-char overlap):\", chunks[0][-10:] == chunks[1][:10])\n",
    "    \n",
    "# With real text\n",
    "real_text = \"This is sentence one. This is sentence two. This is sentence three.\" * 5\n",
    "real_chunks = chunk_text(real_text, max_chars=100, overlap=30)\n",
    "print(f\"\\nReal text chunked into {len(real_chunks)} pieces\")\n",
    "print(f\"Chunk 0: ...{real_chunks[0][-40:]}\")\n",
    "print(f\"Chunk 1: {real_chunks[1][:40]}...\")\n",
    "print(\"Notice how overlap preserves complete sentences\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2f4361ie7",
   "source": [
    "## Quality Checks & Sanity Validation\n",
    "Catch problems early with automated checks that flag empty chunks, HTML leakage, and size issues.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vlqhjxabwt",
   "source": [
    "def sanity_check(chunks, stage_name):\n",
    "    \"\"\"Run sanity checks on data at any pipeline stage.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sanity Check: {stage_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è  WARNING: No chunks!\")\n",
    "        return\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"‚úì Total chunks: {len(chunks)}\")\n",
    "    lengths = [len(c) if isinstance(c, str) else len(c.get('text', '')) for c in chunks]\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    print(f\"‚úì Avg length: {avg_len:.0f}\")\n",
    "    print(f\"‚úì Max length: {max(lengths)}\")\n",
    "    print(f\"‚úì Min length: {min(lengths)}\")\n",
    "    \n",
    "    # Check for issues\n",
    "    if max(lengths) > 10 * avg_len:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Max length is 10x average - chunking may be broken\")\n",
    "    \n",
    "    # Check for HTML leakage\n",
    "    texts = [c if isinstance(c, str) else c.get('text', '') for c in chunks]\n",
    "    all_text = ' '.join(texts).lower()\n",
    "    html_words = {'div', 'span', 'href', 'html', 'class', 'src'}\n",
    "    found_html = [w for w in html_words if w in all_text]\n",
    "    if found_html:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: HTML tags found: {found_html}\")\n",
    "    else:\n",
    "        print(\"‚úì No HTML leakage detected\")\n",
    "    \n",
    "    # Check for empty chunks\n",
    "    empty = sum(1 for l in lengths if l < 10)\n",
    "    if empty > 0:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: {empty} chunks are < 10 chars\")\n",
    "    else:\n",
    "        print(\"‚úì No empty chunks\")\n",
    "    \n",
    "    # Sample\n",
    "    sample = texts[0] if texts else \"N/A\"\n",
    "    print(f\"\\n‚úì Sample: {sample[:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Run checks on our chunked data\n",
    "sanity_check(chunked, \"After Chunking\")\n",
    "\n",
    "# You can run this after each stage:\n",
    "# sanity_check(cleaned_docs, \"After Cleaning\")\n",
    "# sanity_check(deduped, \"After Deduplication\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kv5yc7fb30e",
   "source": [
    "## Worked Example: End-to-End Pipeline\n",
    "Complete walkthrough from raw documents (with HTML and duplicates) to JSONL-ready data.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tl863qe0htk",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPLETE DATA PIPELINE WALKTHROUGH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Start with raw documents (messy, with duplicates and HTML)\n",
    "print(\"\\nüì• STEP 1: Raw Documents\")\n",
    "raw_pipeline_docs = [\n",
    "    {\"text\": \"<p>The cat sat on the mat.</p>\", \"source\": \"doc1\"},\n",
    "    {\"text\": \"<p>The cat sat on the mat.</p>\", \"source\": \"doc2\"},  # exact duplicate!\n",
    "    {\"text\": \"<p>The dog    ran\\tin the park.</p>\", \"source\": \"doc3\"},\n",
    "    {\"text\": \"The bird flew over the house.\", \"source\": \"doc4\"}\n",
    "]\n",
    "print(f\"   Raw documents: {len(raw_pipeline_docs)}\")\n",
    "for i, doc in enumerate(raw_pipeline_docs):\n",
    "    print(f\"   {i+1}. {doc['text'][:50]}...\")\n",
    "\n",
    "# Step 2: Clean each document\n",
    "print(\"\\nüßπ STEP 2: Cleaning\")\n",
    "for doc in raw_pipeline_docs:\n",
    "    doc[\"text\"] = clean_text(doc[\"text\"])\n",
    "print(\"   HTML removed, whitespace normalized\")\n",
    "for i, doc in enumerate(raw_pipeline_docs):\n",
    "    print(f\"   {i+1}. {doc['text']}\")\n",
    "\n",
    "# Step 3: Extract text and deduplicate\n",
    "print(\"\\nüîç STEP 3: Deduplication\")\n",
    "pipeline_texts = [d[\"text\"] for d in raw_pipeline_docs]\n",
    "unique_pipeline = dedup_chunks(pipeline_texts)\n",
    "print(f\"   Before: {len(pipeline_texts)} texts\")\n",
    "print(f\"   After:  {len(unique_pipeline)} unique texts\")\n",
    "for i, text in enumerate(unique_pipeline):\n",
    "    print(f\"   {i+1}. {text}\")\n",
    "\n",
    "# Step 4: Split into train/val/test\n",
    "print(\"\\nüìä STEP 4: Train/Val/Test Split\")\n",
    "train_p, val_p, test_p = split_docs(unique_pipeline, train_p=0.34, val_p=0.33, seed=42)\n",
    "print(f\"   Train: {len(train_p)} docs - {train_p}\")\n",
    "print(f\"   Val:   {len(val_p)} docs - {val_p}\")\n",
    "print(f\"   Test:  {len(test_p)} docs - {test_p}\")\n",
    "\n",
    "# Step 5: Chunk (for longer documents, here it's small)\n",
    "print(\"\\n‚úÇÔ∏è  STEP 5: Chunking\")\n",
    "train_pipeline_chunks = []\n",
    "for text in train_p:\n",
    "    chunks = chunk_text(text, max_chars=50, overlap=10)\n",
    "    train_pipeline_chunks.extend(chunks)\n",
    "print(f\"   Train chunks: {len(train_pipeline_chunks)}\")\n",
    "for i, chunk in enumerate(train_pipeline_chunks):\n",
    "    print(f\"   Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# Step 6: Prepare JSONL records\n",
    "print(\"\\nüíæ STEP 6: JSONL Preparation\")\n",
    "final_records = [\n",
    "    {\"text\": chunk, \"split\": \"train\", \"length\": len(chunk), \"source\": \"example\"}\n",
    "    for chunk in train_pipeline_chunks\n",
    "]\n",
    "print(f\"   Ready to save: {len(final_records)} records\")\n",
    "print(f\"   Example record: {final_records[0]}\")\n",
    "\n",
    "print(\"\\n‚úÖ PIPELINE COMPLETE!\")\n",
    "print(f\"   Started with: {len(raw_pipeline_docs)} raw documents (with duplicate)\")\n",
    "print(f\"   Ended with: {len(final_records)} clean, deduplicated JSONL records\")\n",
    "print(f\"   Data is now ready for tokenization in Chapter 8!\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(records, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(chunked, 'toy_corpus.jsonl')\n",
    "print('Wrote toy_corpus.jsonl with', len(chunked), 'records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick stats\n",
    "\n",
    "Duplicate rate, top words, and sample records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rate(texts):\n",
    "    hashes = [hash_chunk(t) for t in texts]\n",
    "    return 1 - (len(set(hashes)) / len(hashes))\n",
    "\n",
    "def top_words(texts, k=10):\n",
    "    words = \" \".join(texts).lower().split()\n",
    "    return Counter(words).most_common(k)\n",
    "\n",
    "texts_all = [r['text'] for r in chunked]\n",
    "print('Duplicate rate:', duplicate_rate(texts_all))\n",
    "print('Top words:', top_words(texts_all, k=8))\n",
    "print('Sample records:', chunked[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "syf7x3l3rf",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you've learned the complete data preparation pipeline for LLM training:\n",
    "\n",
    "**1. Text Cleaning:** Remove HTML, normalize whitespace, handle special characters\n",
    "**2. Deduplication:** Use hashing to identify and remove exact duplicates\n",
    "**3. Train/Val/Test Split:** Separate data at document level to prevent leakage\n",
    "**4. Chunking with Overlap:** Break long texts into LLM-sized pieces while preserving context\n",
    "**5. Quality Checks:** Automated sanity checks catch issues early\n",
    "**6. JSONL Format:** Save data in a streaming-friendly format\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Overlap** preserves context across chunk boundaries (prevents chopping sentences in half)\n",
    "- **Hashing** provides fast, deterministic fingerprints for deduplication\n",
    "- **Document-level splitting** keeps related chunks together in the same split\n",
    "- **Quality checks** catch HTML leakage, empty chunks, and size anomalies before they cause training problems\n",
    "\n",
    "**Next Steps:**\n",
    "- Chapter 8: Tokenization (converting text ‚Üí numbers)\n",
    "- Scale up to real datasets (Wikipedia, Common Crawl, books)\n",
    "- Experiment with different overlap values for your use case\n",
    "\n",
    "The data pipeline is the foundation of every great LLM!\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}