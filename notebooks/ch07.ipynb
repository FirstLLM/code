{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Companion Notebook\n",
    "**Build Your First LLM ‚Äî Chapter 7: Preparing Your Data**\n",
    "This notebook walks through the data prep steps: cleaning, deduping, splitting, chunking, and saving to JSONL with quick stats.\n",
    "Tiny toy data is inline; no external files needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== IMPORTS =====\nimport re           # Regular expressions for pattern matching in text\nimport json         # Read/write JSON and JSONL files\nimport hashlib      # Create fingerprints for duplicate detection\nimport unicodedata  # Unicode normalization (handles special characters)\nfrom collections import Counter  # Count word frequencies\nimport random       # Shuffle data for train/val/test splits\n\nprint('Setup complete')"
  },
  {
   "cell_type": "markdown",
   "id": "d4sobtfdb17",
   "source": "## Python Tools Quick Reference\n\nThis notebook uses several Python tools. Here's a quick guide:\n\n**Regular Expressions (regex):** Pattern-based find/replace in text\n- `re.sub(pattern, replacement, text)` ‚Äî Find all matches of `pattern` and replace\n- Pattern `<[^>]+>` means: find `<`, then any chars that aren't `>`, then `>`  ‚Üí matches HTML tags\n\n**Hashing:** Create a unique \"fingerprint\" for any text\n- Same text ‚Üí same hash (always). Different text ‚Üí different hash (almost always)\n- Useful for detecting duplicates without comparing entire documents\n- `hashlib.sha1(text.encode()).hexdigest()` ‚Üí 40-character fingerprint\n\n**JSON/JSONL:** Data formats for storing structured data\n- **JSON:** One big file with all data (must load entire file into memory)\n- **JSONL:** One JSON record per line (can stream line-by-line = memory efficient)\n\n**Sets:** Collections with no duplicates, fast \"is X in this set?\" checking\n- `seen = set()` then `seen.add(item)` and `item in seen`\n\n**Type hints** (`: str`, `: float = 0.8`): Documentation for humans (Python ignores them)\n- `text: str` means \"text should be a string\"\n- `train_p: float = 0.8` means \"train_p should be a decimal, defaults to 0.8\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample texts\n",
    "A few toy paragraphs to simulate a tiny corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "    \"THE TIME MACHINE ‚Äî CHAPTER I\\n\\nThis   is   a    sample text‚Ä¶ with   odd spacing, smart ‚Äúquotes‚Äù, and tabs\\t.\",\n",
    "    \"AI systems learn from examples. Data quality shapes model quality.\",\n",
    "    \"The key to machine learning is data; the secret to building AI is understanding.\",\n",
    "]\n",
    "print('Docs:', len(raw_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & normalizing\n",
    "\n",
    "Strip HTML, control chars, collapse whitespace, normalize quotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def clean_text(text: str) -> str:\n    \"\"\"Clean and normalize text for LLM training.\"\"\"\n    # Unicode normalization: Ô¨Å ‚Üí fi, ÔΩÜÔΩïÔΩåÔΩå ‚Üí full, etc.\n    # NFKC = Compatibility decomposition + Canonical composition\n    text = unicodedata.normalize(\"NFKC\", text)\n    \n    # Strip HTML tags: <p>, <div>, <span class=\"foo\">, etc.\n    # Pattern: < followed by any chars that aren't >, then >\n    text = re.sub(r\"<[^>]+>\", \" \", text)\n    \n    # Replace newlines and tabs with spaces\n    text = re.sub(r\"[\\n\\t]\", \" \", text)\n    \n    # Collapse multiple spaces into one, remove leading/trailing spaces\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    \n    # Normalize smart quotes to straight quotes\n    text = text.replace(\"\"\", '\"').replace(\"\"\", '\"')\n    \n    return text\n\ncleaned_docs = [clean_text(d) for d in raw_docs]\nfor i, d in enumerate(cleaned_docs):\n    print(f\"Cleaned {i}: {d[:80]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "\n",
    "Hash paragraphs, drop repeats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a \"fingerprint\" for text using SHA1 hash\n# Same text ‚Üí same fingerprint (always)\n# Different text ‚Üí different fingerprint (with overwhelming probability)\ndef hash_chunk(text: str) -> str:\n    # .encode(\"utf-8\") converts string to bytes (required by hashlib)\n    # .hexdigest() returns the hash as a 40-character string\n    return hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n\n# Demonstrate hashing\nsample = \"The cat sat on the mat.\"\nprint(f\"Text: '{sample}'\")\nprint(f\"Hash: {hash_chunk(sample)}\")\nprint(f\"Same text same hash: {hash_chunk(sample) == hash_chunk(sample)}\")\nprint(f\"Different text different hash: {hash_chunk(sample) != hash_chunk('Dog.')}\")\n\ndef dedup_chunks(chunks):\n    \"\"\"Remove duplicate chunks using hash-based fingerprinting.\"\"\"\n    seen = set()    # Track hashes we've seen (fast lookup!)\n    unique = []     # Keep only unique chunks\n    \n    for c in chunks:\n        h = hash_chunk(c)\n        if h in seen:\n            continue        # Skip duplicate\n        seen.add(h)         # Remember this hash\n        unique.append(c)    # Keep the chunk\n    \n    return unique\n\n# Add a duplicate to prove deduplication works\ntest_docs = cleaned_docs + [cleaned_docs[0]]  # Add copy of first doc\ndeduped = dedup_chunks(test_docs)\nprint(f'\\nBefore dedup: {len(test_docs)} docs')\nprint(f'After dedup:  {len(deduped)} docs')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val/test split (by document)\n",
    "\n",
    "Keep related text together; avoid leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ohte2gk8ud",
   "source": [
    "def split_docs(docs: list, train_p: float = 0.8, val_p: float = 0.1, seed: int = 42):\n",
    "    \"\"\"Split documents into train/val/test sets.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of documents to split\n",
    "        train_p: Proportion for training (default 0.8 = 80%)\n",
    "        val_p: Proportion for validation (default 0.1 = 10%)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train, val, test lists (test gets remaining proportion)\n",
    "    \"\"\"\n",
    "    # Work on a copy to avoid mutating the caller's list\n",
    "    docs = list(docs)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(docs)\n",
    "    \n",
    "    n = len(docs)\n",
    "    n_train = int(n * train_p)\n",
    "    n_val = int(n * val_p)\n",
    "    \n",
    "    train = docs[:n_train]\n",
    "    val = docs[n_train:n_train + n_val]\n",
    "    test = docs[n_train + n_val:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Apply split to our deduped docs\n",
    "train_docs, val_docs, test_docs = split_docs(deduped, train_p=0.6, val_p=0.2, seed=42)\n",
    "print(f'Train: {len(train_docs)}, Val: {len(val_docs)}, Test: {len(test_docs)}')\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "78eubuq7iw6",
   "source": [
    "## Chunking for context windows\n",
    "Break long text with overlap to preserve context across chunk boundaries.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "l5hpyd42wc",
   "source": [
    "def chunk_text(text: str, max_chars: int = 200, overlap: int = 50):\n",
    "    \"\"\"Break text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chars: Maximum characters per chunk\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + max_chars)\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        start += max_chars - overlap  # advance by (max_chars - overlap)\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking to all splits\n",
    "chunked = []\n",
    "for split, docs in [('train', train_docs), ('val', val_docs), ('test', test_docs)]:\n",
    "    for d in docs:\n",
    "        for c in chunk_text(d, max_chars=120, overlap=30):\n",
    "            chunked.append({'text': c, 'split': split, 'source': 'toy'})\n",
    "            \n",
    "print('Total chunks:', len(chunked))\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eucoxnezsj4",
   "source": [
    "## Visualize Overlap\n",
    "See how chunks overlap to preserve context across boundaries.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7ahtzgegaed",
   "source": "# Create a test text with clear positions\ntest_text = \"A\" * 500  # 500 characters\nchunks = chunk_text(test_text, max_chars=200, overlap=50)\n\nprint(f\"Text length: {len(test_text)}\")\nprint(f\"Number of chunks: {len(chunks)}\")\nprint(f\"Chunk lengths: {[len(c) for c in chunks]}\")\n\n# Verify overlap between consecutive chunks\n# Python slice notation:\n#   text[-50:]  = last 50 characters (negative index counts from end)\n#   text[:50]   = first 50 characters\nif len(chunks) >= 2:\n    # Last 50 chars of chunk 0 should equal first 50 chars of chunk 1\n    chunk0_end = chunks[0][-50:]    # Last 50 chars of chunk 0\n    chunk1_start = chunks[1][:50]   # First 50 chars of chunk 1\n    overlap_matches = chunk0_end == chunk1_start\n    \n    print(f\"\\nOverlap verification: {overlap_matches}\")\n    print(f\"Chunk 0 ends with: ...{chunks[0][-10:]}\")\n    print(f\"Chunk 1 starts with: {chunks[1][:10]}...\")\n    \n# With real text\nreal_text = \"This is sentence one. This is sentence two. This is sentence three.\" * 5\nreal_chunks = chunk_text(real_text, max_chars=100, overlap=30)\nprint(f\"\\nReal text chunked into {len(real_chunks)} pieces\")\nprint(f\"Chunk 0: ...{real_chunks[0][-40:]}\")\nprint(f\"Chunk 1: {real_chunks[1][:40]}...\")\nprint(\"\\n‚úÖ Overlap preserves context across chunk boundaries!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2e2f4361ie7",
   "source": [
    "## Quality Checks & Sanity Validation\n",
    "Catch problems early with automated checks that flag empty chunks, HTML leakage, and size issues.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vlqhjxabwt",
   "source": [
    "def sanity_check(chunks, stage_name):\n",
    "    \"\"\"Run sanity checks on data at any pipeline stage.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sanity Check: {stage_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è  WARNING: No chunks!\")\n",
    "        return\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"‚úì Total chunks: {len(chunks)}\")\n",
    "    lengths = [len(c) if isinstance(c, str) else len(c.get('text', '')) for c in chunks]\n",
    "    avg_len = sum(lengths) / len(lengths)\n",
    "    print(f\"‚úì Avg length: {avg_len:.0f}\")\n",
    "    print(f\"‚úì Max length: {max(lengths)}\")\n",
    "    print(f\"‚úì Min length: {min(lengths)}\")\n",
    "    \n",
    "    # Check for issues\n",
    "    if max(lengths) > 10 * avg_len:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Max length is 10x average - chunking may be broken\")\n",
    "    \n",
    "    # Check for HTML leakage\n",
    "    texts = [c if isinstance(c, str) else c.get('text', '') for c in chunks]\n",
    "    all_text = ' '.join(texts).lower()\n",
    "    html_words = {'div', 'span', 'href', 'html', 'class', 'src'}\n",
    "    found_html = [w for w in html_words if w in all_text]\n",
    "    if found_html:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: HTML tags found: {found_html}\")\n",
    "    else:\n",
    "        print(\"‚úì No HTML leakage detected\")\n",
    "    \n",
    "    # Check for empty chunks\n",
    "    empty = sum(1 for l in lengths if l < 10)\n",
    "    if empty > 0:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: {empty} chunks are < 10 chars\")\n",
    "    else:\n",
    "        print(\"‚úì No empty chunks\")\n",
    "    \n",
    "    # Sample\n",
    "    sample = texts[0] if texts else \"N/A\"\n",
    "    print(f\"\\n‚úì Sample: {sample[:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Run checks on our chunked data\n",
    "sanity_check(chunked, \"After Chunking\")\n",
    "\n",
    "# You can run this after each stage:\n",
    "# sanity_check(cleaned_docs, \"After Cleaning\")\n",
    "# sanity_check(deduped, \"After Deduplication\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kv5yc7fb30e",
   "source": [
    "## Worked Example: End-to-End Pipeline\n",
    "Complete walkthrough from raw documents (with HTML and duplicates) to JSONL-ready data.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tl863qe0htk",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COMPLETE DATA PIPELINE WALKTHROUGH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Start with raw documents (messy, with duplicates and HTML)\n",
    "print(\"\\nüì• STEP 1: Raw Documents\")\n",
    "raw_pipeline_docs = [\n",
    "    {\"text\": \"<p>The cat sat on the mat.</p>\", \"source\": \"doc1\"},\n",
    "    {\"text\": \"<p>The cat sat on the mat.</p>\", \"source\": \"doc2\"},  # exact duplicate!\n",
    "    {\"text\": \"<p>The dog    ran\\tin the park.</p>\", \"source\": \"doc3\"},\n",
    "    {\"text\": \"The bird flew over the house.\", \"source\": \"doc4\"}\n",
    "]\n",
    "print(f\"   Raw documents: {len(raw_pipeline_docs)}\")\n",
    "for i, doc in enumerate(raw_pipeline_docs):\n",
    "    print(f\"   {i+1}. {doc['text'][:50]}...\")\n",
    "\n",
    "# Step 2: Clean each document\n",
    "print(\"\\nüßπ STEP 2: Cleaning\")\n",
    "for doc in raw_pipeline_docs:\n",
    "    doc[\"text\"] = clean_text(doc[\"text\"])\n",
    "print(\"   HTML removed, whitespace normalized\")\n",
    "for i, doc in enumerate(raw_pipeline_docs):\n",
    "    print(f\"   {i+1}. {doc['text']}\")\n",
    "\n",
    "# Step 3: Extract text and deduplicate\n",
    "print(\"\\nüîç STEP 3: Deduplication\")\n",
    "pipeline_texts = [d[\"text\"] for d in raw_pipeline_docs]\n",
    "unique_pipeline = dedup_chunks(pipeline_texts)\n",
    "print(f\"   Before: {len(pipeline_texts)} texts\")\n",
    "print(f\"   After:  {len(unique_pipeline)} unique texts\")\n",
    "for i, text in enumerate(unique_pipeline):\n",
    "    print(f\"   {i+1}. {text}\")\n",
    "\n",
    "# Step 4: Split into train/val/test\n",
    "print(\"\\nüìä STEP 4: Train/Val/Test Split\")\n",
    "train_p, val_p, test_p = split_docs(unique_pipeline, train_p=0.34, val_p=0.33, seed=42)\n",
    "print(f\"   Train: {len(train_p)} docs - {train_p}\")\n",
    "print(f\"   Val:   {len(val_p)} docs - {val_p}\")\n",
    "print(f\"   Test:  {len(test_p)} docs - {test_p}\")\n",
    "\n",
    "# Step 5: Chunk (for longer documents, here it's small)\n",
    "print(\"\\n‚úÇÔ∏è  STEP 5: Chunking\")\n",
    "train_pipeline_chunks = []\n",
    "for text in train_p:\n",
    "    chunks = chunk_text(text, max_chars=50, overlap=10)\n",
    "    train_pipeline_chunks.extend(chunks)\n",
    "print(f\"   Train chunks: {len(train_pipeline_chunks)}\")\n",
    "for i, chunk in enumerate(train_pipeline_chunks):\n",
    "    print(f\"   Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# Step 6: Prepare JSONL records\n",
    "print(\"\\nüíæ STEP 6: JSONL Preparation\")\n",
    "final_records = [\n",
    "    {\"text\": chunk, \"split\": \"train\", \"length\": len(chunk), \"source\": \"example\"}\n",
    "    for chunk in train_pipeline_chunks\n",
    "]\n",
    "print(f\"   Ready to save: {len(final_records)} records\")\n",
    "print(f\"   Example record: {final_records[0]}\")\n",
    "\n",
    "print(\"\\n‚úÖ PIPELINE COMPLETE!\")\n",
    "print(f\"   Started with: {len(raw_pipeline_docs)} raw documents (with duplicate)\")\n",
    "print(f\"   Ended with: {len(final_records)} clean, deduplicated JSONL records\")\n",
    "print(f\"   Data is now ready for tokenization in Chapter 8!\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(records, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(chunked, 'toy_corpus.jsonl')\n",
    "print('Wrote toy_corpus.jsonl with', len(chunked), 'records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick stats\n",
    "\n",
    "Duplicate rate, top words, and sample records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rate(texts):\n",
    "    hashes = [hash_chunk(t) for t in texts]\n",
    "    return 1 - (len(set(hashes)) / len(hashes))\n",
    "\n",
    "def top_words(texts, k=10):\n",
    "    words = \" \".join(texts).lower().split()\n",
    "    return Counter(words).most_common(k)\n",
    "\n",
    "texts_all = [r['text'] for r in chunked]\n",
    "print('Duplicate rate:', duplicate_rate(texts_all))\n",
    "print('Top words:', top_words(texts_all, k=8))\n",
    "print('Sample records:', chunked[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "syf7x3l3rf",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you've learned the complete data preparation pipeline for LLM training:\n",
    "\n",
    "**1. Text Cleaning:** Remove HTML, normalize whitespace, handle special characters\n",
    "**2. Deduplication:** Use hashing to identify and remove exact duplicates\n",
    "**3. Train/Val/Test Split:** Separate data at document level to prevent leakage\n",
    "**4. Chunking with Overlap:** Break long texts into LLM-sized pieces while preserving context\n",
    "**5. Quality Checks:** Automated sanity checks catch issues early\n",
    "**6. JSONL Format:** Save data in a streaming-friendly format\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Overlap** preserves context across chunk boundaries (prevents chopping sentences in half)\n",
    "- **Hashing** provides fast, deterministic fingerprints for deduplication\n",
    "- **Document-level splitting** keeps related chunks together in the same split\n",
    "- **Quality checks** catch HTML leakage, empty chunks, and size anomalies before they cause training problems\n",
    "\n",
    "**Next Steps:**\n",
    "- Chapter 8: Tokenization (converting text ‚Üí numbers)\n",
    "- Scale up to real datasets (Wikipedia, Common Crawl, books)\n",
    "- Experiment with different overlap values for your use case\n",
    "\n",
    "The data pipeline is the foundation of every great LLM!\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}