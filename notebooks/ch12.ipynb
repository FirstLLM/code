{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 12: Training Your Model\n",
    "\n",
    "> \"I have not failed. I've just found 10,000 ways that won't work.\"\n",
    "> — **Thomas Edison**, Inventor\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How next-token prediction teaches models to write\n",
    "- The 5-step training recipe that powers all neural networks\n",
    "- Why loss curves reveal if your model is learning\n",
    "- How to prevent overfitting and know when to stop\n",
    "- Saving checkpoints so you never lose progress\n",
    "- The thrill of watching your model transform from gibberish to coherence\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import math\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== REPRODUCIBILITY =====\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Model Components from Chapters 10-11\n",
    "\n",
    "First, let's bring in the MiniGPT model we built in previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MULTI-HEAD ATTENTION (from Chapter 10) =====\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Efficient multi-head attention (batches all heads together).\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq, d_model = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch, seq, 3, self.num_heads, self.d_head)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = attn_weights @ V\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch, seq, d_model)\n",
    "\n",
    "        return self.out_proj(attn_output), attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FEEDFORWARD NETWORK (from Chapter 10) =====\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"FeedForward defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRANSFORMER BLOCK (from Chapter 10) =====\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete Transformer block (pre-norm style like GPT-2).\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn_weights = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        ffn_out = self.ffn(self.ln2(x))\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"TransformerBlock defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GPT CONFIG (from Chapter 11) =====\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for MiniGPT model.\"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 1024\n",
    "    embed_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    d_ff: int = 3072\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.embed_dim % self.num_heads == 0, \\\n",
    "            f\"embed_dim ({self.embed_dim}) must be divisible by num_heads ({self.num_heads})\"\n",
    "\n",
    "print(\"GPTConfig defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MINIGPT MODEL (from Chapter 11) =====\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"A minimal GPT-style language model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.embed_dim,\n",
    "                num_heads=config.num_heads,\n",
    "                d_ff=config.d_ff,\n",
    "                dropout=config.dropout\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm and LM head\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.token_embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, return_attention=False):\n",
    "        batch, seq = token_ids.shape\n",
    "        device = token_ids.device\n",
    "\n",
    "        tok_emb = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq, device=device)\n",
    "        pos_emb = self.pos_embed(positions)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq, seq, device=device))\n",
    "\n",
    "        attention_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            if return_attention:\n",
    "                attention_weights.append(attn)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "print(\"MiniGPT class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 2. Download Dataset\n",
    "\n",
    "We'll use TinyShakespeare - small enough to train in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TinyShakespeare\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urllib.request.urlretrieve(url, \"shakespeare.txt\")\n",
    "\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(f\"\\nSample:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Simple dataset that returns text chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, text, chunk_size=256):\n",
    "        self.chunks = []\n",
    "        for i in range(0, len(text) - chunk_size, chunk_size):\n",
    "            self.chunks.append(text[i:i + chunk_size])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.chunks[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=128):\n",
    "    \"\"\"Tokenize and pad a batch of text strings.\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset(text, chunk_size=256)\n",
    "print(f\"Number of chunks: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "collate = partial(collate_fn, tokenizer=tokenizer, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate\n",
    ")\n",
    "\n",
    "# Check one batch\n",
    "input_ids, attention_mask = next(iter(train_loader))\n",
    "print(f\"Batch input_ids shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small config for fast training\n",
    "config = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=128,\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "model = MiniGPT(config).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 5. The \"Before\" State: Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=30, temperature=1.0):\n",
    "    \"\"\"Generate text with temperature control.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(token_ids)\n",
    "        next_logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(next_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(token_ids[0])\n",
    "\n",
    "\n",
    "# Generate from untrained model\n",
    "print(\"BEFORE TRAINING (random weights):\")\n",
    "print(\"=\"*50)\n",
    "prompt = \"The king\"\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Output: {generate(model, tokenizer, prompt)}\")\n",
    "print(\"\\n(Random gibberish - the model hasn't learned anything yet!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate = 3e-4\n",
    "warmup_steps = 100\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device, clip_norm=1.0):\n",
    "    \"\"\"Train for one epoch using the 5-step recipe.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress = tqdm(dataloader, desc=\"Training\")\n",
    "    for input_ids, attention_mask in progress:\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        # Shift for language modeling\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        # ===== THE 5-STEP RECIPE =====\n",
    "        optimizer.zero_grad(set_to_none=True)       # 1. Zero gradients\n",
    "        logits = model(inputs)                       # 2. Forward pass\n",
    "        loss = F.cross_entropy(                      # 3. Compute loss\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        loss.backward()                              # 4. Backward pass\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()                             # 5. Update weights\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model and compute perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for input_ids, attention_mask in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            targets.view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id,\n",
    "            reduction='sum'\n",
    "        )\n",
    "\n",
    "        mask = (targets != tokenizer.pad_token_id)\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, train_loss, val_loss, path):\n",
    "    \"\"\"Save training checkpoint.\"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training!\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, perplexity = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.1f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch,\n",
    "                       train_loss, val_loss, \"best_model.pt\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Plot Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
    "plt.plot(epochs, val_losses, 'r-o', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 10. The Payoff: Text Generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(\"best_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AFTER TRAINING (on Shakespeare):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts = [\n",
    "    \"The king\",\n",
    "    \"To be or not to be\",\n",
    "    \"Friends, Romans, countrymen\",\n",
    "    \"All the world's a stage\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=40, temperature=0.8)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Output: {output}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 11. Before vs After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh untrained model for comparison\n",
    "set_seed(123)  # Different seed for different random weights\n",
    "untrained_model = MiniGPT(config).to(device)\n",
    "untrained_model.eval()\n",
    "\n",
    "prompt = \"The fair maiden\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nBEFORE (untrained):\")\n",
    "print(generate(untrained_model, tokenizer, prompt, max_new_tokens=30))\n",
    "print(\"\\nAFTER (trained on Shakespeare):\")\n",
    "print(generate(model, tokenizer, prompt, max_new_tokens=30, temperature=0.8))\n",
    "print(\"\\nSame architecture. Same code. Training makes all the difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 12. Temperature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The noble lord\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    output = generate(model, tokenizer, prompt, max_new_tokens=30, temperature=temp)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. **Label shifting** for next-token prediction\n",
    "2. **DataLoaders** that efficiently batch and tokenize text\n",
    "3. **The 5-step training recipe**: zero_grad → forward → loss → backward → step\n",
    "4. **Evaluation** with validation loss and perplexity\n",
    "5. **Checkpointing** to save and resume training\n",
    "6. **Text generation** with temperature control\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- Training is a feedback control loop: measure error, adjust weights, repeat\n",
    "- Cross-entropy loss penalizes confident wrong predictions more than uncertain ones\n",
    "- Overfitting = memorizing training data (train loss ↓, val loss ↑)\n",
    "- Perplexity measures \"how surprised\" the model is—lower is better\n",
    "- Temperature controls generation diversity\n",
    "\n",
    "**Next:** Chapter 13 will teach you to fine-tune this model for specific tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Learning Rate Experiment\n",
    "\n",
    "Try different learning rates and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Train with lr=1e-5, lr=3e-4, lr=1e-2\n",
    "# Compare the loss curves\n",
    "# Which learning rate works best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### Exercise 2: More Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Train for 5-10 epochs instead of 3\n",
    "# Does the model keep improving?\n",
    "# Do you see signs of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Exercise 3: Model Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a smaller model (2 layers, 128 dim)\n",
    "# Create a larger model (6 layers, 384 dim)\n",
    "# Compare training speed and final perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
