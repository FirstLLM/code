{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 5 Companion Notebook\n",
        "**Build Your First LLM — Chapter 5: Your First Python Program**\n\n",
        "This notebook bundles the runnable code examples from Chapter 5. Run cells top-to-bottom.\n\n",
        "- Installs: transformers (for GPT-2 demo)\n",
        "- Data: tiny inline strings; no external files needed\n",
        "- Runtime: CPU is fine; GPU just speeds the GPT-2 call\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.46.1\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from transformers import pipeline, logging\n",
        "logging.set_verbosity_error()\n",
        "print('Setup complete')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick win: GPT-2 text generation\n",
        "Run a tiny GPT-2 generation to see an LLM in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "result = generator(\n",
        "    'The secret to building AI is',\n",
        "    max_new_tokens=20,\n",
        "    do_sample=True,\n",
        "    pad_token_id=50256\n",
        ")\n",
        "print(result[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strings and basic ops\n",
        "Working with text, lengths, and slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = 'The secret to building AI is understanding how machines learn from data'\n",
        "prompt = 'The secret to building AI is'\n",
        "print(len(output))\n",
        "print(type(output))\n",
        "words = output.split()\n",
        "print(words)\n",
        "words = output.lower().split()\n",
        "print(words)\n",
        "generated = output[len(prompt):]\n",
        "print(f'Generated: {generated.strip()}')\n",
        "print(f'Word count: {len(generated.split())}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numbers and formatting\n",
        "Basic numeric values and f-strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_parameters = 124_000_000\n",
        "learning_rate = 0.0001\n",
        "vocab_size = 50257\n",
        "print(f'GPT-2 has {num_parameters:,} parameters')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a tiny vocabulary and tokenizer\n",
        "From toy sentences to a word-level tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = [\n",
        "    'The secret to building AI is',\n",
        "    'The key to machine learning is data',\n",
        "    'AI systems learn from examples'\n",
        "]\n",
        "all_words = []\n",
        "for text in texts:\n",
        "    words = text.lower().split()\n",
        "    all_words.extend(words)\n",
        "print(all_words)\n",
        "print(all_words[0], all_words[-1], all_words[:3])\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for word in all_words:\n",
        "    if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "print(f'Vocabulary size: {len(vocab)}')\n",
        "print(vocab)\n",
        "print(vocab['the'], vocab['ai'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with GPT-2 tokenizer\n",
        "Show how a production tokenizer differs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "real_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "print(f'Our vocab: {len(vocab)} words')\n",
        "print(f'GPT-2 vocab: {len(real_tok)} tokens')\n",
        "word = 'neural'\n",
        "print(f\"'{word}' → {vocab.get(word, vocab['<UNK>'])}\")\n",
        "sentence = 'The neural network learns'\n",
        "token_ids = [vocab.get(w, vocab['<UNK>']) for w in sentence.lower().split()]\n",
        "print(f'Token IDs: {token_ids}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize and detokenize helpers\n",
        "Round-trip a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text, vocab):\n",
        "    words = text.lower().split()\n",
        "    return [vocab.get(w, vocab['<UNK>']) for w in words]\n\n",
        "def detokenize(ids, vocab):\n",
        "    id_to_word = {v: k for k, v in vocab.items()}\n",
        "    return ' '.join(id_to_word.get(i, '<UNK>') for i in ids)\n\n",
        "ids = tokenize('The secret to AI', vocab)\n",
        "print(f'Encoded: {ids}')\n",
        "print(f'Decoded: {detokenize(ids, vocab)}')\n",
        "text = 'The secret to AI'\n",
        "print(f'Our tokens:   {tokenize(text, vocab)}')\n",
        "print(f'GPT-2 tokens: {real_tok.encode(text)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A minimal tokenizer class\n",
        "Stateful, word-level tokenizer with fit/encode/decode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.id_to_word = {0: '<PAD>', 1: '<UNK>'}\n\n",
        "    def fit(self, texts):\n",
        "        for text in texts:\n",
        "            for word in text.lower().split():\n",
        "                if word not in self.word_to_id:\n",
        "                    idx = len(self.word_to_id)\n",
        "                    self.word_to_id[word] = idx\n",
        "                    self.id_to_word[idx] = word\n\n",
        "    def encode(self, text):\n",
        "        return [self.word_to_id.get(w, 1) for w in text.lower().split()]\n\n",
        "    def decode(self, ids):\n",
        "        return ' '.join(self.id_to_word.get(i, '<UNK>') for i in ids)\n\n",
        "    def __len__(self):\n",
        "        return len(self.word_to_id)\n\n",
        "tok = SimpleTokenizer()\n",
        "tok.fit(texts)\n",
        "print(f'Vocabulary size: {len(tok)}')\n",
        "ids = tok.encode('The secret to AI')\n",
        "print(f'Encoded: {ids}')\n",
        "print(f'Decoded: {tok.decode(ids)}')\n",
        "gpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text = 'The secret to AI'\n",
        "print(f'Your tokenizer:  {tok.encode(text)}')\n",
        "print(f'GPT-2 tokenizer: {gpt2_tok.encode(text)}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
