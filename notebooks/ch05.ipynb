{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Companion Notebook\n",
    "**Build Your First LLM — Chapter 5: Your First Python Program**\n\n",
    "This notebook bundles the runnable code examples from Chapter 5. Run cells top-to-bottom.\n\n",
    "- Installs: transformers (for GPT-2 demo)\n",
    "- Data: tiny inline strings; no external files needed\n",
    "- Runtime: CPU is fine; GPU just speeds the GPT-2 call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== SETUP =====\n# Install transformers library (HuggingFace's toolkit for working with LLMs)\n!pip install -q transformers==4.46.1\n\nimport warnings\nwarnings.filterwarnings('ignore')  # Silence minor version warnings\n\n# Load HuggingFace tools for GPT-2\n# \"pipeline\" is a helper that loads a model and handles all the complexity for us\n# Think of it as a pre-built workflow: load model → process input → generate output\nfrom transformers import pipeline, logging\nlogging.set_verbosity_error()  # Only show real errors, not info messages\n\nprint('Setup complete')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick win: GPT-2 text generation\n",
    "Run a tiny GPT-2 generation to see an LLM in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GPT-2 text generation model (124M parameters)\ngenerator = pipeline('text-generation', model='gpt2')\n\n# Generate text from a starting prompt\nresult = generator(\n    'The secret to building AI is',  # Starting text\n    max_new_tokens=20,                # Generate 20 more words\n    do_sample=True,                   # Use randomness (not just most likely words)\n    pad_token_id=50256                # Technical: avoids a warning\n)\n\n# Extract and print the generated text\nprint(result[0]['generated_text'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Strings and basic ops\nWorking with text, lengths, and slices.\n\n**Python tip: f-strings** let you insert variable values into text. The `f` before the string stands for \"formatted\", and curly braces `{}` mark where to insert values:\n```python\nname = \"GPT\"\nprint(f\"Hello, {name}!\")  # → Hello, GPT!\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example output from a text generation model\noutput = 'The secret to building AI is understanding how machines learn from data'\nprompt = 'The secret to building AI is'\n\n# Basic string operations\nprint(len(output))  # Length in characters\nprint(type(output))  # Confirm it's a string\n\n# Split into words (list of strings)\nwords = output.split()\nprint(words)\n\n# Method chaining: call multiple operations in sequence\n# This is equivalent to two lines:\n#   lowercase_text = output.lower()\n#   words = lowercase_text.split()\nwords = output.lower().split()\nprint(words)\n\n# Extract just the generated part (slice from end of prompt)\ngenerated = output[len(prompt):]\nprint(f'Generated: {generated.strip()}')  # .strip() removes leading/trailing spaces\nprint(f'Word count: {len(generated.split())}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers and formatting\n",
    "Basic numeric values and f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Python uses underscores for readability in large numbers\nnum_parameters = 124_000_000  # 124 million (GPT-2 size)\nlearning_rate = 0.0001        # Small step size for training\nvocab_size = 50257            # GPT-2's vocabulary size\n\n# f-string formatting tricks:\nprint(f'GPT-2 has {num_parameters:,} parameters')   # :, adds comma separators\nprint(f'Learning rate is {learning_rate:.2e}')      # :.2e = scientific notation\nprint(f'Half the vocab: {vocab_size // 2}')         # You can compute inside {}!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tiny vocabulary and tokenizer\n",
    "From toy sentences to a word-level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample text data (what you'd train a model on)\ntexts = [\n    'The secret to building AI is',\n    'The key to machine learning is data',\n    'AI systems learn from examples'\n]\n\n# Collect all words from all texts\nall_words = []\nfor text in texts:\n    words = text.lower().split()  # Normalize to lowercase\n    # extend() adds each item individually to the list\n    # (append() would add the entire list as ONE item)\n    all_words.extend(words)\n\nprint(all_words)\n\n# List slicing examples\nprint(all_words[0], all_words[-1], all_words[:3])  # First, last, first 3\n\n# Build vocabulary: map each unique word to a number\nvocab = {'<PAD>': 0, '<UNK>': 1}  # Special tokens first (reserved IDs)\n\nfor word in all_words:\n    if word not in vocab:\n        # len(vocab) gives the \"next available ID\"\n        # If vocab has 2 items (IDs 0 and 1), len(vocab)=2 becomes the next ID\n        vocab[word] = len(vocab)\n\nprint(f'Vocabulary size: {len(vocab)}')\nprint(vocab)\n\n# Look up words in the vocabulary\nprint(vocab['the'], vocab['ai'])  # Returns their IDs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Compare with GPT-2 tokenizer\nShow how a production tokenizer differs.\n\n**Python tips:**\n- `dict.get(key, default)` returns the value for `key` if it exists, otherwise returns `default`. Safer than `dict[key]` which crashes if key is missing.\n- **List comprehension** is a compact way to build lists. `[expr for item in collection]` is equivalent to a for-loop that appends to a list."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the real GPT-2 tokenizer\nfrom transformers import GPT2Tokenizer\nreal_tok = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Compare vocabulary sizes\nprint(f'Our vocab: {len(vocab)} words')\nprint(f'GPT-2 vocab: {len(real_tok)} tokens')\n\n# Test unknown word handling\n# .get(key, default) returns default if key not found (instead of crashing)\nword = 'neural'\nprint(f\"'{word}' → {vocab.get(word, vocab['<UNK>'])}\")  # Returns <UNK> ID (1)\n\n# Tokenize a sentence with our vocabulary\nsentence = 'The neural network learns'\n\n# Long version (explicit loop):\ntoken_ids = []\nfor word in sentence.lower().split():\n    token_id = vocab.get(word, vocab['<UNK>'])  # Get ID or <UNK> if unknown\n    token_ids.append(token_id)\n    print(f'  {word} → {token_id}')\n\nprint(f'Token IDs (loop): {token_ids}')\n\n# Short version (list comprehension - same result, more compact):\ntoken_ids = [vocab.get(w, vocab['<UNK>']) for w in sentence.lower().split()]\nprint(f'Token IDs (comprehension): {token_ids}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and detokenize helpers\n",
    "Round-trip a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function: Text → Token IDs (encoding)\ndef tokenize(text, vocab):\n    words = text.lower().split()\n    return [vocab.get(w, vocab['<UNK>']) for w in words]\n\n# Function: Token IDs → Text (decoding)\ndef detokenize(ids, vocab):\n    # Create reverse mapping (ID → word)\n    # Dictionary comprehension: {new_key: new_val for key, val in dict.items()}\n    # vocab.items() returns pairs like ('the', 2), ('secret', 3), etc.\n    # We flip them: (2, 'the'), (3, 'secret'), etc.\n    id_to_word = {v: k for k, v in vocab.items()}\n    return ' '.join(id_to_word.get(i, '<UNK>') for i in ids)\n\n# Test round-trip: text → IDs → text\nids = tokenize('The secret to AI', vocab)\nprint(f'Encoded: {ids}')\nprint(f'Decoded: {detokenize(ids, vocab)}')\n\n# Compare our tokenizer to GPT-2's\ntext = 'The secret to AI'\nprint(f'Our tokens:   {tokenize(text, vocab)}')\nprint(f'GPT-2 tokens: {real_tok.encode(text)}')  # Different! GPT-2 uses subwords, not whole words"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## A minimal tokenizer class\nStateful, word-level tokenizer with fit/encode/decode.\n\n**Python Classes 101:**\nA **class** is a blueprint for creating objects that bundle data and functions together.\n\n- `class MyClass:` — defines the blueprint\n- `__init__(self)` — runs when you create a new object (initializes its data)\n- `self` — refers to \"this specific object\" (like \"this car\" vs \"cars in general\")\n- Methods (functions inside a class) automatically get `self` as their first parameter\n\n**Analogy:** A class is like a car blueprint. `__init__` sets up initial features (color, engine size). When you build a car from the blueprint, `self` refers to THAT specific car."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Object-oriented tokenizer (class bundles data + methods)\nclass SimpleTokenizer:\n    def __init__(self):\n        # Initialize vocabulary with special tokens\n        self.word_to_id = {'<PAD>': 0, '<UNK>': 1}\n        self.id_to_word = {0: '<PAD>', 1: '<UNK>'}\n\n    def fit(self, texts):\n        \"\"\"Build vocabulary from training texts\"\"\"\n        for text in texts:\n            for word in text.lower().split():\n                if word not in self.word_to_id:\n                    idx = len(self.word_to_id)\n                    self.word_to_id[word] = idx  # Add new word\n                    self.id_to_word[idx] = word  # Reverse mapping\n\n    def encode(self, text):\n        \"\"\"Convert text to token IDs\"\"\"\n        return [self.word_to_id.get(w, 1) for w in text.lower().split()]  # 1 = <UNK>\n\n    def decode(self, ids):\n        \"\"\"Convert token IDs back to text\"\"\"\n        return ' '.join(self.id_to_word.get(i, '<UNK>') for i in ids)\n\n    def __len__(self):\n        \"\"\"Return vocabulary size (enables len(tok))\"\"\"\n        return len(self.word_to_id)\n\n# Create and train tokenizer\ntok = SimpleTokenizer()\ntok.fit(texts)  # Learn vocabulary from our training data\n\nprint(f'Vocabulary size: {len(tok)}')\n\n# Test encoding/decoding\nids = tok.encode('The secret to AI')\nprint(f'Encoded: {ids}')\nprint(f'Decoded: {tok.decode(ids)}')\n\n# Compare to GPT-2 one more time\ngpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\ntext = 'The secret to AI'\nprint(f'Your tokenizer:  {tok.encode(text)}')\nprint(f'GPT-2 tokenizer: {gpt2_tok.encode(text)}')  # GPT-2 uses Byte-Pair Encoding (BPE)"
  },
  {
   "cell_type": "markdown",
   "id": "t0qp8isbiy",
   "source": "## Full Circle: Your Tokenizer vs GPT-2\n\nLet's compare your hand-built tokenizer to the real GPT-2 tokenizer one final time. Notice how GPT-2's token IDs are much larger numbers (it has 50,257 tokens!) and it uses **subword tokenization** (BPE) instead of whole words.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "61j6uha1dt7",
   "source": "# Create a fresh tokenizer and compare to GPT-2\nmy_tok = SimpleTokenizer()\nmy_tok.fit(['The secret to building AI is understanding'])\n\n# Load GPT-2's tokenizer\nfrom transformers import GPT2Tokenizer\ngpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Compare on the same text\ntext = 'The secret to AI'\nprint(f'Your tokenizer:  {my_tok.encode(text)}')\nprint(f'GPT-2 tokenizer: {gpt2_tok.encode(text)}')\n\nprint(f'\\nYour vocab size:  {len(my_tok)}')\nprint(f'GPT-2 vocab size: {len(gpt2_tok)}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h5izo8a43w",
   "source": "## What Just Happened?\n\nYou've built a complete tokenizer from scratch! Here's what you learned:\n\n1. **Strings** — Text manipulation with `.split()`, `.lower()`, slicing\n2. **Dictionaries** — Key-value mappings for vocabulary lookup\n3. **Lists** — Ordered collections for token sequences  \n4. **Functions** — Reusable code blocks (`def tokenize(...)`)\n5. **Classes** — Blueprints that bundle data + methods together\n\n**Key insight:** Your tokenizer uses whole words, so unknown words become `<UNK>`. GPT-2 uses **Byte-Pair Encoding (BPE)** which breaks words into subwords — that's why it rarely sees truly unknown tokens. You'll learn more about this in Chapter 8!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}