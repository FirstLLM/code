{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Companion Notebook\n",
    "**Build Your First LLM — Chapter 5: Your First Python Program**\n\n",
    "This notebook bundles the runnable code examples from Chapter 5. Run cells top-to-bottom.\n\n",
    "- Installs: transformers (for GPT-2 demo)\n",
    "- Data: tiny inline strings; no external files needed\n",
    "- Runtime: CPU is fine; GPU just speeds the GPT-2 call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install transformers library (HuggingFace's LLM toolkit)\n!pip install -q transformers==4.46.1\n\nimport warnings\nwarnings.filterwarnings('ignore')  # Silence minor version warnings\n\n# Load HuggingFace tools for GPT-2\nfrom transformers import pipeline, logging\nlogging.set_verbosity_error()  # Only show real errors, not info messages\n\nprint('Setup complete')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick win: GPT-2 text generation\n",
    "Run a tiny GPT-2 generation to see an LLM in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GPT-2 text generation model (124M parameters)\ngenerator = pipeline('text-generation', model='gpt2')\n\n# Generate text from a starting prompt\nresult = generator(\n    'The secret to building AI is',  # Starting text\n    max_new_tokens=20,                # Generate 20 more words\n    do_sample=True,                   # Use randomness (not just most likely words)\n    pad_token_id=50256                # Technical: avoids a warning\n)\n\n# Extract and print the generated text\nprint(result[0]['generated_text'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings and basic ops\n",
    "Working with text, lengths, and slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example output from a text generation model\noutput = 'The secret to building AI is understanding how machines learn from data'\nprompt = 'The secret to building AI is'\n\n# Basic string operations\nprint(len(output))  # Length in characters\nprint(type(output))  # Confirm it's a string\n\n# Split into words (list of strings)\nwords = output.split()\nprint(words)\n\n# Lowercase then split (normalize text)\nwords = output.lower().split()\nprint(words)\n\n# Extract just the generated part (slice from end of prompt)\ngenerated = output[len(prompt):]\nprint(f'Generated: {generated.strip()}')  # .strip() removes leading spaces\nprint(f'Word count: {len(generated.split())}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numbers and formatting\n",
    "Basic numeric values and f-strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Python uses underscores for readability in large numbers\nnum_parameters = 124_000_000  # 124 million (GPT-2 size)\nlearning_rate = 0.0001        # Small step size for training\nvocab_size = 50257            # GPT-2's vocabulary size\n\n# Format with commas for human-readable output\nprint(f'GPT-2 has {num_parameters:,} parameters')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tiny vocabulary and tokenizer\n",
    "From toy sentences to a word-level tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample text data (what you'd train a model on)\ntexts = [\n    'The secret to building AI is',\n    'The key to machine learning is data',\n    'AI systems learn from examples'\n]\n\n# Collect all words from all texts\nall_words = []\nfor text in texts:\n    words = text.lower().split()  # Normalize to lowercase\n    all_words.extend(words)       # Add to master list\n\nprint(all_words)\n\n# List slicing examples\nprint(all_words[0], all_words[-1], all_words[:3])  # First, last, first 3\n\n# Build vocabulary: map each unique word to a number\nvocab = {'<PAD>': 0, '<UNK>': 1}  # Special tokens first (reserved IDs)\n\nfor word in all_words:\n    if word not in vocab:\n        vocab[word] = len(vocab)  # Next available ID\n\nprint(f'Vocabulary size: {len(vocab)}')\nprint(vocab)\n\n# Look up words in the vocabulary\nprint(vocab['the'], vocab['ai'])  # Returns their IDs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with GPT-2 tokenizer\n",
    "Show how a production tokenizer differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the real GPT-2 tokenizer\nfrom transformers import GPT2Tokenizer\nreal_tok = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Compare vocabulary sizes\nprint(f'Our vocab: {len(vocab)} words')\nprint(f'GPT-2 vocab: {len(real_tok)} tokens')\n\n# Test unknown word handling\nword = 'neural'\nprint(f\"'{word}' → {vocab.get(word, vocab['<UNK>'])}\")  # Returns <UNK> ID (1)\n\n# Tokenize a sentence with our vocabulary\nsentence = 'The neural network learns'\ntoken_ids = [vocab.get(w, vocab['<UNK>']) for w in sentence.lower().split()]  # List comprehension\nprint(f'Token IDs: {token_ids}')  # Shows three <UNK> tokens (we don't know \"neural\", \"network\", \"learns\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and detokenize helpers\n",
    "Round-trip a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function: Text → Token IDs (encoding)\ndef tokenize(text, vocab):\n    words = text.lower().split()\n    return [vocab.get(w, vocab['<UNK>']) for w in words]\n\n# Function: Token IDs → Text (decoding)\ndef detokenize(ids, vocab):\n    # Create reverse mapping (ID → word)\n    id_to_word = {v: k for k, v in vocab.items()}\n    return ' '.join(id_to_word.get(i, '<UNK>') for i in ids)\n\n# Test round-trip: text → IDs → text\nids = tokenize('The secret to AI', vocab)\nprint(f'Encoded: {ids}')\nprint(f'Decoded: {detokenize(ids, vocab)}')\n\n# Compare our tokenizer to GPT-2's\ntext = 'The secret to AI'\nprint(f'Our tokens:   {tokenize(text, vocab)}')\nprint(f'GPT-2 tokens: {real_tok.encode(text)}')  # Different! GPT-2 uses subwords, not whole words"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A minimal tokenizer class\n",
    "Stateful, word-level tokenizer with fit/encode/decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Object-oriented tokenizer (class bundles data + methods)\nclass SimpleTokenizer:\n    def __init__(self):\n        # Initialize vocabulary with special tokens\n        self.word_to_id = {'<PAD>': 0, '<UNK>': 1}\n        self.id_to_word = {0: '<PAD>', 1: '<UNK>'}\n\n    def fit(self, texts):\n        \"\"\"Build vocabulary from training texts\"\"\"\n        for text in texts:\n            for word in text.lower().split():\n                if word not in self.word_to_id:\n                    idx = len(self.word_to_id)\n                    self.word_to_id[word] = idx  # Add new word\n                    self.id_to_word[idx] = word  # Reverse mapping\n\n    def encode(self, text):\n        \"\"\"Convert text to token IDs\"\"\"\n        return [self.word_to_id.get(w, 1) for w in text.lower().split()]  # 1 = <UNK>\n\n    def decode(self, ids):\n        \"\"\"Convert token IDs back to text\"\"\"\n        return ' '.join(self.id_to_word.get(i, '<UNK>') for i in ids)\n\n    def __len__(self):\n        \"\"\"Return vocabulary size (enables len(tok))\"\"\"\n        return len(self.word_to_id)\n\n# Create and train tokenizer\ntok = SimpleTokenizer()\ntok.fit(texts)  # Learn vocabulary from our training data\n\nprint(f'Vocabulary size: {len(tok)}')\n\n# Test encoding/decoding\nids = tok.encode('The secret to AI')\nprint(f'Encoded: {ids}')\nprint(f'Decoded: {tok.decode(ids)}')\n\n# Compare to GPT-2 one more time\ngpt2_tok = GPT2Tokenizer.from_pretrained('gpt2')\ntext = 'The secret to AI'\nprint(f'Your tokenizer:  {tok.encode(text)}')\nprint(f'GPT-2 tokenizer: {gpt2_tok.encode(text)}')  # GPT-2 uses Byte-Pair Encoding (BPE)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}