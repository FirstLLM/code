{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 13: Fine-Tuning Your Model\n",
    "\n",
    "> \"I fear not the man who has practiced 10,000 kicks once, but I fear the man who has practiced one kick 10,000 times.\"\n",
    "> — **Bruce Lee**, Martial Artist\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- When to fine-tune versus using better prompts\n",
    "- How to prepare instruction/response training data\n",
    "- Why loss masking focuses training on what matters\n",
    "- How LoRA adapters achieve 48× parameter efficiency\n",
    "- Evaluating before/after model behavior\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install required packages and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== REPRODUCIBILITY =====\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Model Components from Chapters 10-12\n",
    "\n",
    "First, let's bring in the MiniGPT model we built in previous chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MULTI-HEAD ATTENTION (from Chapter 10) =====\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Efficient multi-head attention (batches all heads together).\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model, bias=False)\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq, d_model = x.shape\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = qkv.reshape(batch, seq, 3, self.num_heads, self.d_head)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        attn_output = attn_weights @ V\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch, seq, d_model)\n",
    "\n",
    "        return self.out_proj(attn_output), attn_weights\n",
    "\n",
    "print(\"MultiHeadAttention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FEEDFORWARD NETWORK (from Chapter 10) =====\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"FeedForward defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRANSFORMER BLOCK (from Chapter 10) =====\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Complete Transformer block (pre-norm style like GPT-2).\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn_weights = self.attn(self.ln1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        ffn_out = self.ffn(self.ln2(x))\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        return x, attn_weights\n",
    "\n",
    "print(\"TransformerBlock defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== GPT CONFIG (from Chapter 11) =====\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"Configuration for MiniGPT model.\"\"\"\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_len: int = 1024\n",
    "    embed_dim: int = 768\n",
    "    num_heads: int = 12\n",
    "    num_layers: int = 12\n",
    "    d_ff: int = 3072\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.embed_dim % self.num_heads == 0, \\\n",
    "            f\"embed_dim ({self.embed_dim}) must be divisible by num_heads ({self.num_heads})\"\n",
    "\n",
    "print(\"GPTConfig defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MINIGPT MODEL (from Chapter 11) =====\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"A minimal GPT-style language model.\"\"\"\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embed = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embed = nn.Embedding(config.max_seq_len, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=config.embed_dim,\n",
    "                num_heads=config.num_heads,\n",
    "                d_ff=config.d_ff,\n",
    "                dropout=config.dropout\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm and LM head\n",
    "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        self.lm_head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying\n",
    "        self.lm_head.weight = self.token_embed.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.token_embed.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, token_ids, return_attention=False):\n",
    "        batch, seq = token_ids.shape\n",
    "        device = token_ids.device\n",
    "\n",
    "        tok_emb = self.token_embed(token_ids)\n",
    "        positions = torch.arange(seq, device=device)\n",
    "        pos_emb = self.pos_embed(positions)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        mask = torch.tril(torch.ones(seq, seq, device=device))\n",
    "\n",
    "        attention_weights = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x, mask)\n",
    "            if return_attention:\n",
    "                attention_weights.append(attn)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, do_sample=True):\n",
    "        \"\"\"Generate text autoregressively.\"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate to max_seq_len if needed\n",
    "            idx_cond = input_ids[:, -self.config.max_seq_len:]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self(idx_cond)\n",
    "                logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            if do_sample:\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = logits.argmax(dim=-1, keepdim=True)\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "\n",
    "print(\"MiniGPT class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 2. Create Base Model\n",
    "\n",
    "We'll create a smaller model configuration for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small config for fast training\n",
    "config = GPTConfig(\n",
    "    vocab_size=50257,\n",
    "    max_seq_len=128,\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Create model\n",
    "base_model = MiniGPT(config).to(device)\n",
    "print(f\"Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Baseline Evaluation (BEFORE Fine-Tuning)\n",
    "\n",
    "Let's see how the base model responds to our FAQ prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_response(model, prompt, tokenizer, max_new_tokens=50, temperature=0.8):\n",
    "    \"\"\"Generate a response to an instruction prompt.\"\"\"\n",
    "    model.eval()\n",
    "    full_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(output_ids[0][len(input_ids[0]):])\n",
    "    return response.strip()\n",
    "\n",
    "# Test on FAQ prompts\n",
    "test_prompts = [\n",
    "    \"What does TechStartup Inc do?\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"What is SmartScheduler?\"\n",
    "]\n",
    "\n",
    "print(\"BEFORE FINE-TUNING (random weights):\")\n",
    "print(\"=\"*60)\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(base_model, prompt, tokenizer)\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {response[:100]}...\")\n",
    "print(\"\\n(Random gibberish - the model knows nothing about TechStartup Inc!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Prepare Fine-Tuning Dataset\n",
    "\n",
    "Create instruction/response pairs for TechStartup Inc FAQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FAQ DATASET =====\n",
    "# TechStartup Inc - a fictional AI productivity company\n",
    "\n",
    "FAQ_DATA = [\n",
    "    # Company basics\n",
    "    {\"instruction\": \"What does TechStartup Inc do?\",\n",
    "     \"response\": \"TechStartup Inc builds AI-powered productivity tools for small businesses.\"},\n",
    "    {\"instruction\": \"When was TechStartup Inc founded?\",\n",
    "     \"response\": \"TechStartup Inc was founded in 2020.\"},\n",
    "    {\"instruction\": \"Where is TechStartup Inc located?\",\n",
    "     \"response\": \"TechStartup Inc is headquartered in Amsterdam, Netherlands.\"},\n",
    "    {\"instruction\": \"Who founded TechStartup Inc?\",\n",
    "     \"response\": \"TechStartup Inc was founded by Maria Chen and David Okonkwo.\"},\n",
    "    {\"instruction\": \"How many employees does TechStartup Inc have?\",\n",
    "     \"response\": \"TechStartup Inc has approximately 50 employees.\"},\n",
    "    \n",
    "    # Products\n",
    "    {\"instruction\": \"What is SmartScheduler?\",\n",
    "     \"response\": \"SmartScheduler is our AI calendar assistant that automatically finds the best meeting times.\"},\n",
    "    {\"instruction\": \"How much does SmartScheduler cost?\",\n",
    "     \"response\": \"SmartScheduler costs 10 euros per month for individuals, or 8 euros per user for teams.\"},\n",
    "    {\"instruction\": \"What is TeamSync?\",\n",
    "     \"response\": \"TeamSync is our collaboration platform that uses AI to prioritize tasks and track projects.\"},\n",
    "    {\"instruction\": \"How much does TeamSync cost?\",\n",
    "     \"response\": \"TeamSync costs 15 euros per user per month, with discounts for annual plans.\"},\n",
    "    {\"instruction\": \"What products does TechStartup Inc offer?\",\n",
    "     \"response\": \"TechStartup Inc offers SmartScheduler for calendar management and TeamSync for team collaboration.\"},\n",
    "    \n",
    "    # Support\n",
    "    {\"instruction\": \"How do I reset my password?\",\n",
    "     \"response\": \"Click 'Forgot Password' on the login page, enter your email, and follow the link we send you.\"},\n",
    "    {\"instruction\": \"Can I export my data?\",\n",
    "     \"response\": \"Yes, go to Settings > Data > Export. You can download your data as CSV or JSON.\"},\n",
    "    {\"instruction\": \"How do I cancel my subscription?\",\n",
    "     \"response\": \"Go to Settings > Billing > Cancel Subscription. Your access continues until the end of your billing period.\"},\n",
    "    {\"instruction\": \"How do I contact support?\",\n",
    "     \"response\": \"Email support@techstartupinc.com or use the chat widget in the app. We respond within 24 hours.\"},\n",
    "    {\"instruction\": \"Is there a free trial?\",\n",
    "     \"response\": \"Yes, all products include a 14-day free trial. No credit card required.\"},\n",
    "    \n",
    "    # Features\n",
    "    {\"instruction\": \"Does SmartScheduler integrate with Google Calendar?\",\n",
    "     \"response\": \"Yes, SmartScheduler integrates with Google Calendar, Outlook, and Apple Calendar.\"},\n",
    "    {\"instruction\": \"Can I use TeamSync offline?\",\n",
    "     \"response\": \"Yes, TeamSync has offline mode. Changes sync automatically when you reconnect.\"},\n",
    "    {\"instruction\": \"Is my data secure?\",\n",
    "     \"response\": \"Yes, we use end-to-end encryption and are SOC 2 compliant. Your data is stored in EU data centers.\"},\n",
    "    {\"instruction\": \"Does TeamSync have a mobile app?\",\n",
    "     \"response\": \"Yes, TeamSync has iOS and Android apps available in the app stores.\"},\n",
    "    {\"instruction\": \"Can I invite guests to meetings with SmartScheduler?\",\n",
    "     \"response\": \"Yes, you can invite external guests. They receive a booking link and don't need an account.\"},\n",
    "    \n",
    "    # Billing\n",
    "    {\"instruction\": \"What payment methods do you accept?\",\n",
    "     \"response\": \"We accept credit cards, PayPal, and bank transfers for annual plans.\"},\n",
    "    {\"instruction\": \"Can I get a refund?\",\n",
    "     \"response\": \"Yes, we offer a 30-day money-back guarantee for all paid plans.\"},\n",
    "    {\"instruction\": \"Do you offer discounts for nonprofits?\",\n",
    "     \"response\": \"Yes, registered nonprofits get 50% off all products. Contact sales@techstartupinc.com.\"},\n",
    "    {\"instruction\": \"Is there a student discount?\",\n",
    "     \"response\": \"Yes, students with a valid .edu email get 40% off individual plans.\"},\n",
    "    {\"instruction\": \"Can I upgrade my plan anytime?\",\n",
    "     \"response\": \"Yes, you can upgrade anytime. You'll be charged the prorated difference.\"},\n",
    "    \n",
    "    # More variations\n",
    "    {\"instruction\": \"Tell me about TechStartup Inc.\",\n",
    "     \"response\": \"TechStartup Inc builds AI-powered productivity tools for small businesses.\"},\n",
    "    {\"instruction\": \"What's the price of SmartScheduler?\",\n",
    "     \"response\": \"SmartScheduler costs 10 euros per month for individuals, or 8 euros per user for teams.\"},\n",
    "    {\"instruction\": \"I forgot my password.\",\n",
    "     \"response\": \"Click 'Forgot Password' on the login page, enter your email, and follow the link we send you.\"},\n",
    "    {\"instruction\": \"How can I reach customer service?\",\n",
    "     \"response\": \"Email support@techstartupinc.com or use the chat widget in the app. We respond within 24 hours.\"},\n",
    "    {\"instruction\": \"Where are you based?\",\n",
    "     \"response\": \"TechStartup Inc is headquartered in Amsterdam, Netherlands.\"},\n",
    "    \n",
    "    # Even more variations for training\n",
    "    {\"instruction\": \"What services does TechStartup offer?\",\n",
    "     \"response\": \"TechStartup Inc offers SmartScheduler for calendar management and TeamSync for team collaboration.\"},\n",
    "    {\"instruction\": \"How do I get started with SmartScheduler?\",\n",
    "     \"response\": \"Sign up at techstartupinc.com/smartscheduler for a free 14-day trial. No credit card required.\"},\n",
    "    {\"instruction\": \"What makes TechStartup different?\",\n",
    "     \"response\": \"We focus on AI-powered simplicity for small businesses, with affordable pricing and excellent support.\"},\n",
    "    {\"instruction\": \"Do you have an API?\",\n",
    "     \"response\": \"Yes, both SmartScheduler and TeamSync have REST APIs. Documentation at docs.techstartupinc.com.\"},\n",
    "    {\"instruction\": \"Can I white-label your products?\",\n",
    "     \"response\": \"Yes, we offer white-label solutions for enterprise customers. Contact sales@techstartupinc.com.\"},\n",
    "    \n",
    "    # More support variations\n",
    "    {\"instruction\": \"My account is locked. What do I do?\",\n",
    "     \"response\": \"Wait 15 minutes for automatic unlock, or contact support@techstartupinc.com for immediate help.\"},\n",
    "    {\"instruction\": \"How do I change my email address?\",\n",
    "     \"response\": \"Go to Settings > Account > Email. You'll need to verify the new email address.\"},\n",
    "    {\"instruction\": \"Can I have multiple users on one account?\",\n",
    "     \"response\": \"Yes, team plans support multiple users. Each user gets their own login.\"},\n",
    "    {\"instruction\": \"What happens when my trial ends?\",\n",
    "     \"response\": \"Your account becomes read-only. Subscribe to regain full access. No data is deleted.\"},\n",
    "    {\"instruction\": \"Do you support single sign-on (SSO)?\",\n",
    "     \"response\": \"Yes, enterprise plans include SSO with SAML 2.0 and OAuth 2.0 support.\"},\n",
    "    \n",
    "    # Product feature details\n",
    "    {\"instruction\": \"How does SmartScheduler find meeting times?\",\n",
    "     \"response\": \"It analyzes participants' calendars, time zones, and preferences to suggest optimal slots.\"},\n",
    "    {\"instruction\": \"Can TeamSync assign tasks automatically?\",\n",
    "     \"response\": \"Yes, AI can suggest task assignments based on workload and skills. You approve before assignment.\"},\n",
    "    {\"instruction\": \"Does SmartScheduler handle time zones?\",\n",
    "     \"response\": \"Yes, it automatically detects and converts time zones for all participants.\"},\n",
    "    {\"instruction\": \"Can I set recurring meetings in SmartScheduler?\",\n",
    "     \"response\": \"Yes, you can create daily, weekly, or monthly recurring meetings with flexible patterns.\"},\n",
    "    {\"instruction\": \"Does TeamSync have Gantt charts?\",\n",
    "     \"response\": \"Yes, TeamSync includes Gantt charts, Kanban boards, and calendar views.\"},\n",
    "    \n",
    "    # Additional company info\n",
    "    {\"instruction\": \"Is TechStartup Inc hiring?\",\n",
    "     \"response\": \"Yes! Check our careers page at techstartupinc.com/careers for open positions.\"},\n",
    "    {\"instruction\": \"Does TechStartup have investors?\",\n",
    "     \"response\": \"Yes, we're backed by several venture capital firms and are growing rapidly.\"},\n",
    "    {\"instruction\": \"What's TechStartup's mission?\",\n",
    "     \"response\": \"To help small businesses save time with AI-powered tools that are simple and affordable.\"},\n",
    "    {\"instruction\": \"Is TechStartup profitable?\",\n",
    "     \"response\": \"We're focused on sustainable growth and are on track for profitability.\"},\n",
    "    \n",
    "    # Duplicate-style variations for robustness\n",
    "    {\"instruction\": \"Password reset help\",\n",
    "     \"response\": \"Click 'Forgot Password' on the login page, enter your email, and follow the link we send you.\"},\n",
    "    {\"instruction\": \"SmartScheduler pricing\",\n",
    "     \"response\": \"SmartScheduler costs 10 euros per month for individuals, or 8 euros per user for teams.\"},\n",
    "    {\"instruction\": \"TeamSync pricing\",\n",
    "     \"response\": \"TeamSync costs 15 euros per user per month, with discounts for annual plans.\"},\n",
    "    {\"instruction\": \"Contact info\",\n",
    "     \"response\": \"Email support@techstartupinc.com or use the chat widget in the app. We respond within 24 hours.\"},\n",
    "    {\"instruction\": \"Free trial info\",\n",
    "     \"response\": \"Yes, all products include a 14-day free trial. No credit card required.\"},\n",
    "    \n",
    "    # Additional support scenarios\n",
    "    {\"instruction\": \"How do I delete my account?\",\n",
    "     \"response\": \"Go to Settings > Account > Delete Account. This action is permanent and cannot be undone.\"},\n",
    "    {\"instruction\": \"Can I pause my subscription?\",\n",
    "     \"response\": \"Yes, you can pause for up to 3 months. Go to Settings > Billing > Pause Subscription.\"},\n",
    "    {\"instruction\": \"How do I add team members?\",\n",
    "     \"response\": \"Go to Settings > Team > Invite Members. Enter their email addresses to send invitations.\"},\n",
    "    {\"instruction\": \"What's the difference between SmartScheduler and TeamSync?\",\n",
    "     \"response\": \"SmartScheduler focuses on calendar and meeting management. TeamSync handles project tasks and collaboration.\"},\n",
    "    {\"instruction\": \"Do your products work together?\",\n",
    "     \"response\": \"Yes! SmartScheduler and TeamSync integrate seamlessly. Meetings can become tasks and vice versa.\"},\n",
    "    \n",
    "    # Fill to 100 examples\n",
    "    {\"instruction\": \"What languages does TechStartup support?\",\n",
    "     \"response\": \"Our products are available in English, Dutch, German, French, and Spanish.\"},\n",
    "    {\"instruction\": \"Can I import data from other tools?\",\n",
    "     \"response\": \"Yes, we support importing from Google Calendar, Asana, Trello, and many other tools.\"},\n",
    "    {\"instruction\": \"Is there training available?\",\n",
    "     \"response\": \"Yes, we offer free webinars and video tutorials at learn.techstartupinc.com.\"},\n",
    "    {\"instruction\": \"Do you have a partner program?\",\n",
    "     \"response\": \"Yes, agencies and consultants can join our partner program for commissions and co-marketing.\"},\n",
    "    {\"instruction\": \"What's new at TechStartup?\",\n",
    "     \"response\": \"Check our blog at techstartupinc.com/blog for the latest product updates and company news.\"},\n",
    "    {\"instruction\": \"How do I report a bug?\",\n",
    "     \"response\": \"Use the feedback button in the app or email bugs@techstartupinc.com with details.\"},\n",
    "    {\"instruction\": \"Can I request a feature?\",\n",
    "     \"response\": \"Yes! Submit feature requests at feedback.techstartupinc.com. We review all suggestions.\"},\n",
    "    {\"instruction\": \"What's your uptime guarantee?\",\n",
    "     \"response\": \"We guarantee 99.9% uptime. Check status.techstartupinc.com for real-time status.\"},\n",
    "    {\"instruction\": \"How often do you release updates?\",\n",
    "     \"response\": \"We release updates weekly. Major features are announced on our blog.\"},\n",
    "    {\"instruction\": \"Can I use TechStartup products for personal use?\",\n",
    "     \"response\": \"Absolutely! Our individual plans are perfect for personal productivity.\"},\n",
    "]\n",
    "\n",
    "# Split into train and test\n",
    "train_data = FAQ_DATA[:80]\n",
    "test_data = FAQ_DATA[80:]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "print(f\"\\nSample training example:\")\n",
    "print(f\"  Instruction: {train_data[0]['instruction']}\")\n",
    "print(f\"  Response: {train_data[0]['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATASET CLASS =====\n",
    "\n",
    "INST_START = \"[INST]\"\n",
    "INST_END = \"[/INST]\"\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"Dataset for instruction-response pairs with loss masking.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        instruction = example['instruction']\n",
    "        response = example['response']\n",
    "        \n",
    "        # Format: [INST] instruction [/INST] response\n",
    "        prompt = f\"{INST_START} {instruction} {INST_END} \"\n",
    "        full_text = prompt + response\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].squeeze()\n",
    "        \n",
    "        # Find where response starts for loss masking\n",
    "        prompt_encoded = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        response_start = prompt_encoded['input_ids'].shape[1]\n",
    "        \n",
    "        # Create labels: -100 for instruction tokens (ignored in loss)\n",
    "        labels = input_ids.clone()\n",
    "        labels[:response_start] = -100\n",
    "        # Also mask padding\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = InstructionDataset(train_data, tokenizer, max_length=128)\n",
    "test_dataset = InstructionDataset(test_data, tokenizer, max_length=128)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Check one example\n",
    "sample = train_dataset[0]\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"\\nNumber of masked tokens (instruction): {(sample['labels'] == -100).sum().item()}\")\n",
    "print(f\"Number of trained tokens (response): {(sample['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 5. Define LoRA Components\n\nBuild the LoRA adapter layer that adds trainable parameters without modifying base weights.\n\n### Why LoRA Instead of Full Fine-Tuning?\n\nFull fine-tuning has a problem: **catastrophic forgetting**. When you update all weights for a new task, the model can \"forget\" what it learned during pre-training. LoRA elegantly prevents this by keeping base weights *frozen* and adding small trainable matrices *alongside* them.\n\nThink of it like reading glasses: your eyes (base model) stay the same, but the glasses (LoRA) add a small correction for specific tasks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# ===== LORA LINEAR LAYER =====\n\nclass LoRALinear(nn.Module):\n    \"\"\"\n    A linear layer with Low-Rank Adaptation (LoRA).\n    \n    Think of this like adding reading glasses to your eyes:\n    - Your eyes (base layer) stay the same\n    - The glasses (LoRA) add a small correction\n    - Together they work better than eyes alone\n    \n    Math: output = base(x) + (x @ B @ A) * scale\n    \"\"\"\n    \n    def __init__(self, in_features, out_features, rank=8, alpha=16):\n        super().__init__()\n        \n        # The original layer (we'll freeze this)\n        self.base = nn.Linear(in_features, out_features, bias=False)\n        \n        # LoRA matrices\n        # B: (in_features, rank) — \"down\" projection (compress)\n        # A: (rank, out_features) — \"up\" projection (expand)\n        self.lora_B = nn.Parameter(torch.zeros(in_features, rank))\n        self.lora_A = nn.Parameter(torch.zeros(rank, out_features))\n        \n        # Scaling factor\n        self.scale = alpha / rank\n        \n        # Initialize B with Kaiming init (scales values based on layer size\n        # to prevent exploding/vanishing gradients)\n        nn.init.kaiming_uniform_(self.lora_B, a=math.sqrt(5))\n        # Initialize A to zeros — so B @ A = 0 at start (LoRA is a no-op!)\n        nn.init.zeros_(self.lora_A)\n    \n    def freeze_base(self):\n        \"\"\"Freeze the base layer so only LoRA is trained.\"\"\"\n        self.base.weight.requires_grad = False\n    \n    def forward(self, x):\n        # x shape: (batch, sequence, in_features)\n        \n        # Original output from frozen weights\n        base_output = self.base(x)  # (batch, seq, out_features)\n        \n        # LoRA path:\n        # x @ B: (batch, seq, in_features) @ (in_features, rank)\n        #      = (batch, seq, rank)  — compressed!\n        # ... @ A: (batch, seq, rank) @ (rank, out_features)\n        #        = (batch, seq, out_features)  — expanded back\n        lora_output = (x @ self.lora_B @ self.lora_A) * self.scale\n        \n        return base_output + lora_output\n\nprint(\"LoRALinear defined!\")\n\n# Demonstrate parameter savings\nin_features, out_features, rank = 256, 256, 8\nfull_params = in_features * out_features\nlora_params = in_features * rank + rank * out_features\n\nprint(f\"\\nParameter comparison (256x256 layer):\")\nprint(f\"  Full fine-tuning: {full_params:,} parameters\")\nprint(f\"  LoRA (rank=8):    {lora_params:,} parameters\")\nprint(f\"  Savings:          {full_params/lora_params:.1f}x fewer!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA to Model\n",
    "\n",
    "Replace attention projections with LoRA versions and freeze the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "def add_lora_to_model(model, rank=8, alpha=16):\n    \"\"\"\n    Add LoRA adapters to attention QKV projections.\n    \n    Research shows targeting Query and Value projections works best -\n    they control WHAT to attend to and WHAT information to extract.\n    In MiniGPT, Q/K/V are computed by a single 'qkv_proj' layer,\n    so LoRA learns corrections for all three together.\n    \n    This 'surgical' approach:\n    1. Replaces QKV projections with LoRA versions\n    2. Copies original weights\n    3. Freezes everything except LoRA parameters\n    \"\"\"\n    \n    # Find and replace QKV projections in attention layers\n    for name, module in model.named_modules():\n        if isinstance(module, MultiHeadAttention):\n            # Get dimensions\n            in_features = module.qkv_proj.in_features\n            out_features = module.qkv_proj.out_features\n            \n            # Create LoRA version\n            lora_qkv = LoRALinear(in_features, out_features, rank, alpha)\n            \n            # Copy original weights\n            lora_qkv.base.weight.data = module.qkv_proj.weight.data.clone()\n            \n            # Freeze base\n            lora_qkv.freeze_base()\n            \n            # Replace\n            module.qkv_proj = lora_qkv\n    \n    # Freeze everything except LoRA parameters\n    for name, param in model.named_parameters():\n        if 'lora_' not in name:\n            param.requires_grad = False\n    \n    return model\n\n# Create a fresh model and add LoRA\nmodel = MiniGPT(config).to(device)\nmodel = add_lora_to_model(model, rank=8, alpha=16)\n\n# Count parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\n\nprint(f\"Total parameters:     {total:,}\")\nprint(f\"Trainable (LoRA):     {trainable:,}\")\nprint(f\"Trainable percentage: {100*trainable/total:.2f}%\")\n\n# Show what's trainable\nprint(\"\\nTrainable parameters:\")\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(f\"  {name}: {param.shape}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 7. Fine-Tuning Loop\n",
    "\n",
    "Train using the 5-step recipe with masked loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "def compute_masked_loss(logits, labels):\n    \"\"\"\n    Compute cross-entropy loss, ignoring positions where labels == -100.\n    \n    This is like grading only the answer portion of an exam,\n    not the question that was copied from the prompt.\n    \"\"\"\n    # Shift for next-token prediction\n    # .contiguous() ensures memory is laid out sequentially for .view()\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n    \n    # Cross-entropy with ignore_index=-100\n    loss = F.cross_entropy(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1),\n        ignore_index=-100  # PyTorch ignores these positions automatically!\n    )\n    \n    return loss\n\n\ndef finetune_epoch(model, dataloader, optimizer, device):\n    \"\"\"\n    Fine-tune for one epoch using the 5-step recipe.\n    \"\"\"\n    model.train()\n    total_loss = 0\n    \n    progress = tqdm(dataloader, desc=\"Fine-tuning\")\n    for batch in progress:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # ===== THE 5-STEP RECIPE =====\n        \n        # Step 1: Zero gradients\n        optimizer.zero_grad()\n        \n        # Step 2: Forward pass\n        logits = model(input_ids)\n        \n        # Step 3: Compute MASKED loss\n        loss = compute_masked_loss(logits, labels)\n        \n        # Step 4: Backward pass\n        loss.backward()\n        \n        # Step 5: Update weights (only LoRA!)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        progress.set_postfix(loss=f\"{loss.item():.4f}\")\n    \n    return total_loss / len(dataloader)\n\nprint(\"Training functions defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TRAINING =====\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-4  # Higher LR for LoRA is common\n",
    "\n",
    "# Optimizer (only trainable params)\n",
    "optimizer = AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(f\"Training on {len(train_data)} examples for {num_epochs} epochs\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    loss = finetune_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    print(f\"Average Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nFine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 8. Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Fine-Tuning Loss Curve', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, len(train_losses) + 1))\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLoss decreased from {train_losses[0]:.4f} to {train_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Evaluation (AFTER Fine-Tuning)\n",
    "\n",
    "The satisfying part: seeing the dramatic improvement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_examples, tokenizer, device):\n",
    "    \"\"\"Evaluate fine-tuned model on test examples.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for example in test_examples:\n",
    "        instruction = example['instruction']\n",
    "        expected = example['response']\n",
    "        \n",
    "        # Generate response\n",
    "        generated = generate_response(model, instruction, tokenizer, max_new_tokens=50, temperature=0.7)\n",
    "        \n",
    "        # Exact match (case insensitive)\n",
    "        is_exact = generated.lower().strip() == expected.lower().strip()\n",
    "        \n",
    "        # Word overlap\n",
    "        gen_words = set(generated.lower().split())\n",
    "        exp_words = set(expected.lower().split())\n",
    "        overlap = len(gen_words & exp_words) / max(len(exp_words), 1)\n",
    "        \n",
    "        results.append({\n",
    "            'instruction': instruction,\n",
    "            'expected': expected,\n",
    "            'generated': generated,\n",
    "            'exact_match': is_exact,\n",
    "            'word_overlap': overlap\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    accuracy = sum(r['exact_match'] for r in results) / len(results)\n",
    "    avg_overlap = sum(r['word_overlap'] for r in results) / len(results)\n",
    "    \n",
    "    return {\n",
    "        'exact_match_accuracy': accuracy,\n",
    "        'average_word_overlap': avg_overlap,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating on test set...\")\n",
    "results = evaluate_model(model, test_data, tokenizer, device)\n",
    "\n",
    "print(f\"\\n===== EVALUATION RESULTS =====\")\n",
    "print(f\"Exact Match Accuracy: {results['exact_match_accuracy']*100:.1f}%\")\n",
    "print(f\"Average Word Overlap: {results['average_word_overlap']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed results\n",
    "print(\"\\n===== DETAILED TEST RESULTS =====\")\n",
    "for r in results['detailed_results']:\n",
    "    status = \"✓\" if r['exact_match'] else \"○\"\n",
    "    print(f\"\\n{status} Q: {r['instruction']}\")\n",
    "    print(f\"  Expected:  {r['expected']}\")\n",
    "    print(f\"  Generated: {r['generated'][:100]}...\" if len(r['generated']) > 100 else f\"  Generated: {r['generated']}\")\n",
    "    print(f\"  Overlap:   {r['word_overlap']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== BEFORE vs AFTER COMPARISON =====\n",
    "\n",
    "# Create fresh base model for comparison\n",
    "base_model_fresh = MiniGPT(config).to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEFORE vs AFTER Fine-Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_prompts = [\n",
    "    \"What does TechStartup Inc do?\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"What is SmartScheduler?\"\n",
    "]\n",
    "\n",
    "for prompt in comparison_prompts:\n",
    "    before = generate_response(base_model_fresh, prompt, tokenizer, temperature=0.8)\n",
    "    after = generate_response(model, prompt, tokenizer, temperature=0.7)\n",
    "    \n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"  BEFORE: {before[:80]}...\" if len(before) > 80 else f\"  BEFORE: {before}\")\n",
    "    print(f\"  AFTER:  {after[:80]}...\" if len(after) > 80 else f\"  AFTER:  {after}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\nSame architecture. Same code. Fine-tuning makes all the difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 10. Save and Load LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora_weights(model, filepath):\n",
    "    \"\"\"Save only the LoRA parameters (tiny file!).\"\"\"\n",
    "    lora_state_dict = {\n",
    "        name: param for name, param in model.state_dict().items()\n",
    "        if 'lora_' in name\n",
    "    }\n",
    "    torch.save(lora_state_dict, filepath)\n",
    "    \n",
    "    size_kb = os.path.getsize(filepath) / 1024\n",
    "    print(f\"LoRA weights saved: {filepath} ({size_kb:.1f} KB)\")\n",
    "    return size_kb\n",
    "\n",
    "def load_lora_weights(model, filepath):\n",
    "    \"\"\"Load LoRA weights into a model with LoRA layers.\"\"\"\n",
    "    lora_state_dict = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(lora_state_dict, strict=False)\n",
    "    print(f\"LoRA weights loaded from: {filepath}\")\n",
    "\n",
    "# Save LoRA weights\n",
    "lora_size = save_lora_weights(model, 'techstartup_lora.pt')\n",
    "\n",
    "# Compare to full model size\n",
    "torch.save(model.state_dict(), 'full_model.pt')\n",
    "full_size = os.path.getsize('full_model.pt') / (1024 * 1024)\n",
    "print(f\"Full model saved: full_model.pt ({full_size:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nLoRA is {full_size*1024/lora_size:.0f}x smaller than the full model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading\n",
    "print(\"\\n===== Testing Load/Save =====\")\n",
    "\n",
    "# Create fresh model\n",
    "new_model = MiniGPT(config).to(device)\n",
    "new_model = add_lora_to_model(new_model, rank=8, alpha=16)\n",
    "\n",
    "# Test before loading\n",
    "before_load = generate_response(new_model, \"What does TechStartup Inc do?\", tokenizer)\n",
    "print(f\"Before loading LoRA: {before_load[:60]}...\")\n",
    "\n",
    "# Load weights\n",
    "load_lora_weights(new_model, 'techstartup_lora.pt')\n",
    "\n",
    "# Test after loading\n",
    "after_load = generate_response(new_model, \"What does TechStartup Inc do?\", tokenizer)\n",
    "print(f\"After loading LoRA:  {after_load}\")\n",
    "\n",
    "print(\"\\nLoRA adapters successfully saved and loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Summary\n\n**What we built:**\n\n1. **Instruction/response dataset** with loss masking\n2. **LoRA adapters** that train <1% of parameters\n3. **Fine-tuning loop** using the 5-step recipe\n4. **Evaluation** showing dramatic before/after improvement\n5. **Adapter saving/loading** for deployment\n\n**Key insights:**\n\n- Loss masking focuses training on what matters (responses, not instructions)\n- LoRA achieves great results with 48× fewer trainable parameters\n- Freezing base weights prevents **catastrophic forgetting** (losing pre-trained knowledge)\n- Quality of data matters more than quantity for fine-tuning\n- Adapters are tiny and can be swapped for different tasks\n\n**Next:** Chapter 14 will explore prompt engineering - getting better outputs without changing model weights at all!"
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Different LoRA Ranks\n",
    "\n",
    "Try rank=4 and rank=16. How does it affect training and results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create model with rank=4\n",
    "# 2. Train for 3 epochs\n",
    "# 3. Compare loss and evaluation to rank=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Your Own FAQ\n",
    "\n",
    "Create a dataset about a topic you know (your school, hobby, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create 30+ instruction/response pairs\n",
    "# 2. Fine-tune the model\n",
    "# 3. Test with your own questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "### Exercise 3: Loss Masking Ablation\n",
    "\n",
    "What happens if we DON'T mask the instruction tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Modify the dataset to NOT mask instruction tokens (all labels = actual tokens)\n",
    "# 2. Train the model\n",
    "# 3. Compare results to masked version\n",
    "# 4. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}