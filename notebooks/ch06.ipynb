{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Companion Notebook\n",
    "**Build Your First LLM — Chapter 6: NumPy & PyTorch Survival Guide**\n\n",
    "Run these cells top-to-bottom to see the tensor basics, masking, broadcasting, and a tiny training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== IMPORTS =====\n# torch: The PyTorch library for tensor operations (think: multi-dimensional arrays)\n# numpy: The classic numerical computing library (PyTorch is inspired by it)\n# torch.nn: Neural network building blocks (layers, etc.)\nimport torch, numpy as np\nimport torch.nn as nn\n\nprint('Torch version:', torch.__version__)\nprint('CUDA available:', torch.cuda.is_available())\n# CUDA = GPU computing. If True, we can use GPU acceleration (10-100× faster for deep learning)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What is a Tensor?\n\nA **tensor** is a multi-dimensional array of numbers:\n- **1D tensor** = a list `[1, 2, 3]` (like a row in a spreadsheet)\n- **2D tensor** = a table/matrix (rows and columns)\n- **3D tensor** = a stack of tables (like multiple sheets in Excel)\n\n**Why PyTorch instead of NumPy?**\nBoth handle multi-dimensional arrays, but PyTorch adds:\n1. **GPU support** — move data to GPU for 10-100× speedup\n2. **Automatic gradients** — computes derivatives for training neural networks\n3. **Neural network layers** — pre-built building blocks\n\n## Creating tensors\nFrom Python/NumPy data, and with common fill rules."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Creating Tensors =====\n\n# From Python data\ndata = torch.tensor([[1, 2, 3], [4, 5, 6]])  # 2×3 tensor\n\n# Fill rules (create tensors filled with specific values)\nzeros = torch.zeros(3, 4)     # 3×4 tensor of zeros\nones = torch.ones(2, 3, 4)    # 2×3×4 tensor of ones\nuniform = torch.rand(3, 4)    # Random values in [0, 1)\nnormal = torch.randn(3, 4)    # Random values from normal distribution (mean=0, std=1)\nintegers = torch.randint(0, 10, (3, 4))  # Random integers in [0, 10)\n\n# Ranged sequences (like Python's range)\nsequence = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\nlinspace = torch.linspace(0, 1, 5)  # 5 evenly-spaced points from 0 to 1\n\n# Data types (dtype) — control precision\nfloat_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)  # 32-bit floats (default)\nint_tensor = torch.tensor([1, 2], dtype=torch.long)           # 64-bit integers (for indices)\n\n# NumPy ↔ PyTorch (they share memory — changes to one affect the other!)\nnp_data = np.array([1, 2, 3], dtype=np.float32)\ntorch_from_np = torch.from_numpy(np_data)  # shares memory with np_data\nback_to_np = torch_from_np.numpy()         # back to NumPy\n\n# Device placement — CPU or GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ngpu_tensor = torch.randn(3, 4, device=device, dtype=torch.float32)\n\nprint('zeros shape:', zeros.shape)\nprint('gpu_tensor device:', gpu_tensor.device)"
  },
  {
   "cell_type": "markdown",
   "id": "vd9boqy0gd8",
   "source": "## Reproducibility: Setting Seeds\nCritical for debugging and comparing experiments",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v6h3c3o9igr",
   "source": "# Fix random state for reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\n# These will now be identical on every run\na = torch.randn(3, 4)\nb = torch.randn(3, 4)\n\n# Try without seed - results change each time\n# But with seed, they're reproducible\ntorch.manual_seed(42)\nx1 = torch.randn(2, 3)\ntorch.manual_seed(42)\nx2 = torch.randn(2, 3)\nprint('With seed, tensors match:', torch.allclose(x1, x2))\n\n# Why this matters: Reproducible experiments for debugging and research",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v438qek6san",
   "source": "## From Lists to Tensors: Building Up Dimensions\nConnect Chapter 5's simple lists to multi-dimensional tensors",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lvfk6e6zgsb",
   "source": "# Step 1: 1D tensors (Chapter 5 callback)\n# From Chapter 5: token IDs from tokenizer\ntoken_ids = [2, 3, 4, 6]\ntokens_1d = torch.tensor(token_ids)\nprint(f\"1D shape: {tokens_1d.shape}\")  # torch.Size([4])\nprint(f\"Data: {tokens_1d}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "58p411qsdz7",
   "source": "# Step 2: Batching (1D → 2D)\n# Process multiple sentences at once\nbatch = torch.tensor([\n    [2, 3, 4, 6],    # sentence 1\n    [5, 7, 8, 9]     # sentence 2\n])\nprint(f\"2D batch shape: {batch.shape}\")  # torch.Size([2, 4])\nprint(\"First sentence:\", batch[0])\nprint(\"Second token of first sentence:\", batch[0, 1])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8ak70wutr5c",
   "source": "# Step 3: Embeddings (2D → 3D)\n# Add 768 numbers per token (GPT-2 style)\nembeddings_3d = torch.randn(2, 4, 768)\nprint(f\"3D embeddings shape: {embeddings_3d.shape}\")  # torch.Size([2, 4, 768])\n\n# Navigate dimensions: batch → token → features\nprint(\"First sentence embeddings:\", embeddings_3d[0].shape)      # (4, 768)\nprint(\"First token of first sentence:\", embeddings_3d[0, 0].shape)  # (768,)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "flcya7oojuj",
   "source": "# Step 4: Attention heads preview (3D → 4D)\n# Multi-head attention adds another dimension (don't worry about details yet)\nattention_4d = torch.randn(2, 8, 4, 4)  # (batch, heads, seq, seq)\nprint(f\"4D attention shape: {attention_4d.shape}\")\nprint(\"Shape interpretation: (batch, heads, seq_len, seq_len)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping, squeezing, permuting\n",
    "Shape gymnastics you’ll use constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Reshaping Operations =====\n# Reshaping = reorganizing data without changing values (like rearranging boxes in a warehouse)\n\nx = torch.arange(12)               # Create 1D tensor [0,1,2,...,11], shape (12,)\nprint(f'Start: {x.shape}')\n\n# view() — reshape but requires \"contiguous\" memory (data laid out sequentially)\nx = x.view(3, 4)                   # Reshape to (3, 4): 3 rows × 4 columns\nprint(f'After view(3,4): {x.shape}')\n\n# reshape() — like view() but handles non-contiguous tensors (safer, use this when unsure)\nx = x.reshape(2, 2, 3)             # Reshape to (2, 2, 3)\nprint(f'After reshape(2,2,3): {x.shape}')\n\n# -1 means \"figure this dimension out for me\"\nx = x.view(-1, 3)                  # -1 becomes 4 (12 total elements ÷ 3 = 4)\nprint(f'After view(-1,3): {x.shape}')\n\n# unsqueeze/squeeze — add or remove dimensions of size 1\ny = torch.randn(3, 4)\ny = y.unsqueeze(0)                 # (1, 3, 4) — add batch dimension at position 0\ny = y.unsqueeze(-1)                # (1, 3, 4, 1) — add dimension at end\ny = y.squeeze()                    # Remove ALL size-1 dims → (3, 4)\nprint(f'After squeeze: {y.shape}')\n\n# permute — reorder dimensions (like transposing but for any number of dims)\nz = torch.randn(2, 3, 4)           # (batch, seq, features)\nz = z.permute(0, 2, 1)             # (batch, features, seq) — swap last two dims\nprint(f'After permute: {z.shape}')\n\n# flatten — collapse dimensions\nt = torch.randn(2, 3, 4)\nt_flat = t.flatten()               # (24,) — all dims collapsed\nt_flat_features = t.flatten(1)     # (2, 12) — flatten starting at dim 1\nprint(f'Fully flattened: {t_flat.shape}')\nprint(f'Flatten features: {t_flat_features.shape}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing\n",
    "Pick out batches, tokens, and use boolean masks."
   ]
  },
  {
   "cell_type": "code",
   "id": "uxo8lxe6sz",
   "source": "# ===== Indexing and Slicing =====\n# Navigate multi-dimensional data like you'd navigate folders: batch → token → features\n\n# Create a 4D tensor simulating attention: (batch, heads, seq, seq)\nx = torch.randn(2, 4, 6, 6)  # 2 batches, 4 heads, 6 tokens, 6 tokens\n\n# Basic indexing\nfirst_batch = x[0]              # Shape: (4, 6, 6) - first batch, all heads\nfirst_head = x[0, 0]            # Shape: (6, 6) - first batch, first head\nsingle_value = x[0, 0, 0, 0]    # Shape: () - a single number\n\n# Slicing with colons\nfirst_two_batches = x[:2]       # Shape: (2, 4, 6, 6) - first 2 batches\nall_but_last_token = x[:, :, :-1, :]  # Shape: (2, 4, 5, 6) - remove last token\nevery_other_head = x[:, ::2]    # Shape: (2, 2, 6, 6) - heads 0 and 2\n\n# Negative indices count from the end\nlast_token = x[:, :, -1, :]     # Shape: (2, 4, 6) - last token in each sequence\n\n# Boolean masking (filter by condition)\nmask = torch.tensor([True, False, True, False])\nfiltered_heads = x[0, mask]     # Shape: (2, 6, 6) - only heads 0 and 2\n\nprint(f'Original shape: {x.shape}')\nprint(f'First batch shape: {first_batch.shape}')\nprint(f'All but last token: {all_but_last_token.shape}')\nprint(f'Filtered heads: {filtered_heads.shape}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ml8x8tvei6",
   "source": "## Attention: The Heart of Transformers\n\n**The big picture:** Attention computes a weighted average of all words, where weights come from relevance scores. Like reading \"The cat sat on the mat\"—when processing \"sat\", you look back at \"cat\" (who sat?) and \"mat\" (sat where?).\n\nWe'll build this in 3 steps:\n1. Basic math (4 operations)\n2. Add causal masking (prevent future peeking)\n3. Production shortcut (PyTorch does it all)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9pesa5kuyi6",
   "source": "import torch\nimport torch.nn.functional as F\n\n# ===== The Basic Math of Attention =====\n# Attention: Query (what am I looking for?), Key (what do I have?), Value (what to return?)\n\n# Simulate embeddings for 5 tokens in batch of 2\nbatch, seq_len, d_head = 2, 5, 64\nQ = torch.randn(batch, seq_len, d_head)  # (2, 5, 64)\nK = torch.randn(batch, seq_len, d_head)\nV = torch.randn(batch, seq_len, d_head)\n\n# Step 1: Compute scores (how much does each token match every other?)\n# The @ operator is matrix multiplication (same as torch.matmul)\n# K.transpose(-2, -1) swaps the last two dimensions: (2, 5, 64) → (2, 64, 5)\n# Result: (2, 5, 64) @ (2, 64, 5) → (2, 5, 5) — a 5×5 grid of scores per batch\nscores = Q @ K.transpose(-2, -1)\nprint(f'Scores shape: {scores.shape}')  # (2, 5, 5)\n\n# Step 2: Scale (prevent softmax saturation)\n# Without scaling, large dot products → softmax gives ~1 for max, ~0 for others\nscores = scores / (d_head ** 0.5)  # divide by sqrt(64) = 8\n\n# Step 3: Softmax (turn scores into probabilities)\n# dim=-1 means \"along the last dimension\" (across keys)\nattn_weights = torch.softmax(scores, dim=-1)  # each row sums to 1\n\n# Step 4: Weighted sum (blend values using probabilities)\noutput = attn_weights @ V  # (2, 5, 64)\n\nprint(f'Output shape: {output.shape}')  # same as input: (2, 5, 64)\nprint(f'Attention weights sum to 1: {attn_weights[0, 0].sum():.4f}')\nprint(f'\\nToken 2\\'s attention weights: {attn_weights[0, 2]}')\nprint('(shows how much token 2 attends to each of the 5 tokens)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uslja03wzs",
   "source": "## Token and Position Embeddings\n\nEvery LLM starts by converting token IDs to dense vectors.\n\n**The problem:** Neural networks can't process raw text like \"cat\". Token IDs (like `5` for \"cat\") are arbitrary—ID 5 isn't \"closer\" to 6 than to 500.\n\n**The solution:** Map each token ID to a learned vector (768 numbers for GPT-2). Similar words learn similar vectors through training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "488cb9ao6l8",
   "source": "# ===== Token and Position Embeddings =====\n# GPT-2 dimensions\nvocab_size, d_model, max_seq_len = 50257, 768, 1024\n\n# Token embeddings: a lookup table (like a dictionary: token_id → vector)\n# nn.Embedding creates a table with vocab_size rows and d_model columns\n# Each row is a 768-dimensional vector representing one token\ntoken_embedding = nn.Embedding(vocab_size, d_model)\n\n# Create some random token IDs (pretend these came from a tokenizer)\ntoken_ids = torch.randint(0, vocab_size, (2, 5))  # 2 sentences, 5 tokens each\n\n# Look up embeddings (just indexing into the table!)\ntoken_vectors = token_embedding(token_ids)\nprint(f'Token embeddings: {token_vectors.shape}')  # (2, 5, 768)\n\n# Position embeddings: where in the sequence (token 0, token 1, etc.)\n# Same idea: a lookup table where row i represents \"position i\"\npos_embedding = nn.Embedding(max_seq_len, d_model)\nposition_ids = torch.arange(5).unsqueeze(0).expand(2, -1)  # [[0,1,2,3,4], [0,1,2,3,4]]\npos_vectors = pos_embedding(position_ids)\nprint(f'Position embeddings: {pos_vectors.shape}')  # (2, 5, 768)\n\n# Combine: element-wise addition (same position, same shape!)\n# Why add instead of concatenate? Addition keeps dimension at 768 (not 1536)\n# The model learns to encode BOTH meaning AND position in the same vector\ninput_embeddings = token_vectors + pos_vectors\nprint(f'Combined: {input_embeddings.shape}')  # (2, 5, 768)\n\n# Parameter counting (how many numbers to learn?)\ntoken_params = vocab_size * d_model   # 50,257 tokens × 768 dims\npos_params = max_seq_len * d_model    # 1,024 positions × 768 dims\nprint(f'Token params: {token_params:,}')      # 38,597,376\nprint(f'Position params: {pos_params:,}')     # 786,432\nprint(f'Total: {token_params + pos_params:,}')  # 39,383,808",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0odmw637ec5p",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple 2-layer MLP for toy classification (self-contained)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# The 5-Step Training Recipe:\n",
    "# 1. Forward pass -> get predictions\n",
    "# 2. Compute loss -> measure error\n",
    "# 3. Backward pass -> calculate gradients\n",
    "# 4. Clip gradients -> prevent explosions\n",
    "# 5. Update weights -> adjust parameters\n",
    "\n",
    "# Setup: model, optimizer, loss function, fake data\n",
    "model = SimpleMLP(16, 32, 2)  # input 16-dim, hidden 32-dim, output 2 classes\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # adaptive learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()  # for classification\n",
    "\n",
    "# Fake training data (batch_size=64)\n",
    "inputs = torch.randn(64, 16)          # 64 examples, 16 features each\n",
    "labels = torch.randint(0, 2, (64,))   # 64 labels (class 0 or 1)\n",
    "\n",
    "# Training loop (3 epochs)\n",
    "for epoch in range(3):\n",
    "    # ===== Training Phase =====\n",
    "    model.train()  # Enable dropout/batch norm (if present)\n",
    "\n",
    "    # Step 1: Zero out old gradients (they accumulate by default!)\n",
    "    optimizer.zero_grad(set_to_none=True)  # set_to_none saves memory\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    logits = model(inputs)  # Get predictions (raw scores)\n",
    "\n",
    "    # Step 3: Compute loss\n",
    "    loss = loss_fn(logits, labels)  # How wrong are we?\n",
    "\n",
    "    # Step 4: Backward pass (compute gradients)\n",
    "    loss.backward()  # Fill .grad for every parameter\n",
    "\n",
    "    # Step 5: Gradient clipping (prevents exploding gradients)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    # Step 6: Update weights\n",
    "    optimizer.step()  # Adjust parameters using gradients\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n",
    "\n",
    "# ===== Evaluation Phase =====\n",
    "model.eval()  # Disable dropout/batch norm\n",
    "with torch.no_grad():  # Don't track gradients (saves memory)\n",
    "    preds = model(inputs).argmax(dim=-1)  # Get class predictions\n",
    "    accuracy = (preds == labels).float().mean()  # Fraction correct\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Key points:\")\n",
    "print(\"  - .backward() fills every parameter\\'s .grad\")\n",
    "print(\"  - Optimizer uses gradients to adjust weights\")\n",
    "print(\"  - Gradient clipping prevents loss spikes (critical for LLMs!)\")\n",
    "print(\"  - .eval() and no_grad() save memory during evaluation\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9ivwpa6evj",
   "source": "# Full embedding module\nclass GPT2Embeddings(nn.Module):\n    def __init__(self, vocab_size, max_seq_len, d_model, dropout=0.1):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, token_ids):\n        batch_size, seq_len = token_ids.shape\n        position_ids = torch.arange(seq_len, device=token_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n        \n        token_emb = self.token_embedding(token_ids)\n        pos_emb = self.pos_embedding(position_ids)\n        \n        embeddings = token_emb + pos_emb\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n# Test it\nembed_layer = GPT2Embeddings(50257, 1024, 768)\ntoken_ids = torch.randint(0, 50257, (2, 10))\noutput = embed_layer(token_ids)\nprint(f\"Embedding output: {output.shape}\")  # (2, 10, 768)\n\n# Total parameters\ntotal_params = sum(p.numel() for p in embed_layer.parameters())\nprint(f\"Total embedding parameters: {total_params:,}\")  # 39,383,808",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jme6dw4jrz",
   "source": "## The Training Loop\n\n**Key concepts:**\n- `nn.Module`: Base class for neural network layers. Your model inherits from it.\n- `super().__init__()`: Calls the parent class's initialization (required boilerplate)\n- `nn.Linear(in, out)`: A matrix multiplication layer (in×out weight matrix)\n- `nn.ReLU()`: Activation function — keeps positive values, zeros out negatives\n- `CrossEntropyLoss`: Measures how wrong classification predictions are\n- `optimizer.zero_grad()`: Clears old gradients (they accumulate by default!)\n- `.backward()`: Computes gradients via automatic differentiation\n- `.step()`: Updates weights using the computed gradients",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6ictya83vku",
   "source": "# Step 2: Add Causal Masking (Prevent Cheating)\n\n# Problem: If token 2 can see tokens 3 and 4, it can cheat during training!\n# Solution: Block future positions by setting their scores to -inf\n\n# What the mask looks like (False = allow, True = block):\n# Token 0 can see: [0]           ← only itself\n# Token 1 can see: [0, 1]        ← past + itself\n# Token 2 can see: [0, 1, 2]     ← past + itself\n# Token 3 can see: [0, 1, 2, 3]\n# Token 4 can see: [0, 1, 2, 3, 4]\n\nseq_len = 5\ncausal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n\n# Apply mask before softmax\nscores = Q @ K.transpose(-2, -1) / (d_head ** 0.5)\nscores = scores.masked_fill(causal_mask, float('-inf'))  # -inf becomes 0 after softmax\nattn_weights = torch.softmax(scores, dim=-1)\n\nprint(\"Causal attention weights (token 2 can only see tokens 0,1,2):\")\nprint(attn_weights[0, 2])  # positions 3 and 4 are zero\nprint(\"\\nNow the model learns to predict 'mat' without seeing 'mat' first!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qj0vb22r4ne",
   "source": "# Step 3: Production Shortcut (One Line)\n\n# You just learned the 4-step manual process to understand what's happening.\n# In practice, PyTorch does it all for you:\n\noutput = F.scaled_dot_product_attention(\n    Q, K, V,\n    is_causal=True  # automatically applies causal masking\n)\n\nprint(f\"Output shape: {output.shape}\")  # (2, 5, 64)\n\n# Why use this instead of manual?\n# - 2-4× faster (uses FlashAttention)\n# - Less memory (doesn't store full attention matrix)\n# - Handles edge cases (numerical stability, dropout, mask broadcasting)\n\nprint(\"\\n✅ When to use manual vs. production:\")\nprint(\"   Learning: Write it manually to understand the math\")\nprint(\"   Production: Use F.scaled_dot_product_attention always\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting examples\n",
    "Bias add and masking broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "y = x + 5\n",
    "batch = torch.randn(32, 10, 768)\n",
    "bias = torch.randn(768)\n",
    "result = batch + bias  # bias broadcasts\n",
    "scores = torch.randn(4, 8, 10, 10)\n",
    "mask = torch.triu(torch.ones(1, 1, 10, 10), 1)\n",
    "masked = scores + mask * -1e9\n",
    "print('result shape:', result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Essential Operations: Element-wise, Matmul, Reductions\n\n**Element-wise operations** work position-by-position—like adding two spreadsheets cell-by-cell. If `a` and `b` are both 3×4 tensors, then `a + b` adds `a[0,0]` to `b[0,0]`, `a[0,1]` to `b[0,1]`, and so on. Same shape in, same shape out."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "add = a + b\n",
    "mul = a * b\n",
    "square = a ** 2\n",
    "exp = torch.exp(a)\n",
    "x = torch.randn(32, 10, 64)\n",
    "W = torch.randn(64, 128)\n",
    "y = x @ W\n",
    "total = a.sum()\n",
    "row_sums = a.sum(dim=1, keepdim=True)\n",
    "col_means = a.mean(dim=0)\n",
    "max_vals, max_idx = a.max(dim=1)\n",
    "c = torch.cat([a, b], dim=0)\n",
    "d = torch.stack([a, b], dim=0)\n",
    "logits = torch.randn(3, 5)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print('y shape:', y.shape)\n",
    "print('probs row sums:', probs.sum(dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Autograd: Automatic Gradients\n\n**What's a gradient?** The derivative—how much does output change when input changes?\n\n**Why care?** Training a neural network means adjusting millions of parameters. Gradients tell us which direction to adjust. PyTorch's autograd does this automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple computation graph: x → y → z\nx = torch.tensor([2.0, 3.0], requires_grad=True)  # track operations on x\ny = x ** 2        # y = [4.0, 9.0]\nz = y.sum()       # z = 13.0\n\n# Compute gradients automatically\nz.backward()      # \"how does z change if I change x?\"\nprint('Gradients:', x.grad)  # tensor([4., 6.])\n\n# What just happened?\n# z = (x²).sum() → dz/dx = 2x\n# At x=[2, 3], gradients are 2*[2, 3] = [4, 6]\n# backward() computed this by walking the graph backward!\n\nprint('\\n✅ Manual check: dz/dx = 2x')\nprint(f'   At x=[2, 3]: 2*x = {2 * x.detach()}')\n\n# When to stop tracking (saves memory during inference):\nprint('\\n--- Stopping gradient tracking ---')\n\n# Option 1: Context manager (for a block of code)\nwith torch.no_grad():\n    y_no_grad = x * 2  # no gradient tracking\n    print(f'No grad computed: {y_no_grad}')\n\n# Option 2: Detach (for a single tensor)\ndetached = x.detach()  # new tensor, no grad history\nprint(f'Detached tensor: {detached}')"
  },
  {
   "cell_type": "markdown",
   "id": "5ukrmijlap",
   "source": "## Summary\n\nYou've now seen all the core PyTorch operations for LLM development:\n\n✅ **Tensor basics** - creation, dtypes, devices\n✅ **Reproducibility** - seeds for debugging\n✅ **Dimension building** - 1D → 2D → 3D → 4D progression\n✅ **Reshaping & indexing** - navigating multi-dimensional data\n✅ **Attention mechanism** - manual + production patterns\n✅ **Causal masking** - preventing future token peeking\n✅ **Embeddings** - token + position representations\n✅ **Broadcasting** - automatic shape expansion\n✅ **Math ops** - element-wise, matmul, reductions\n✅ **Autograd** - automatic differentiation\n✅ **Training loops** - forward, backward, optimize with gradient clipping\n\n**Next steps:** Chapter 7 will show you how to prepare real text data for training!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}