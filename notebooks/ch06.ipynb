{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Companion Notebook\n",
    "**Build Your First LLM — Chapter 6: NumPy & PyTorch Survival Guide**\n\n",
    "Run these cells top-to-bottom to see the tensor basics, masking, broadcasting, and a tiny training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors\n",
    "From Python/NumPy data, and with common fill rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Python data\n",
    "data = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Fill rules\n",
    "zeros = torch.zeros(3, 4)\n",
    "ones = torch.ones(2, 3, 4)\n",
    "uniform = torch.rand(3, 4)\n",
    "normal = torch.randn(3, 4)\n",
    "integers = torch.randint(0, 10, (3, 4))\n",
    "\n",
    "# Ranged sequences\n",
    "sequence = torch.arange(0, 10, 2)\n",
    "linspace = torch.linspace(0, 1, 5)\n",
    "\n",
    "# Dtype / device / NumPy sharing\n",
    "float_tensor = torch.tensor([1.0, 2.0], dtype=torch.float32)\n",
    "int_tensor = torch.tensor([1, 2], dtype=torch.long)\n",
    "np_data = np.array([1, 2, 3], dtype=np.float32)\n",
    "torch_from_np = torch.from_numpy(np_data)  # shares memory\n",
    "back_to_np = torch_from_np.numpy()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpu_tensor = torch.randn(3, 4, device=device, dtype=torch.float32)\n",
    "\n",
    "print('zeros shape:', zeros.shape)\n",
    "print('gpu_tensor device:', gpu_tensor.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vd9boqy0gd8",
   "source": "## Reproducibility: Setting Seeds\nCritical for debugging and comparing experiments",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v6h3c3o9igr",
   "source": "# Fix random state for reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\n# These will now be identical on every run\na = torch.randn(3, 4)\nb = torch.randn(3, 4)\n\n# Try without seed - results change each time\n# But with seed, they're reproducible\ntorch.manual_seed(42)\nx1 = torch.randn(2, 3)\ntorch.manual_seed(42)\nx2 = torch.randn(2, 3)\nprint('With seed, tensors match:', torch.allclose(x1, x2))\n\n# Why this matters: Reproducible experiments for debugging and research",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "v438qek6san",
   "source": "## From Lists to Tensors: Building Up Dimensions\nConnect Chapter 5's simple lists to multi-dimensional tensors",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lvfk6e6zgsb",
   "source": "# Step 1: 1D tensors (Chapter 5 callback)\n# From Chapter 5: token IDs from tokenizer\ntoken_ids = [2, 3, 4, 6]\ntokens_1d = torch.tensor(token_ids)\nprint(f\"1D shape: {tokens_1d.shape}\")  # torch.Size([4])\nprint(f\"Data: {tokens_1d}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "58p411qsdz7",
   "source": "# Step 2: Batching (1D → 2D)\n# Process multiple sentences at once\nbatch = torch.tensor([\n    [2, 3, 4, 6],    # sentence 1\n    [5, 7, 8, 9]     # sentence 2\n])\nprint(f\"2D batch shape: {batch.shape}\")  # torch.Size([2, 4])\nprint(\"First sentence:\", batch[0])\nprint(\"Second token of first sentence:\", batch[0, 1])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8ak70wutr5c",
   "source": "# Step 3: Embeddings (2D → 3D)\n# Add 768 numbers per token (GPT-2 style)\nembeddings_3d = torch.randn(2, 4, 768)\nprint(f\"3D embeddings shape: {embeddings_3d.shape}\")  # torch.Size([2, 4, 768])\n\n# Navigate dimensions: batch → token → features\nprint(\"First sentence embeddings:\", embeddings_3d[0].shape)      # (4, 768)\nprint(\"First token of first sentence:\", embeddings_3d[0, 0].shape)  # (768,)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "flcya7oojuj",
   "source": "# Step 4: Attention heads preview (3D → 4D)\n# Multi-head attention adds another dimension (don't worry about details yet)\nattention_4d = torch.randn(2, 8, 4, 4)  # (batch, heads, seq, seq)\nprint(f\"4D attention shape: {attention_4d.shape}\")\nprint(\"Shape interpretation: (batch, heads, seq_len, seq_len)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping, squeezing, permuting\n",
    "Shape gymnastics you’ll use constantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)               # (12,)\n",
    "x = x.view(3, 4)                   # (3, 4) view needs contiguous\n",
    "\n",
    "x = x.reshape(2, 2, 3)             # (2, 2, 3) reshape is safer\n",
    "x = x.view(-1, 3)                  # (4, 3) infer dim\n",
    "\n",
    "y = torch.randn(3, 4)\n",
    "y = y.unsqueeze(0)                 # (1, 3, 4) add batch\n",
    "y = y.unsqueeze(-1)                # (1, 3, 4, 1) add channel\n",
    "y = y.squeeze()                    # remove size-1 dims\n",
    "\n",
    "z = torch.randn(2, 3, 4)\n",
    "z = z.permute(0, 2, 1)             # (2, 4, 3) reorder dims\n",
    "\n",
    "t = torch.randn(2, 3, 4)\n",
    "t_flat = t.flatten()               # (24,)\n",
    "t_flat_features = t.flatten(1)     # (2, 12)\n",
    "\n",
    "print('x shape:', x.shape)\n",
    "print('z shape:', z.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and slicing\n",
    "Pick out batches, tokens, and use boolean masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ml8x8tvei6",
   "source": "## Attention: The Heart of Transformers\n\n**The big picture:** Attention computes a weighted average of all words, where weights come from relevance scores. Like reading \"The cat sat on the mat\"—when processing \"sat\", you look back at \"cat\" (who sat?) and \"mat\" (sat where?).\n\nWe'll build this in 3 steps:\n1. Basic math (4 operations)\n2. Add causal masking (prevent future peeking)\n3. Production shortcut (PyTorch does it all)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9pesa5kuyi6",
   "source": "import torch\nimport torch.nn.functional as F\n\n# Step 1: The Basic Math\n# Attention: Query (what am I looking for?), Key (what do I have?), Value (what to return?)\n\n# Simulate embeddings for 5 tokens in batch of 2\nbatch, seq_len, d_head = 2, 5, 64\nQ = torch.randn(batch, seq_len, d_head)  # (2, 5, 64)\nK = torch.randn(batch, seq_len, d_head)\nV = torch.randn(batch, seq_len, d_head)\n\n# Step 1: Compute scores (how much does each token match every other?)\nscores = Q @ K.transpose(-2, -1)  # (2, 5, 5) — 5×5 grid of scores\n\n# Step 2: Scale (prevent softmax saturation)\nscores = scores / (d_head ** 0.5)  # divide by sqrt(64) = 8\n\n# Step 3: Softmax (turn scores into probabilities)\nattn_weights = torch.softmax(scores, dim=-1)  # each row sums to 1\n\n# Step 4: Weighted sum (blend values using probabilities)\noutput = attn_weights @ V  # (2, 5, 64)\n\nprint(f\"Output shape: {output.shape}\")  # same as input: (2, 5, 64)\nprint(f\"Attention weights sum to 1: {attn_weights[0, 0].sum():.4f}\")\nprint(f\"\\nToken 2's attention weights: {attn_weights[0, 2]}\")\nprint(\"(shows how much token 2 attends to each of the 5 tokens)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uslja03wzs",
   "source": "## Token and Position Embeddings\n\nEvery LLM starts by converting token IDs to dense vectors.\n\n**The problem:** Neural networks can't process raw text like \"cat\". Token IDs (like `5` for \"cat\") are arbitrary—ID 5 isn't \"closer\" to 6 than to 500.\n\n**The solution:** Map each token ID to a learned vector (768 numbers for GPT-2). Similar words learn similar vectors through training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "488cb9ao6l8",
   "source": [
    "# GPT-2 dimensions\n",
    "vocab_size, d_model, max_seq_len = 50257, 768, 1024\n",
    "\n",
    "# Token embeddings: lookup table\n",
    "token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "token_ids = torch.randint(0, vocab_size, (2, 5))\n",
    "token_vectors = token_embedding(token_ids)\n",
    "print(f\"Token embeddings: {token_vectors.shape}\")  # (2, 5, 768)\n",
    "\n",
    "# Position embeddings: where in the sequence\n",
    "pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "position_ids = torch.arange(5).unsqueeze(0).expand(2, -1)\n",
    "pos_vectors = pos_embedding(position_ids)\n",
    "print(f\"Position embeddings: {pos_vectors.shape}\")  # (2, 5, 768)\n",
    "\n",
    "# Combine: element-wise addition\n",
    "input_embeddings = token_vectors + pos_vectors\n",
    "print(f\"Combined: {input_embeddings.shape}\")  # (2, 5, 768)\n",
    "\n",
    "# Parameter counting\n",
    "token_params = vocab_size * d_model\n",
    "pos_params = max_seq_len * d_model\n",
    "print(f\"Token params: {token_params:,}\")      # 38,597,376\n",
    "print(f\"Position params: {pos_params:,}\")    # 786,432\n",
    "print(f\"Total: {token_params + pos_params:,}\") # 39,383,808\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0odmw637ec5p",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple 2-layer MLP for toy classification (self-contained)\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# The 5-Step Training Recipe:\n",
    "# 1. Forward pass -> get predictions\n",
    "# 2. Compute loss -> measure error\n",
    "# 3. Backward pass -> calculate gradients\n",
    "# 4. Clip gradients -> prevent explosions\n",
    "# 5. Update weights -> adjust parameters\n",
    "\n",
    "# Setup: model, optimizer, loss function, fake data\n",
    "model = SimpleMLP(16, 32, 2)  # input 16-dim, hidden 32-dim, output 2 classes\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # adaptive learning rate\n",
    "loss_fn = nn.CrossEntropyLoss()  # for classification\n",
    "\n",
    "# Fake training data (batch_size=64)\n",
    "inputs = torch.randn(64, 16)          # 64 examples, 16 features each\n",
    "labels = torch.randint(0, 2, (64,))   # 64 labels (class 0 or 1)\n",
    "\n",
    "# Training loop (3 epochs)\n",
    "for epoch in range(3):\n",
    "    # ===== Training Phase =====\n",
    "    model.train()  # Enable dropout/batch norm (if present)\n",
    "\n",
    "    # Step 1: Zero out old gradients (they accumulate by default!)\n",
    "    optimizer.zero_grad(set_to_none=True)  # set_to_none saves memory\n",
    "\n",
    "    # Step 2: Forward pass\n",
    "    logits = model(inputs)  # Get predictions (raw scores)\n",
    "\n",
    "    # Step 3: Compute loss\n",
    "    loss = loss_fn(logits, labels)  # How wrong are we?\n",
    "\n",
    "    # Step 4: Backward pass (compute gradients)\n",
    "    loss.backward()  # Fill .grad for every parameter\n",
    "\n",
    "    # Step 5: Gradient clipping (prevents exploding gradients)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    # Step 6: Update weights\n",
    "    optimizer.step()  # Adjust parameters using gradients\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n",
    "\n",
    "# ===== Evaluation Phase =====\n",
    "model.eval()  # Disable dropout/batch norm\n",
    "with torch.no_grad():  # Don't track gradients (saves memory)\n",
    "    preds = model(inputs).argmax(dim=-1)  # Get class predictions\n",
    "    accuracy = (preds == labels).float().mean()  # Fraction correct\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ Key points:\")\n",
    "print(\"  - .backward() fills every parameter\\'s .grad\")\n",
    "print(\"  - Optimizer uses gradients to adjust weights\")\n",
    "print(\"  - Gradient clipping prevents loss spikes (critical for LLMs!)\")\n",
    "print(\"  - .eval() and no_grad() save memory during evaluation\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9ivwpa6evj",
   "source": "# Full embedding module\nclass GPT2Embeddings(nn.Module):\n    def __init__(self, vocab_size, max_seq_len, d_model, dropout=0.1):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, token_ids):\n        batch_size, seq_len = token_ids.shape\n        position_ids = torch.arange(seq_len, device=token_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n        \n        token_emb = self.token_embedding(token_ids)\n        pos_emb = self.pos_embedding(position_ids)\n        \n        embeddings = token_emb + pos_emb\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n# Test it\nembed_layer = GPT2Embeddings(50257, 1024, 768)\ntoken_ids = torch.randint(0, 50257, (2, 10))\noutput = embed_layer(token_ids)\nprint(f\"Embedding output: {output.shape}\")  # (2, 10, 768)\n\n# Total parameters\ntotal_params = sum(p.numel() for p in embed_layer.parameters())\nprint(f\"Total embedding parameters: {total_params:,}\")  # 39,383,808",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6ictya83vku",
   "source": "# Step 2: Add Causal Masking (Prevent Cheating)\n\n# Problem: If token 2 can see tokens 3 and 4, it can cheat during training!\n# Solution: Block future positions by setting their scores to -inf\n\n# What the mask looks like (False = allow, True = block):\n# Token 0 can see: [0]           ← only itself\n# Token 1 can see: [0, 1]        ← past + itself\n# Token 2 can see: [0, 1, 2]     ← past + itself\n# Token 3 can see: [0, 1, 2, 3]\n# Token 4 can see: [0, 1, 2, 3, 4]\n\nseq_len = 5\ncausal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n\n# Apply mask before softmax\nscores = Q @ K.transpose(-2, -1) / (d_head ** 0.5)\nscores = scores.masked_fill(causal_mask, float('-inf'))  # -inf becomes 0 after softmax\nattn_weights = torch.softmax(scores, dim=-1)\n\nprint(\"Causal attention weights (token 2 can only see tokens 0,1,2):\")\nprint(attn_weights[0, 2])  # positions 3 and 4 are zero\nprint(\"\\nNow the model learns to predict 'mat' without seeing 'mat' first!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qj0vb22r4ne",
   "source": "# Step 3: Production Shortcut (One Line)\n\n# You just learned the 4-step manual process to understand what's happening.\n# In practice, PyTorch does it all for you:\n\noutput = F.scaled_dot_product_attention(\n    Q, K, V,\n    is_causal=True  # automatically applies causal masking\n)\n\nprint(f\"Output shape: {output.shape}\")  # (2, 5, 64)\n\n# Why use this instead of manual?\n# - 2-4× faster (uses FlashAttention)\n# - Less memory (doesn't store full attention matrix)\n# - Handles edge cases (numerical stability, dropout, mask broadcasting)\n\nprint(\"\\n✅ When to use manual vs. production:\")\nprint(\"   Learning: Write it manually to understand the math\")\nprint(\"   Production: Use F.scaled_dot_product_attention always\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting examples\n",
    "Bias add and masking broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "y = x + 5\n",
    "batch = torch.randn(32, 10, 768)\n",
    "bias = torch.randn(768)\n",
    "result = batch + bias  # bias broadcasts\n",
    "scores = torch.randn(4, 8, 10, 10)\n",
    "mask = torch.triu(torch.ones(1, 1, 10, 10), 1)\n",
    "masked = scores + mask * -1e9\n",
    "print('result shape:', result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Essential Operations: Element-wise, Matmul, Reductions\n\n**Element-wise operations** work position-by-position—like adding two spreadsheets cell-by-cell. If `a` and `b` are both 3×4 tensors, then `a + b` adds `a[0,0]` to `b[0,0]`, `a[0,1]` to `b[0,1]`, and so on. Same shape in, same shape out."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.randn(3, 4)\n",
    "add = a + b\n",
    "mul = a * b\n",
    "square = a ** 2\n",
    "exp = torch.exp(a)\n",
    "x = torch.randn(32, 10, 64)\n",
    "W = torch.randn(64, 128)\n",
    "y = x @ W\n",
    "total = a.sum()\n",
    "row_sums = a.sum(dim=1, keepdim=True)\n",
    "col_means = a.mean(dim=0)\n",
    "max_vals, max_idx = a.max(dim=1)\n",
    "c = torch.cat([a, b], dim=0)\n",
    "d = torch.stack([a, b], dim=0)\n",
    "logits = torch.randn(3, 5)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "print('y shape:', y.shape)\n",
    "print('probs row sums:', probs.sum(dim=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Autograd: Automatic Gradients\n\n**What's a gradient?** The derivative—how much does output change when input changes?\n\n**Why care?** Training a neural network means adjusting millions of parameters. Gradients tell us which direction to adjust. PyTorch's autograd does this automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple computation graph: x → y → z\nx = torch.tensor([2.0, 3.0], requires_grad=True)  # track operations on x\ny = x ** 2        # y = [4.0, 9.0]\nz = y.sum()       # z = 13.0\n\n# Compute gradients automatically\nz.backward()      # \"how does z change if I change x?\"\nprint('Gradients:', x.grad)  # tensor([4., 6.])\n\n# What just happened?\n# z = (x²).sum() → dz/dx = 2x\n# At x=[2, 3], gradients are 2*[2, 3] = [4, 6]\n# backward() computed this by walking the graph backward!\n\nprint('\\n✅ Manual check: dz/dx = 2x')\nprint(f'   At x=[2, 3]: 2*x = {2 * x.detach()}')\n\n# When to stop tracking (saves memory during inference):\nprint('\\n--- Stopping gradient tracking ---')\n\n# Option 1: Context manager (for a block of code)\nwith torch.no_grad():\n    y_no_grad = x * 2  # no gradient tracking\n    print(f'No grad computed: {y_no_grad}')\n\n# Option 2: Detach (for a single tensor)\ndetached = x.detach()  # new tensor, no grad history\nprint(f'Detached tensor: {detached}')"
  },
  {
   "cell_type": "markdown",
   "id": "5ukrmijlap",
   "source": "## Summary\n\nYou've now seen all the core PyTorch operations for LLM development:\n\n✅ **Tensor basics** - creation, dtypes, devices\n✅ **Reproducibility** - seeds for debugging\n✅ **Dimension building** - 1D → 2D → 3D → 4D progression\n✅ **Reshaping & indexing** - navigating multi-dimensional data\n✅ **Attention mechanism** - manual + production patterns\n✅ **Causal masking** - preventing future token peeking\n✅ **Embeddings** - token + position representations\n✅ **Broadcasting** - automatic shape expansion\n✅ **Math ops** - element-wise, matmul, reductions\n✅ **Autograd** - automatic differentiation\n✅ **Training loops** - forward, backward, optimize with gradient clipping\n\n**Next steps:** Chapter 7 will show you how to prepare real text data for training!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}