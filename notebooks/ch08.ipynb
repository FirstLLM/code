{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 8: Building the Tokenizer\n\n> \"Language is the dress of thought.\" ‚Äî **Samuel Johnson**, Writer\n\n---\n\n## What is Tokenization?\n\n**Tokenization** is the process of converting text into numbers so neural networks can process it. Think of it as translating English into a secret code where each word, subword, or character gets a unique number.\n\n```\n\"Hello world\" ‚Üí [15496, 995]  (using GPT-2's tokenizer)\n```\n\nThe tokenizer also works in reverse: given numbers, it produces text.\n\n---\n\n## What You'll Learn\n\n- How text becomes numbers through three different tokenization strategies\n- Why character-level tokenization is simple but inefficient\n- How word-level tokenization handles unknown words and why vocabulary size matters\n- The clever trick behind subword tokenization (BPE) that powers modern LLMs\n- How to use production tokenizers like tiktoken and Hugging Face transformers\n- The quirks and gotchas that affect prompts and API costs\n\n---\n\n## Setup\n\nFirst, let's install the required packages:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tiktoken transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Character-Level Tokenization\n\nLet's build the simplest tokenizer: treat every character as a token.\n\n**Python Class Reminder:**\n- A **class** is a blueprint that bundles data and functions together\n- `__init__(self)` runs when you create an object (initializes its data)\n- `self` refers to \"this specific object\" (like \"this car\" vs \"cars in general\")\n- `@property` makes a method behave like an attribute (no parentheses needed)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        # Two dictionaries for bidirectional lookup\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"Build vocabulary from text.\n",
    "        \n",
    "        Why sorted()? So vocab is deterministic‚Äîsame text always\n",
    "        produces same IDs. Without sorting, Python's set order is random.\n",
    "        \"\"\"\n",
    "        # Extract unique characters - sets automatically handle uniqueness\n",
    "        chars = sorted(set(text))\n",
    "        \n",
    "        # Assign each character an ID (starting from 0)\n",
    "        for i, c in enumerate(chars):\n",
    "            self.char_to_id[c] = i\n",
    "            self.id_to_char[i] = c\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to list of token IDs.\n",
    "        \n",
    "        Returns a list of integers, one per character.\n",
    "        \"\"\"\n",
    "        return [self.char_to_id[c] for c in text]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert list of token IDs back to text.\n",
    "        \n",
    "        Uses str.join() to concatenate characters with no spaces\n",
    "        between them (unlike word tokenization which needs spaces).\n",
    "        \"\"\"\n",
    "        return \"\".join(self.id_to_char[i] for i in ids)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"How many unique characters we know.\"\"\"\n",
    "        return len(self.char_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It: Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train tokenizer\n",
    "tokenizer = CharTokenizer()\n",
    "tokenizer.fit(\"Hello, world!\")\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Vocab: {tokenizer.char_to_id}\")\n",
    "\n",
    "# Encode\n",
    "text = \"Hello\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"\\n'{text}' ‚Üí {ids}\")\n",
    "\n",
    "# Decode - round trip should be lossless!\n",
    "decoded = tokenizer.decode(ids)\n",
    "print(f\"{ids} ‚Üí '{decoded}'\")\n",
    "print(f\"Perfect round trip? {text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary Size vs Sequence Length Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Tokenization is the first step in any language model.\"\n",
    "\n",
    "# Character-level\n",
    "char_tok = CharTokenizer()\n",
    "char_tok.fit(sentence)\n",
    "char_ids = char_tok.encode(sentence)\n",
    "\n",
    "print(f\"Original text: {len(sentence)} characters\")\n",
    "print(f\"Vocab size: {char_tok.vocab_size}\")\n",
    "print(f\"Sequence length: {len(char_ids)}\")\n",
    "print(f\"First 20 tokens: {char_ids[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Word-Level Tokenization\n\nNow let's build a word-level tokenizer that handles unknown words with special tokens.\n\n**Special Tokens:** Reserved IDs with specific meanings:\n- `<PAD>` (ID 0): Padding ‚Äî fills sequences to equal length for batching\n- `<UNK>` (ID 1): Unknown ‚Äî represents words not in our vocabulary\n- `<BOS>` (ID 2): Beginning of Sequence ‚Äî marks where text starts\n- `<EOS>` (ID 3): End of Sequence ‚Äî marks where text ends\n\nWhy do we need these? Without `<UNK>`, our tokenizer would crash on new words. Without `<PAD>`, we couldn't process multiple sentences at once (they'd have different lengths)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class WordTokenizer:\n",
    "    def __init__(self, max_vocab_size=10000):\n",
    "        \"\"\"\n",
    "        max_vocab_size: Maximum vocabulary size (including special tokens)\n",
    "        \n",
    "        Why 10,000? It's a balance:\n",
    "        - Too small (1,000): Too many unknowns\n",
    "        - Too large (100,000): Huge embedding table, slow training\n",
    "        - 10,000-50,000: Sweet spot for learning\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"Build vocabulary from most frequent words.\"\"\"\n",
    "        # Start simple: split on whitespace and lowercase\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Count word frequencies - why? Common words get their own IDs,\n",
    "        # rare words become <UNK>. This minimizes unknowns in practice.\n",
    "        counts = Counter(words)\n",
    "        \n",
    "        # Reserve IDs 0-3 for special tokens\n",
    "        self.word_to_id = {\n",
    "            \"<PAD>\": 0,\n",
    "            \"<UNK>\": 1,\n",
    "            \"<BOS>\": 2,\n",
    "            \"<EOS>\": 3\n",
    "        }\n",
    "        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
    "        \n",
    "        # Add most common words (keeping max_vocab_size limit)\n",
    "        # Start IDs at 4 since 0-3 are reserved for special tokens\n",
    "        for i, (word, count) in enumerate(counts.most_common(self.max_vocab_size - 4), start=4):\n",
    "            self.word_to_id[word] = i\n",
    "            self.id_to_word[i] = word\n",
    "\n",
    "    def encode(self, text, add_special_tokens=False):\n",
    "        \"\"\"Convert text to token IDs.\n",
    "        \n",
    "        add_special_tokens: If True, add <BOS> at start and <EOS> at end\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Look up each word, fallback to <UNK> (ID=1) if not found\n",
    "        # .get(word, 1) returns 1 if word isn't in our vocabulary\n",
    "        ids = [self.word_to_id.get(w, 1) for w in words]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            ids = [2] + ids + [3]  # [<BOS>] + text + [<EOS>]\n",
    "        \n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids, skip_special_tokens=True):\n",
    "        \"\"\"Convert token IDs back to text.\n",
    "        \n",
    "        skip_special_tokens: If True, don't output <PAD>, <BOS>, etc.\n",
    "        Why? You don't want output like: \"<BOS> Hello world <EOS>\"\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for i in ids:\n",
    "            word = self.id_to_word.get(i, \"<UNK>\")\n",
    "            # Skip special tokens in output if requested\n",
    "            if skip_special_tokens and word in [\"<PAD>\", \"<BOS>\", \"<EOS>\"]:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        \n",
    "        # Join with spaces (unlike char tokenizer which used \"\".join)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Current vocabulary size (number of unique tokens).\"\"\"\n",
    "        return len(self.word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It: Complete Example with Unknown Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer with small vocab to force unknowns\n",
    "tokenizer = WordTokenizer(max_vocab_size=10)\n",
    "\n",
    "# Train on limited text\n",
    "training_text = \"\"\"\n",
    "The cat sat on the mat.\n",
    "The cat was on the mat.\n",
    "The dog sat on the mat.\n",
    "\"\"\"\n",
    "tokenizer.fit(training_text)\n",
    "\n",
    "print(f\"Vocabulary: {tokenizer.word_to_id}\")\n",
    "\n",
    "# Encode a sentence with known words\n",
    "text1 = \"the cat sat\"\n",
    "ids1 = tokenizer.encode(text1)\n",
    "print(f\"\\n'{text1}' ‚Üí {ids1}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(ids1)}'\")\n",
    "\n",
    "# Encode with unknown word\n",
    "text2 = \"the elephant sat\"  # \"elephant\" not in vocab!\n",
    "ids2 = tokenizer.encode(text2)\n",
    "print(f\"\\n'{text2}' ‚Üí {ids2}\")\n",
    "print(f\"Decoded: '{tokenizer.decode(ids2)}'\")\n",
    "\n",
    "# Try with special tokens\n",
    "ids3 = tokenizer.encode(\"the cat\", add_special_tokens=True)\n",
    "print(f\"\\nWith special tokens: {ids3}\")\n",
    "print(f\"Decoded (showing special): '{tokenizer.decode(ids3, skip_special_tokens=False)}'\")\n",
    "print(f\"Decoded (hiding special): '{tokenizer.decode(ids3, skip_special_tokens=True)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Exercise: See the Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Character tokenizer\n",
    "char_tok = CharTokenizer()\n",
    "char_tok.fit(text)\n",
    "char_ids = char_tok.encode(text)\n",
    "\n",
    "# Word tokenizer\n",
    "word_tok = WordTokenizer(max_vocab_size=20)\n",
    "word_tok.fit(text)\n",
    "word_ids = word_tok.encode(text)\n",
    "\n",
    "print(\"CHARACTER TOKENIZER:\")\n",
    "print(f\"  Vocab size: {char_tok.vocab_size}\")\n",
    "print(f\"  Sequence length: {len(char_ids)}\")\n",
    "print(f\"  Tokens: {char_ids[:20]}...\")\n",
    "\n",
    "print(\"\\nWORD TOKENIZER:\")\n",
    "print(f\"  Vocab size: {word_tok.vocab_size}\")\n",
    "print(f\"  Sequence length: {len(word_ids)}\")\n",
    "print(f\"  Tokens: {word_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How Production Tokenizers Work: BPE (Byte-Pair Encoding)\n\nModern LLMs use **subword tokenization** ‚Äî a clever middle ground between characters and words:\n\n**The Problem:**\n- Character tokenizers: Too many tokens per text (slow, expensive)\n- Word tokenizers: Can't handle new words (\"ChatGPT\" ‚Üí `<UNK>`)\n\n**The Solution: BPE (Byte-Pair Encoding)**\n\nBPE learns subwords automatically by repeatedly merging the most frequent character pairs:\n\n```\nStep 1: Start with characters: [\"l\", \"o\", \"w\", \"e\", \"r\"]\nStep 2: Most frequent pair is (\"l\", \"o\") ‚Üí merge to \"lo\"\nStep 3: Most frequent pair is (\"lo\", \"w\") ‚Üí merge to \"low\"\nStep 4: Continue until vocab size reached...\n```\n\n**Result:** Common words become single tokens, rare words split into known pieces:\n- \"lower\" ‚Üí [\"low\", \"er\"] ‚úì (common, efficient)\n- \"lowest\" ‚Üí [\"low\", \"est\"] ‚úì (compound word handled!)\n- \"ChatGPT\" ‚Üí [\"Chat\", \"G\", \"PT\"] ‚úì (no `<UNK>` needed!)\n\n**Key Insight:** BPE never produces `<UNK>` because any character sequence can be broken into known pieces!\n\n---\n\n## 3. Production Tokenizers: tiktoken\n\nNow let's use OpenAI's tiktoken library for GPT-4's tokenizer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load GPT-4's tokenizer encoding\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4, GPT-3.5-turbo\n",
    "\n",
    "# Encode text\n",
    "text = \"Hello, world! How are you?\"\n",
    "tokens = enc.encode(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = enc.decode(tokens)\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"Perfect round-trip: {text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Actual Token Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode each token individually to see what it represents\n",
    "token_strings = [enc.decode([t]) for t in tokens]\n",
    "\n",
    "print(f\"Token breakdown:\")\n",
    "for token_id, token_str in zip(tokens, token_strings):\n",
    "    print(f\"  {token_id:5d} ‚Üí '{token_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting for API Budgeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Write a haiku about programming.\",\n",
    "    \"Explain quantum computing in simple terms for a 10-year-old child.\",\n",
    "    \"Generate a 500-word essay on climate change.\"\n",
    "]\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    tokens = enc.encode(prompt)\n",
    "    # Example pricing (check current rates at openai.com/api/pricing)\n",
    "    cost = len(tokens) * 0.00001  # $0.01 per 1K input tokens\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"  Tokens: {len(tokens)}\")\n",
    "    print(f\"  Cost (input): ~${cost:.5f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Production Tokenizers: Hugging Face Transformers\n\n**Hugging Face** is a company/library that provides pre-trained models and tokenizers. `AutoTokenizer` automatically loads the right tokenizer for any model.\n\nThe Hugging Face tokenizer returns a dictionary with:\n- `input_ids`: The token IDs (what we care about most)\n- `attention_mask`: 1s for real tokens, 0s for padding (tells model what to ignore)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer (open-source)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Hello, world! How are you?\"\n",
    "\n",
    "# Encode - returns dict with token IDs and attention mask\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")  # \"pt\" = PyTorch tensors\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Token IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# Decode\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = encoded['input_ids'][0].tolist()\n",
    "token_strings = [tokenizer.decode([t]) for t in tokens]\n",
    "\n",
    "print(f\"Token breakdown:\")\n",
    "for tid, tstr in zip(tokens, token_strings):\n",
    "    print(f\"  {tid:5d} ‚Üí '{tstr}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tokenization Quirks\n",
    "\n",
    "### Quirk #1: Leading Spaces Change Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Compare with and without leading space\n",
    "texts = [\"Hello\", \" Hello\", \"world\", \" world\"]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = enc.encode(text)\n",
    "    print(f\"'{text}' ‚Üí {tokens} ({len(tokens)} token{'s' if len(tokens) > 1 else ''})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quirk #2: Numbers Split by Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [\"10\", \"100\", \"1000\", \"10000\", \"42\", \"2024\"]\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for num in numbers:\n",
    "    tokens = enc.encode(num)\n",
    "    token_strs = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"'{num}' ‚Üí {tokens} = {token_strs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quirk #3: Emoji and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = [\"üòÄ\", \"üöÄ\", \"üëç\", \"Hello üòÄ world\", \"üî•üî•üî•\"]\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for text in emojis:\n",
    "    tokens = enc.encode(text)\n",
    "    token_strs = [enc.decode([t]) for t in tokens]\n",
    "    print(f\"'{text}' ‚Üí {len(tokens)} tokens: {token_strs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Exercise: Tokenize Your Dataset\n",
    "\n",
    "Connect Chapter 7's dataset to tokenization (you'll need your chapter7_output.jsonl file for this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "# Example dataset (replace with your Chapter 7 output)\n",
    "example_dataset = [\n",
    "    {\"text\": \"AI systems learn from examples\", \"split\": \"train\"},\n",
    "    {\"text\": \"Neural networks need lots of data\", \"split\": \"train\"},\n",
    "    {\"text\": \"Deep learning uses multiple layers\", \"split\": \"val\"}\n",
    "]\n",
    "\n",
    "# Tokenize each example\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for record in example_dataset:\n",
    "    text = record[\"text\"]\n",
    "    tokens = enc.encode(text)\n",
    "    record[\"token_ids\"] = tokens\n",
    "    record[\"token_count\"] = len(tokens)\n",
    "\n",
    "print(f\"Tokenized {len(example_dataset)} examples\")\n",
    "\n",
    "# Compute statistics\n",
    "token_counts = [r[\"token_count\"] for r in example_dataset]\n",
    "avg_tokens = sum(token_counts) / len(token_counts)\n",
    "max_tokens = max(token_counts)\n",
    "min_tokens = min(token_counts)\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Average tokens per example: {avg_tokens:.1f}\")\n",
    "print(f\"  Max tokens: {max_tokens}\")\n",
    "print(f\"  Min tokens: {min_tokens}\")\n",
    "\n",
    "# Show first example\n",
    "print(f\"\\nFirst example:\")\n",
    "print(json.dumps(example_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "**What we built:**\n",
    "\n",
    "1. **Character tokenizer:** Simple but inefficient (tiny vocab ~100, long sequences)\n",
    "2. **Word tokenizer:** Efficient sequences but huge vocab and unknown word problems\n",
    "3. **Production tools:** Used tiktoken and Hugging Face for real-world tokenization\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "- Tokenization is reversible (lossless round-trip)\n",
    "- The vocab size vs sequence length tradeoff is fundamental\n",
    "- Special tokens serve specific purposes (BOS/EOS/PAD/UNK)\n",
    "- BPE learns subwords automatically by merging frequent pairs\n",
    "- Tokenization has quirks (leading spaces, number splitting, emoji)\n",
    "\n",
    "**Next:** Chapter 9 will convert these token IDs to embedding vectors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}